[
  {
    "objectID": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html",
    "href": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html",
    "title": "When Growth Forgot Happiness â€” The Hidden Curve of Human Progress",
    "section": "",
    "text": "Economic growth was supposed to lift all boats. But did more money always buy better lives? Using Gapminder, we trace the hidden curve where prosperity rose yet well-being lagged â€” and ask what truly counts as progress."
  },
  {
    "objectID": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#the-question-what-if-growth-isnt-happiness",
    "href": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#the-question-what-if-growth-isnt-happiness",
    "title": "When Growth Forgot Happiness â€” The Hidden Curve of Human Progress",
    "section": "ğŸ•Šï¸ 1. The Question â€” What if Growth Isnâ€™t Happiness?",
    "text": "ğŸ•Šï¸ 1. The Question â€” What if Growth Isnâ€™t Happiness?\nFor decades, countries chased GDP â€” the grand total of what they produced.\nBut can numbers capture joy, belonging, or peace of mind?\nThis story starts with a simple but daring question:\n\nIf GDP rises, does life always feel better?\n\nWeâ€™ll keep the math gentle and the meaning clear.\nYou donâ€™t need a statistics background â€” only curiosity."
  },
  {
    "objectID": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#building-a-fair-baseline",
    "href": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#building-a-fair-baseline",
    "title": "When Growth Forgot Happiness â€” The Hidden Curve of Human Progress",
    "section": "ğŸ”¢ 2. Building a Fair Baseline",
    "text": "ğŸ”¢ 2. Building a Fair Baseline\nWeâ€™ll connect income (GDP per person) and life expectancy,\nthen look for nations that live longer or shorter lives than their income predicts.\nThat difference â€” the gap between money and meaning â€”\nis what we call the âœ¨ Decoupling Index.\n\n\nCode\ndf &lt;- gapminder %&gt;%\n    mutate(\n        log_gdppc = log10(gdpPercap),\n        decade = (year %/% 10) * 10\n    )\n\nlo &lt;- loess(lifeExp ~ log_gdppc + year, data = df, span = 0.5, degree = 2)\n\ndf_index &lt;- df %&gt;%\n    mutate(\n        lifeExp_hat = predict(lo, newdata = df),\n        decouple = lifeExp - lifeExp_hat\n    )\n\n\nInterpretation:\n\nğŸ’™ Positive Index â†’ People live longer than income predicts.\nâ¤ï¸ Negative Index â†’ People live shorter lives than income predicts."
  },
  {
    "objectID": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#the-hidden-curve-appears",
    "href": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#the-hidden-curve-appears",
    "title": "When Growth Forgot Happiness â€” The Hidden Curve of Human Progress",
    "section": "ğŸ“ˆ 3. The Hidden Curve Appears",
    "text": "ğŸ“ˆ 3. The Hidden Curve Appears\n\n\nCode\ndf_index %&gt;%\n    ggplot(aes(x = gdpPercap, y = lifeExp, color = decouple)) +\n    geom_point(alpha = 0.6) +\n    scale_x_log10(labels = label_dollar(scale = 1)) +\n    scale_y_continuous(limits = c(25, 90)) +\n    scale_color_gradient2(\n        low = \"#a50f15\", mid = \"#f0f0f0\", high = \"#08519c\",\n        midpoint = 0,\n        name = \"Decoupling\\nIndex\"\n    ) +\n    labs(x = \"GDP per person (log scale)\", y = \"Life expectancy (years)\") +\n    theme_minimal(base_size = 13)\n\n\n\n\n\nğŸ’« The Hidden Curve â€” life vs.Â income, colored by Decoupling Index.\n\n\n\n\nâœ¨ Look closely: some nations sparkle above the line â€” others droop below.\nThe curve isnâ€™t smooth progress; itâ€™s a collage of human choices."
  },
  {
    "objectID": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#time-reveals-who-outran-money",
    "href": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#time-reveals-who-outran-money",
    "title": "When Growth Forgot Happiness â€” The Hidden Curve of Human Progress",
    "section": "â³ 4. Time Reveals Who Outran Money",
    "text": "â³ 4. Time Reveals Who Outran Money\n\n\nCode\ntrend &lt;- df_index %&gt;%\n    group_by(country) %&gt;%\n    arrange(year, .by_group = TRUE) %&gt;%\n    summarize(\n        start_decouple = first(decouple),\n        end_decouple   = last(decouple),\n        change = end_decouple - start_decouple,\n        continent = first(continent),\n        .groups = \"drop\"\n    ) %&gt;%\n    arrange(desc(change))\n\n\n\n\nCode\ntop_risers   &lt;- trend |&gt; slice_max(change, n = 10)\ntop_slippers &lt;- trend |&gt; slice_min(change, n = 10)\n\nbind_rows(\n    top_risers |&gt; mutate(type = \"ğŸŒŸ Riser\"),\n    top_slippers |&gt; mutate(type = \"ğŸ’¤ Slipper\")\n    ) |&gt;\n    select(type, country, continent, start_decouple, end_decouple, change) |&gt;\n    knitr::kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype\ncountry\ncontinent\nstart_decouple\nend_decouple\nchange\n\n\n\n\nğŸŒŸ Riser\nNicaragua\nAmericas\n-10.82\n9.60\n20.42\n\n\nğŸŒŸ Riser\nSaudi Arabia\nAsia\n-22.50\n-3.82\n18.68\n\n\nğŸŒŸ Riser\nGambia\nAfrica\n-7.86\n8.47\n16.33\n\n\nğŸŒŸ Riser\nHaiti\nAmericas\n-9.41\n5.83\n15.24\n\n\nğŸŒŸ Riser\nMadagascar\nAfrica\n-7.82\n5.63\n13.45\n\n\nğŸŒŸ Riser\nComoros\nAfrica\n-1.52\n11.85\n13.37\n\n\nğŸŒŸ Riser\nBahrain\nAsia\n-16.02\n-2.79\n13.23\n\n\nğŸŒŸ Riser\nPeru\nAmericas\n-11.47\n1.13\n12.60\n\n\nğŸŒŸ Riser\nHonduras\nAmericas\n-7.07\n4.74\n11.81\n\n\nğŸŒŸ Riser\nSenegal\nAfrica\n-7.27\n4.47\n11.74\n\n\nğŸ’¤ Slipper\nBotswana\nAfrica\n7.26\n-22.74\n-30.00\n\n\nğŸ’¤ Slipper\nSwaziland\nAfrica\n-1.15\n-27.56\n-26.41\n\n\nğŸ’¤ Slipper\nLesotho\nAfrica\n4.73\n-15.11\n-19.84\n\n\nğŸ’¤ Slipper\nEquatorial Guinea\nAfrica\n-2.94\n-21.69\n-18.74\n\n\nğŸ’¤ Slipper\nTaiwan\nAsia\n15.55\n0.18\n-15.37\n\n\nğŸ’¤ Slipper\nZimbabwe\nAfrica\n10.94\n-3.91\n-14.84\n\n\nğŸ’¤ Slipper\nZambia\nAfrica\n-0.51\n-13.23\n-12.72\n\n\nğŸ’¤ Slipper\nTrinidad and Tobago\nAmericas\n6.31\n-5.73\n-12.04\n\n\nğŸ’¤ Slipper\nSingapore\nAsia\n10.79\n-1.00\n-11.79\n\n\nğŸ’¤ Slipper\nThailand\nAsia\n11.19\n0.28\n-10.91\n\n\n\nğŸŒ Top ten risers and slippers by Decoupling Index.\n\n\nThese countries are your story leads â€” places where life pulled ahead or fell behind wealth."
  },
  {
    "objectID": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#country-micro-stories-human-over-graph",
    "href": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#country-micro-stories-human-over-graph",
    "title": "When Growth Forgot Happiness â€” The Hidden Curve of Human Progress",
    "section": "ğŸ‡§ğŸ‡¹ 5. Country Micro-Stories â€” Human Over Graph",
    "text": "ğŸ‡§ğŸ‡¹ 5. Country Micro-Stories â€” Human Over Graph\nWeâ€™ll soon explore pairs like:\n\nğŸ‡§ğŸ‡¹ Bhutan, where happiness led the economy.\nğŸ‡¯ğŸ‡µ Japan, where prosperity met stagnation.\nğŸ‡ºğŸ‡¸ United States, where health plateaued despite riches.\n\nEach line tells a tale â€” of policy, culture, and priorities.\n\n\nCode\nset.seed(1)\nCASE_RISER   &lt;- top_risers$country[1]\nCASE_SLIPPER &lt;- top_slippers$country[1]\nglue(\"Riser example: {CASE_RISER}\")\n\n\nRiser example: Nicaragua\n\n\nCode\nglue(\"Slipper example: {CASE_SLIPPER}\")\n\n\nSlipper example: Botswana"
  },
  {
    "objectID": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#the-quiet-threshold-when-money-stops-buying-life",
    "href": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#the-quiet-threshold-when-money-stops-buying-life",
    "title": "When Growth Forgot Happiness â€” The Hidden Curve of Human Progress",
    "section": "ğŸ’¡ 6. The Quiet Threshold â€” When Money Stops Buying Life",
    "text": "ğŸ’¡ 6. The Quiet Threshold â€” When Money Stops Buying Life\n\n\nCode\nref_year &lt;- 2007\ncurve_df &lt;- df_index |&gt; filter(year == ref_year)\nlo_ref &lt;- loess(lifeExp ~ log_gdppc, data = curve_df, span = 0.5)\ngridx &lt;- tibble(log_gdppc = seq(min(curve_df$log_gdppc), max(curve_df$log_gdppc), length.out = 200))\npred  &lt;- predict(lo_ref, newdata = gridx)\ncurve_plot &lt;- bind_cols(gridx, tibble(lifeExp_hat = pred)) |&gt;\n    mutate(gdpPercap = 10^log_gdppc)\n\ncurve_df |&gt;\n    ggplot(aes(gdpPercap, lifeExp)) +\n    geom_point(alpha = 0.6) +\n    geom_line(data = curve_plot, aes(gdpPercap, lifeExp_hat), linewidth = 1) +\n    scale_x_log10(labels = label_dollar(scale = 1)) +\n    labs(\n        x = \"GDP per person (log scale)\",\n        y = \"Life expectancy (years)\",\n        subtitle = \"Beyond this bend, more income buys little extra life.\"\n    ) +\n    theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\n\nBeyond a comfort zone (roughly $30 000 per person),\nthe curve flattens â€” proof that lifeâ€™s deepest gains come from what money canâ€™t buy."
  },
  {
    "objectID": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#where-we-go-next",
    "href": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#where-we-go-next",
    "title": "When Growth Forgot Happiness â€” The Hidden Curve of Human Progress",
    "section": "ğŸš€ 7. Where We Go Next",
    "text": "ğŸš€ 7. Where We Go Next\nEach finding births a future story:\n\nğŸ˜Š The Happiness Deficit â€” when wealth outpaces joy.\nğŸ”„ The Curve That Bends Twice â€” tech progress, then overwhelm.\nğŸ§® Counting What Counts â€” education, safety, belonging.\nğŸ’¬ Data of Dignity â€” measuring lives beyond the ledger.\n\nOur mission: turn data into empathy, and statistics into stories that feel alive.\n\n Dataset: Gapminder"
  },
  {
    "objectID": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#references-credits",
    "href": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#references-credits",
    "title": "When Growth Forgot Happiness â€” The Hidden Curve of Human Progress",
    "section": "ğŸ“š References & Credits",
    "text": "ğŸ“š References & Credits\nData Sources\n\nğŸŒ Gapminder Foundation â€” Gapminder Data Portal\nğŸŒ World Bank Open Data â€” World Development Indicators\nğŸ‡ºğŸ‡³ UNDP â€” Human Development Reports\nğŸ˜Š World Happiness Report (UN SDSN)\n\nBackground & Inspiration\n\nğŸ“– Easterlin (1974) Does Economic Growth Improve the Human Lot?\nğŸ•Šï¸ Sen (1999) Development as Freedom\nğŸ’­ Layard (2005) Happiness: Lessons from a New Science\nğŸŒˆ Rosling (2018) Factfulness\n\nCredits\n\nâœï¸ Concept & Writing â€” Insightful Tales\nğŸ¨ Visual Design â€” Insightful Tales Studio\nğŸ“Š Data Stewardship â€” Gapminder Foundation"
  },
  {
    "objectID": "stories/the-hidden-rhythm-of-baseball.html",
    "href": "stories/the-hidden-rhythm-of-baseball.html",
    "title": "The Hidden Rhythm of Baseball",
    "section": "",
    "text": "A hundred years of baseball heard, not just measured â€” where numbers whisper the story of how the game learned to breathe differently with each generation."
  },
  {
    "objectID": "stories/the-hidden-rhythm-of-baseball.html#the-game-that-breathes",
    "href": "stories/the-hidden-rhythm-of-baseball.html#the-game-that-breathes",
    "title": "The Hidden Rhythm of Baseball",
    "section": "ğŸ§¢ The Game That Breathes",
    "text": "ğŸ§¢ The Game That Breathes\nBaseball isnâ€™t timed by a clock, but it still has a tempo. Some eras feel quick and scrappy, others patient and thunderous. In this story, weâ€™ll listen to that tempo using longâ€‘run league averages. If you donâ€™t speak data â€” no problem. Think of every chart as a stethoscope on the gameâ€™s chest.\n\nğŸ§° What data are we using?\nWeâ€™ll use the wellâ€‘known Lahman database (public, free) through R. Weâ€™ll count simple things anyone can grasp: runs, home runs, walks, strikeouts, and stolen bases. From these counts, we compute commonsense rates, like â€œhome runs per plate appearance.â€\n\n\nCode\nlibrary(tidyverse)\nlibrary(Lahman)\n\n# Safe rate helper\nrate &lt;- function(n, d) ifelse(d &gt; 0, n / d, NA_real_)\n\n# Build league-level season summary from Teams\nseason &lt;- Teams %&gt;%\n  transmute(\n    year = yearID,\n    g   = G,\n    r   = R,\n    hr  = HR,\n    bb  = BB,\n    so  = SO,\n    sb  = SB,\n    ab  = AB,\n    hbp = coalesce(HBP, 0L),\n    sf  = coalesce(SF, 0L),\n    pa  = AB + BB + hbp + sf\n  ) %&gt;%\n  group_by(year) %&gt;%\n  summarise(\n    g      = sum(g,  na.rm = TRUE),\n    r_pg   = rate(sum(r,  na.rm = TRUE), g),\n    # Use summed plate appearances; if pa has gaps early on, fall back to AB+BB\n    pa_sum = sum(pa, na.rm = TRUE),\n    abbb   = sum(ab + bb, na.rm = TRUE),\n    denom  = ifelse(pa_sum &gt; 0, pa_sum, abbb),\n\n    hr_pa  = rate(sum(hr, na.rm = TRUE), denom),\n    bb_pa  = rate(sum(bb, na.rm = TRUE), denom),\n    so_pa  = rate(sum(so, na.rm = TRUE), denom),\n    sb_pa  = rate(sum(sb, na.rm = TRUE), denom)\n  ) %&gt;%\n  ungroup() %&gt;%\n  arrange(year) %&gt;%\n  mutate(\n    # 5-year right-aligned moving average using base R (no zoo needed)\n    rpg_roll5 = as.numeric(stats::filter(r_pg, rep(1/5, 5), sides = 1)),\n    tto_share = hr_pa + bb_pa + so_pa,\n    bip_share = pmax(0, 1 - tto_share)\n  )\n\n\n\nDecoder\n\nper PA â‰ˆ per trip to the plate.\nRuns per game = average runs a team scores in a game that year.\nBalls in play = anything that isnâ€™t a HR, walk, or strikeout."
  },
  {
    "objectID": "stories/the-hidden-rhythm-of-baseball.html#a-heartbeat-you-can-see-runs-per-game",
    "href": "stories/the-hidden-rhythm-of-baseball.html#a-heartbeat-you-can-see-runs-per-game",
    "title": "The Hidden Rhythm of Baseball",
    "section": "ğŸ¼ A Heartbeat You Can See: Runs Per Game",
    "text": "ğŸ¼ A Heartbeat You Can See: Runs Per Game\nIf the game were breathing, runs per game would be its chest rising and falling. Peaks = lively offense. Valleys = pitcherâ€‘dominated eras.\n\n\nCode\nseason %&gt;%\n    ggplot(aes(year, r_pg)) +\n    geom_line(alpha = 0.85) +\n    geom_line(aes(y = rpg_roll5), linewidth = 1.1) +\n    labs(\n        title = \"Runs per Team Game over Time\",\n        subtitle = \"The 5â€‘year smoother reveals gentle waves rather than a straight trend\",\n        x = NULL, y = \"Runs per game\"\n    )\n\n\n\n\n\n\n\n\n\nWhat to notice: The line waves. Offense doesnâ€™t simply climb forever â€” it surges and recedes across generations."
  },
  {
    "objectID": "stories/the-hidden-rhythm-of-baseball.html#fewer-things-happen-louder-when-they-do",
    "href": "stories/the-hidden-rhythm-of-baseball.html#fewer-things-happen-louder-when-they-do",
    "title": "The Hidden Rhythm of Baseball",
    "section": "ğŸ’¥ Fewer Things Happen, Louder When They Do",
    "text": "ğŸ’¥ Fewer Things Happen, Louder When They Do\nModern baseball increasingly ends in one of three ways: home run, walk, or strikeout. That means fewer balls are fielded. The game can feel slower between bursts of action, even while the big moments are bigger.\n\n\nCode\nseason %&gt;%\n    select(year, hr_pa, bb_pa, so_pa) %&gt;%\n    pivot_longer(-year, names_to = \"metric\", values_to = \"rate\"\n    ) %&gt;%\n    mutate(\n        metric = recode(metric,\n        hr_pa = \"Home runs per PA\",\n        bb_pa = \"Walks per PA\",\n        so_pa = \"Strikeouts per PA\")\n    ) %&gt;%\n    ggplot(aes(year, rate, linetype = metric)) +\n    geom_line() +\n    labs(\n        title = \"The Three True Outcomes over Time\",\n        subtitle = \"Share per plate appearance (league average)\",\n        x = NULL, y = \"Rate\"\n    )\n\n\n\n\n\n\n\n\n\nWhat to notice: Strikeouts climb steadily; home runs rise in bursts; walks bob up and down. Together they take up more of the action pie."
  },
  {
    "objectID": "stories/the-hidden-rhythm-of-baseball.html#thread-count-how-much-play-is-in-play",
    "href": "stories/the-hidden-rhythm-of-baseball.html#thread-count-how-much-play-is-in-play",
    "title": "The Hidden Rhythm of Baseball",
    "section": "ğŸ§¶ Thread Count: How Much Play Is In Play?",
    "text": "ğŸ§¶ Thread Count: How Much Play Is In Play?\nLetâ€™s peek at the share of balls in play â€” the portion of plate appearances that require a fielder to do something.\n\n\nCode\nseason %&gt;%\n    ggplot(aes(year, bip_share)) +\n    geom_line() +\n    labs(\n        title = \"Balls In Play as a Share of All Plate Appearances\",\n        subtitle = \"When this falls, the game has fewer fielding plays and more allâ€‘orâ€‘nothing outcomes\",\n        x = NULL, y = \"Share (0â€“1)\"\n    )\n\n\n\n\n\n\n\n\n\nWhat to notice: When this dips, the gameâ€™s rhythm changes â€” fewer sprints and throws, more waiting for the next big swing."
  },
  {
    "objectID": "stories/the-hidden-rhythm-of-baseball.html#speeds-comeback-arc",
    "href": "stories/the-hidden-rhythm-of-baseball.html#speeds-comeback-arc",
    "title": "The Hidden Rhythm of Baseball",
    "section": "ğŸƒ Speedâ€™s Comeback Arc",
    "text": "ğŸƒ Speedâ€™s Comeback Arc\nSteals add heartbeat. Their rise and fall shows how teams toggle between risk and power.\n\n\nCode\nseason %&gt;%\n    ggplot(aes(year, sb_pa)) +\n    geom_line() +\n    labs(\n        title = \"Stolen Bases per Plate Appearance\",\n        subtitle = \"Risk tolerance in motion\",\n        x = NULL, y = \"SB per PA\"\n    )\n\n\n\n\n\n\n\n\n\nWhat to notice: Cycles. The sport rediscovering speed isnâ€™t nostalgia â€” itâ€™s ecology. When defenses and pitchers neutralize power, movement becomes valuable again."
  },
  {
    "objectID": "stories/the-hidden-rhythm-of-baseball.html#a-simple-zeit-rhythm-index",
    "href": "stories/the-hidden-rhythm-of-baseball.html#a-simple-zeit-rhythm-index",
    "title": "The Hidden Rhythm of Baseball",
    "section": "ğŸŒ€ A Simple â€œZeit Rhythmâ€ Index",
    "text": "ğŸŒ€ A Simple â€œZeit Rhythmâ€ Index\nTo summarize the feel of an era, we blend four dials:\n\nPower (HR/PA)\nMiss (SO/PA)\nMovement (SB/PA)\nPlay in the Field (Balls in play share, but we subtract it â€” less BIP = more allâ€‘orâ€‘nothing feel)\n\nThe result is a single line: higher = louder, burstier baseball; lower = more contactâ€‘heavy flow.\n\n\nCode\nzi &lt;- season %&gt;%\n  mutate(across(c(hr_pa, so_pa, sb_pa, bip_share), scale)) %&gt;%\n  mutate(ZI = as.numeric(hr_pa + so_pa + sb_pa - bip_share)) %&gt;%\n  mutate(ZI_roll5 = as.numeric(stats::filter(ZI, rep(1/5, 5), sides = 1))) %&gt;%\n  select(year, ZI, ZI_roll5)\n\n\n\nzi %&gt;% \n    ggplot(aes(year, ZI)) +\n    geom_hline(yintercept = 0, linewidth = 0.3) +\n    geom_line(alpha = 0.55) +\n    geom_line(aes(y = ZI_roll5), linewidth = 1.1) +\n    labs(\n        title = \"Zeit Rhythm Index\",\n        subtitle = \"A blended, zâ€‘scored feel of power + miss + movement âˆ’ balls in play\",\n        x = NULL, y = \"Index (0 â‰ˆ average era)\"\n    )\n\n\n\n\n\n\n\n\n\nWhat to notice: The line oscillates in multiâ€‘decade waves rather than drifting randomly â€” a hint that baseball selfâ€‘corrects when any one style takes over."
  },
  {
    "objectID": "stories/the-hidden-rhythm-of-baseball.html#what-this-suggests-next",
    "href": "stories/the-hidden-rhythm-of-baseball.html#what-this-suggests-next",
    "title": "The Hidden Rhythm of Baseball",
    "section": "ğŸ”­ What This Suggests Next",
    "text": "ğŸ”­ What This Suggests Next\n\nContact renaissance: As pitchers and defenses optimize for strikeouts and power prevention, putting the ball in play may regain value â€” not as nostalgia, but as an edge.\nSmart speed: Selective stealing and hitâ€‘andâ€‘run tactics may return in pockets where they exploit pitcher/catcher timing.\nFielding theater: If leagues want more visible action, rule tweaks that increase balls in play could shift the rhythm again."
  },
  {
    "objectID": "stories/the-hidden-rhythm-of-baseball.html#reproducibility-notes",
    "href": "stories/the-hidden-rhythm-of-baseball.html#reproducibility-notes",
    "title": "The Hidden Rhythm of Baseball",
    "section": "ğŸ“¦ Reproducibility Notes",
    "text": "ğŸ“¦ Reproducibility Notes\n\nSource: The public Lahman database via the Lahman R package.\nAverages: We averaged teams into a single league line per year to hear the broad rhythm (details will vary by ballpark or league).\nApproximations: Older seasons are missing some counts (like sacrifice flies). We used simple, conservative fallbacks that donâ€™t change the big picture.\nSmoothing: A gentle 5â€‘year average makes the waves easier to see.\n\n\n Dataset: Lahman Baseball Database"
  },
  {
    "objectID": "stories/the-hidden-rhythm-of-baseball.html#references-credits",
    "href": "stories/the-hidden-rhythm-of-baseball.html#references-credits",
    "title": "The Hidden Rhythm of Baseball",
    "section": "ğŸ“š References & Credits",
    "text": "ğŸ“š References & Credits\n\nSean Lahman â€” Lahman Baseball Database (public historical stats).\nLahman (R package, CRAN) â€” convenient access to the database.\ntidyverse (CRAN) â€” data wrangling and plotting in R.\nzoo (CRAN) â€” rolling averages.\n\nInspirations: FanGraphs and Baseballâ€‘Reference for longstanding public analytics; the broader sabermetric community for opening up the conversation."
  },
  {
    "objectID": "stories/geography-of-luck.html",
    "href": "stories/geography-of-luck.html",
    "title": "The Geography of Luck",
    "section": "",
    "text": "What we call â€œluckâ€ often begins with latitude and longitude. This story uncovers how place, mobility, and systems intertwine to shape the fortunes of people and nations."
  },
  {
    "objectID": "stories/geography-of-luck.html#premise",
    "href": "stories/geography-of-luck.html#premise",
    "title": "The Geography of Luck",
    "section": "ğŸ’¡ Premise",
    "text": "ğŸ’¡ Premise\nIf you were to spin a globe and stop it at random, the odds of landing somewhere prosperous are not uniform. Latitude has long acted as the silent architect of opportunity â€” shaping climate, agriculture, disease, and, through them, destiny.\nThis story asks: Does geography still rule luck in the 21st century?"
  },
  {
    "objectID": "stories/geography-of-luck.html#data-sources",
    "href": "stories/geography-of-luck.html#data-sources",
    "title": "The Geography of Luck",
    "section": "ğŸŒ Data Sources",
    "text": "ğŸŒ Data Sources\nWe combine multiple open datasets (2020 snapshot for a clean cross-section):\n\n\n\n\n\n\n\n\nTheme\nSource\nIndicator\n\n\n\n\nEconomic output\nWorld Bank\nGDP per capita (constant USD, NY.GDP.PCAP.KD)\n\n\nHealth\nWorld Bank\nLife expectancy at birth (SP.DYN.LE00.IN)\n\n\nGeography\nNatural Earth\nCountry boundaries + ISO3 (for joins)"
  },
  {
    "objectID": "stories/geography-of-luck.html#first-glance-the-shape-of-prosperity",
    "href": "stories/geography-of-luck.html#first-glance-the-shape-of-prosperity",
    "title": "The Geography of Luck",
    "section": "ğŸŒ± First Glance â€” The Shape of Prosperity",
    "text": "ğŸŒ± First Glance â€” The Shape of Prosperity\n\n\nCode\n# GDP vs |latitude|\nggplot(wb, aes(abs_lat, gdp)) +\n  geom_point(aes(color = region), alpha = .7, size = 2, show.legend = FALSE) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"black\") +\n  scale_y_log10(labels = label_dollar()) +\n  labs(title = \"The Geography of Luck\",\n       subtitle = \"GDP per capita (log scale) vs. absolute latitude, 2020 (World Bank)\",\n       x = \"Absolute Latitude (Â°)\", y = \"GDP per capita (2015 USD, log scale)\")\n\n\n\n\n\n\n\n\n\nReading: Prosperity tends to rise away from the equator, plateau in the midâ€‘latitudes, and soften again near the poles â€” a climatic parabola of fortune."
  },
  {
    "objectID": "stories/geography-of-luck.html#beyond-wealth-life-follows-latitude",
    "href": "stories/geography-of-luck.html#beyond-wealth-life-follows-latitude",
    "title": "The Geography of Luck",
    "section": "ğŸ’° Beyond Wealth â€” Life Follows Latitude",
    "text": "ğŸ’° Beyond Wealth â€” Life Follows Latitude\n\n\nCode\n# Life expectancy vs |latitude|\nwb |&gt; ggplot(aes(abs_lat, life)) +\n  geom_point(alpha = .6, size = 2, color = \"#555\") +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"#111\") +\n  labs(title = \"Latitude vs. Life Expectancy\",\n       subtitle = \"Life expectancy at birth vs. absolute latitude, 2020\",\n       x = \"Absolute Latitude (Â°)\", y = \"Years\")\n\n\n\n\n\n\n\n\n\nReading: Health shadows wealth: life expectancy arcs with latitude, but with notable outliers."
  },
  {
    "objectID": "stories/geography-of-luck.html#exceptions-when-geography-loses-residuals",
    "href": "stories/geography-of-luck.html#exceptions-when-geography-loses-residuals",
    "title": "The Geography of Luck",
    "section": "ğŸŒ Exceptions â€” When Geography Loses (Residuals)",
    "text": "ğŸŒ Exceptions â€” When Geography Loses (Residuals)\nWe fit a simple model of wealth on latitude and look for countries that outperform what geography alone would predict.\n\n\nCode\n# Fit on complete cases only, then join residuals back by iso3c\nwb_cc &lt;- wb |&gt; dplyr::filter(is.finite(gdp_log), is.finite(abs_lat), !is.na(iso3c))\nmodel_simple &lt;- lm(gdp_log ~ abs_lat, data = wb_cc)\nres_tbl &lt;- tibble::tibble(iso3c = wb_cc$iso3c, resid_gdp_lat = unname(resid(model_simple)))\n\nwb &lt;- wb |&gt; dplyr::left_join(res_tbl, by = \"iso3c\") |&gt;\n  dplyr::mutate(luck_index = as.numeric(scale(resid_gdp_lat)))\n\n# Top/bottom outperformers by residuals (ignore NA)\nout_top &lt;- wb |&gt; dplyr::filter(!is.na(resid_gdp_lat)) |&gt;\n  dplyr::slice_max(resid_gdp_lat, n = 10) |&gt;\n  dplyr::select(country, region, resid_gdp_lat)\n\nout_bot &lt;- wb |&gt; dplyr::filter(!is.na(resid_gdp_lat)) |&gt;\n  dplyr::slice_min(resid_gdp_lat, n = 10) |&gt;\n  dplyr::select(country, region, resid_gdp_lat)\n\nknitr::kable(out_top, digits = 2, caption = \"Top 10 'Lucky Defiers' â€” richer than latitude alone predicts (log residual)\")\n\n\n\nTop 10 â€˜Lucky Defiersâ€™ â€” richer than latitude alone predicts (log residual)\n\n\ncountry\nregion\nresid_gdp_lat\n\n\n\n\nSingapore\nEast Asia & Pacific\n1.42\n\n\nCayman Islands\nLatin America & Caribbean\n1.18\n\n\nBermuda\nNorth America\n1.06\n\n\nBrunei Darussalam\nEast Asia & Pacific\n1.05\n\n\nMonaco\nEurope & Central Asia\n1.05\n\n\nQatar\nMiddle East & North Africa\n0.95\n\n\nGuam\nEast Asia & Pacific\n0.93\n\n\nVirgin Islands (U.S.)\nLatin America & Caribbean\n0.88\n\n\nHong Kong SAR, China\nEast Asia & Pacific\n0.86\n\n\nUnited Arab Emirates\nMiddle East & North Africa\n0.82\n\n\n\n\n\n\n\nCode\nknitr::kable(out_bot, digits = 2, caption = \"Bottom 10 â€” poorer than latitude alone predicts (log residual)\")\n\n\n\nBottom 10 â€” poorer than latitude alone predicts (log residual)\n\n\ncountry\nregion\nresid_gdp_lat\n\n\n\n\nAfghanistan\nSouth Asia\n-1.26\n\n\nKyrgyz Republic\nEurope & Central Asia\n-1.10\n\n\nSyrian Arab Republic\nMiddle East & North Africa\n-1.09\n\n\nMadagascar\nSub-Saharan Africa\n-1.09\n\n\nMozambique\nSub-Saharan Africa\n-1.05\n\n\nTajikistan\nEurope & Central Asia\n-0.99\n\n\nBurundi\nSub-Saharan Africa\n-0.99\n\n\nUkraine\nEurope & Central Asia\n-0.92\n\n\nLesotho\nSub-Saharan Africa\n-0.91\n\n\nNiger\nSub-Saharan Africa\n-0.86\n\n\n\n\n\n\nInterpretation: Highâ€‘performers often pair trade centrality, education, or resource rents with policy stability. Underâ€‘performers frequently face governance or conflict frictions."
  },
  {
    "objectID": "stories/geography-of-luck.html#optional-map-the-luck-index-residual-choropleth",
    "href": "stories/geography-of-luck.html#optional-map-the-luck-index-residual-choropleth",
    "title": "The Geography of Luck",
    "section": "ğŸ—ºï¸ Optional Map â€” The Luck Index (Residual Choropleth)",
    "text": "ğŸ—ºï¸ Optional Map â€” The Luck Index (Residual Choropleth)\nA choropleth map of the residuals (â€œluck indexâ€) highlights countries that overâ€‘ or underâ€‘perform relative to their absolute latitude.\n\n\nCode\n# Luck-index choropleth (guarded)\nif (isTRUE(has_ne)) {\n  # Merge residuals back to geometry\n  map_dat &lt;- dat |&gt; dplyr::left_join(wb |&gt; dplyr::select(iso3c, resid_gdp_lat), by = c(\"iso_a3\" = \"iso3c\"))\n\n  # Breaks for a symmetric diverging map around zero residual\n  brks &lt;- c(-Inf, -0.6, -0.3, -0.15, 0, 0.15, 0.3, 0.6, Inf)\n  map_dat$res_bin &lt;- cut(map_dat$resid_gdp_lat, breaks = brks, include.lowest = TRUE)\n\n  # Plot\n  ggplot(map_dat) +\n    geom_sf(aes(fill = res_bin), color = \"white\", size = 0.1) +\n    scale_fill_brewer(type = \"div\", palette = \"RdBu\", direction = -1, na.value = \"#e5e7eb\",\n                      name = \"Luck index\n(GDP~vs~|lat| residual)\") +\n    labs(title = \"Outperformers and Underperformers â€” Geography of Luck\",\n         subtitle = \"Residuals from log(GDP per capita) ~ absolute latitude, 2020\",\n         caption = \"Data: World Bank (WDI), Natural Earth. Negative = underperforming latitude; positive = outperforming.\") +\n    theme(legend.position = \"right\")\n} else {\n  message(\"Optional map disabled: install.packages(c('sf','rnaturalearth','rnaturalearthdata')) to enable the choropleth.\")\n}\n\n\n\n\n\n\n\n\n\nHow to read: Blue shades underperform their latitude (negative residuals); red shades outperform (positive residuals). Neutral grays are missing data."
  },
  {
    "objectID": "stories/geography-of-luck.html#reflection",
    "href": "stories/geography-of-luck.html#reflection",
    "title": "The Geography of Luck",
    "section": "ğŸ’­ Reflection",
    "text": "ğŸ’­ Reflection\n\nThe sun may rise for everyone, but it still shines longer for some.\n\nLatitude once dictated agriculture, disease, and labor. Today, it still whispers through economies â€” a relic of environmental inheritance.\nYet each bright outlier on the map is a defiance of fate: policy and ingenuity turning geography into geographyâ€™s undoing.\n\n\n Dataset: nycflights13"
  },
  {
    "objectID": "stories/geography-of-luck.html#technical-appendix",
    "href": "stories/geography-of-luck.html#technical-appendix",
    "title": "The Geography of Luck",
    "section": "âš™ï¸ Technical Appendix",
    "text": "âš™ï¸ Technical Appendix\n\nPackages: WDI, rnaturalearth, sf, tidyverse, janitor, scales\n\nYear: 2020 snapshot (World Bank)\n\nTransforms: log10(GDP per capita); residuals from lm(gdp_log ~ abs_lat)\n\nCRS: EPSG:4326 (WGS84)\n\nReproducibility: Set a specific year for comparability; extend to multiâ€‘year to track weakening/strengthening latitude effects.\n\n\n\nCode\n# Time evolution â€” correlation weakening/strengthening over time (1980â€“2023)\nhist &lt;- WDI(indicator = c(gdp = \"NY.GDP.PCAP.KD\"),\n            start = 1980, end = 2023, extra = TRUE) |&gt;\n  janitor::clean_names() |&gt;\n  dplyr::filter(region != \"Aggregates\") |&gt;\n  dplyr::mutate(\n    # make sure latitude is numeric before abs()\n    latitude = suppressWarnings(readr::parse_number(as.character(latitude))),\n    abs_lat  = abs(latitude),\n    gdp_log  = log10(gdp)\n  ) |&gt;\n  dplyr::group_by(year) |&gt;\n  dplyr::summarize(\n    r_lat_gdp = cor(abs_lat, gdp_log, use = \"complete.obs\"),\n    .groups = \"drop\"\n  )\n\nggplot(hist, aes(year, r_lat_gdp)) +\n  geom_line() +\n  labs(\n    title = \"Latitudeâ€“Wealth Correlation Over Time\",\n    x = \"Year\", y = \"Correlation r (|lat| vs log GDP per capita)\"\n  )"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Insightful Tales",
    "section": "",
    "text": "Insightful Tales is where numbers find their voice. Through thoughtful storytelling, rich visuals, and clear analysis, the site turns datasets into human-focused narratives about how the world moves, grows, and remembers."
  },
  {
    "objectID": "index.html#featured-stories",
    "href": "index.html#featured-stories",
    "title": "Insightful Tales",
    "section": "âœ¨ Featured Stories",
    "text": "âœ¨ Featured Stories\nEach story turns data into narrative â€” click to explore.\n\n\nğŸ§­ Geography of Luck â€” The Latitude Lottery\n\nAirports closer to the poles face harsher weather â€” but does geography itself decide punctuality?\nThis story measures how latitude quietly skews flight reliability, uncovering patterns of chance, climate, and coordination.\n\n\n\nğŸ¦‹ Butterfly Morning Delays â€” Small Causes, Wide Ripples\n\nA delay at dawn can echo across an entire air-traffic network.\nUsing NYC flight data, this tale visualizes how minute perturbations evolve into widespread effects â€” a modern butterfly effect.\n\n\n\nğŸŒ¦ï¸ Weather Memory â€” How the Sky Remembers\n\nDoes weather have â€œmomentumâ€?\nBy correlating daily anomalies, this piece explores how temperature and precipitation persist â€” revealing the hidden inertia of the atmosphere.\n\n\n\nâš¾ Baseball Zeitgeist â€” A Century of Swings\n\nUsing Lahmanâ€™s database, this story blends sport, sociology, and technology to show how Americaâ€™s game mirrors its evolving spirit â€” from dead-ball grit to data-driven power.\n\n\n\nğŸ’° The Wealth of Nations, Revisited â€” Gapminder Tales\n\nA visual narrative on global progress and paradox: who truly caught up, who stalled, and what it means when GDP rises but well-being lags behind.\n\n\n\nğŸ›°ï¸ Planetary Pulse â€” Reading Earth from Orbit\n\nSatellite imagery meets storytelling.\nFrom greening deserts to vanishing glaciers, this piece turns NASAâ€™s pixels into a planetary diary of change.\n\n\n\nğŸš‡ London in Motion â€” The Pulse of a City\n\nEvery entry, exit, and delay tells a story of urban life.\nThis narrative transforms Transport for London data into a living map of rhythm, pressure, and resilience.\n\n\n\nğŸ•¹ï¸ The Hype Curve â€” Life and Death of a Game\n\nSteam data visualized as cultural heartbeat:\nhow trends surge, communities form, and nostalgia revives titles long thought dormant.\n\n\n\nğŸŒ† Lights and Lives â€” Cities from Space\n\nNight-light data reveals how urban sprawl, inequality, and energy intersect.\nA story told not in words but in luminous patterns across the Earthâ€™s surface."
  },
  {
    "objectID": "index.html#origins-evolution-of-data",
    "href": "index.html#origins-evolution-of-data",
    "title": "Insightful Tales",
    "section": "ğŸ§¬ Origins & Evolution of Data",
    "text": "ğŸ§¬ Origins & Evolution of Data\nEvery number has an origin story. These tales uncover how data was born, who shaped it, and how it came to define the modern world.\n\n\nğŸ§® The Genesis of Data â€” When Counting Became Knowing\n\nFrom ancient tablets to industrial ledgers, the human instinct to measure the world laid the groundwork for todayâ€™s data revolution."
  },
  {
    "objectID": "index.html#featured-datasets-where-stories-begin",
    "href": "index.html#featured-datasets-where-stories-begin",
    "title": "Insightful Tales",
    "section": "ğŸ“š Featured Datasets â€” Where Stories Begin",
    "text": "ğŸ“š Featured Datasets â€” Where Stories Begin\nEach of these datasets offers more than numbersâ€”theyâ€™re story engines. Click to expand.\n\n\n\n nycflights13: The DataSet That Took Off\n\nWhat it is: All flights departing NYC in 2013, linked to weather, airlines, and airports.\nWhy itâ€™s worthy: Rich joins (flights â‡„ weather â‡„ carriers) + time series + networks.\nStory sparks: Morning fog as nationwide dominoes; how buffer time beats chronic delay; the â€œgeography of luckâ€ in departure times.\n\n\n\n\n The Gapminder Dataset â€” A History of Human Progress, Told Through Data\n\nWhat it is: Country-level health, wealth, and population over time.\nWhy itâ€™s worthy: Long horizons + comparable units = clean cross-country narratives.\nStory sparks: Convergence vs.Â divergence; health gains without wealth; â€œlate bloomersâ€ that flip the script.\n\n\n\n\nLahman Baseball Database â€” A Chronicle of the Gameâ€™s Digital Memory\n\nWhat it is: Player/team stats from the 19th century to now.\nWhy itâ€™s worthy: Deep history, rule changes, moneyball erasâ€”quant + culture.\nStory sparks: Expansion, integration, and the long-ball epochs; globalization in surnames and birthplaces.\n\n\n\n\nğŸŒ¦ï¸  NOAA Daily Weather â€” Memory of the Sky\n\nWhat it is: Decades of daily observations for thousands of stations (GHCN).\nWhy itâ€™s worthy: Local variability meets climate trends; perfect for â€œweather memoryâ€ ideas.\nStory sparks: How yesterday biases today (humans & grids); asymmetries in extremes.\n\n\n\n\nğŸš‡ Transport for London (TfL) â€” The Pulse Beneath the City\n\nWhat it is: Open ridership, delays, closures, and more from London Underground.\nWhy itâ€™s worthy: Network effects + peak waves = emergent patterns.\nStory sparks: Fragility maps; how small failures ripple; the true â€œrushâ€ in rush hour.\n\n\n\n\nğŸ›ï¸ UCI Online Retail â€” Human Behavior in Transactions\n\nWhat it is: Line-item invoices for a UK e-commerce retailer (2010â€“2011).\nWhy itâ€™s worthy: Basket analysis, seasonality, long-tail dynamics.\nStory sparks: Product co-occurrence graphs; rare combos that drive profit; return behavior arcs.\n\n\n\n\nğŸ’° FRED Economic Data â€” The Pulse of Economies\n\nWhat it is: US/international macro time series (inflation, jobs, rates).\nWhy itâ€™s worthy: High-frequency, consequential, and composable.\nStory sparks: Recession â€œfingerprintsâ€; sectoral divergence; policy shock timelines.\n\n\n\n\nğŸ›°ï¸ NASA MODIS / Earth Observatory â€” A Planet in Pixels\n\nWhat it is: Satellite observations (vegetation, fires, aerosols, ice).\nWhy itâ€™s worthy: Visual storytelling + measurable change.\nStory sparks: Greening/ browning cycles; fire seasons as migrating fronts; glacier retreat as lived time.\n\n\n\n\nğŸ•¹ï¸ Steam Games â€” Collective Taste in the Wild\n\nWhat it is: Game metadata, reviews, tags, and (often) playtime stats.\nWhy itâ€™s worthy: Social proof, hype cycles, networked preference landscapes.\nStory sparks: Genre constellations; the life cycle of hits; price elasticity vs.Â review sentiment.\n\n\n\n\nğŸŒ† Global Urban Footprint â€” Cities from Space\n\nWhat it is: High-resolution, satellite-derived urban extent worldwide.\nWhy itâ€™s worthy: Comparable geometry of cities across cultures and terrains.\nStory sparks: Edge growth vs.Â infill; coastal constraints; night-lights vs.Â census discrepancies."
  },
  {
    "objectID": "datasets/the-noaa-daily-weather-dataset.html",
    "href": "datasets/the-noaa-daily-weather-dataset.html",
    "title": "ğŸŒ¦ï¸ The NOAA Daily Weather Dataset",
    "section": "",
    "text": "The NOAA Daily Weather dataset, curated by the National Centers for Environmental Information (NCEI), is one of the worldâ€™s most detailed archives of day-by-day weather. It preserves the highs, lows, and patterns of the planetâ€™s pulse â€” turning routine weather into long-term climate memory."
  },
  {
    "objectID": "datasets/the-noaa-daily-weather-dataset.html#a-planets-daily-logbook",
    "href": "datasets/the-noaa-daily-weather-dataset.html#a-planets-daily-logbook",
    "title": "ğŸŒ¦ï¸ The NOAA Daily Weather Dataset",
    "section": "ğŸª¶ A Planetâ€™s Daily Logbook",
    "text": "ğŸª¶ A Planetâ€™s Daily Logbook\nIf the Earth kept a diary, the NOAA Daily Weather dataset would be its journal â€” quietly recording what happened in the sky each day, everywhere.\nFrom sweltering July afternoons in Texas to silent January snow in Maine, each entry captures our atmosphereâ€™s daily rhythm.\nAnd collectively, they reveal the long memory of weather that shapes our lives."
  },
  {
    "objectID": "datasets/the-noaa-daily-weather-dataset.html#origins-from-observers-to-databases",
    "href": "datasets/the-noaa-daily-weather-dataset.html#origins-from-observers-to-databases",
    "title": "ğŸŒ¦ï¸ The NOAA Daily Weather Dataset",
    "section": "ğŸ§­ Origins â€” From Observers to Databases",
    "text": "ğŸ§­ Origins â€” From Observers to Databases\nThe story begins in the late 19th century.\nLong before satellites and sensors, weather was measured by hand â€” Cooperative Observer Program (COOP) volunteers using thermometers and rain gauges.\nIn 1890, the U.S. Weather Bureau (now the National Weather Service) organized these thousands of observers to provide a daily national picture.\nBy the 1930s, these logs were centralized under what became the National Climatic Data Center (NCDC) â€” and eventually NOAAâ€™s National Centers for Environmental Information (NCEI).\nTheir mission was simple but monumental: preserve the planetâ€™s weather memory.\nToday, the result is the Global Historical Climatology Network â€“ Daily (GHCN-Daily), containing billions of daily records from over 100,000 land-based stations across 180 countries.\n\nâ€œClimate is what you expect. Weather is what you get.â€ â€” Robert Heinlein\nThis dataset captures both."
  },
  {
    "objectID": "datasets/the-noaa-daily-weather-dataset.html#what-it-contains",
    "href": "datasets/the-noaa-daily-weather-dataset.html#what-it-contains",
    "title": "ğŸŒ¦ï¸ The NOAA Daily Weather Dataset",
    "section": "ğŸŒ What It Contains",
    "text": "ğŸŒ What It Contains\n\n\n\n\n\n\n\n\nElement\nDescription\nExample Variable\n\n\n\n\nTemperature\nDaily maximum & minimum (Â°C or Â°F)\nTMAX, TMIN\n\n\nPrecipitation\nTotal daily precipitation\nPRCP\n\n\nSnowfall\nDaily snow accumulation\nSNOW\n\n\nSnow Depth\nDepth of snow on the ground\nSNWD\n\n\n\nEach station reports one row per day â€” a simple rhythm of measurements stretching back, in some places, over 150 years.\nNOAA also provides derived gridded versions, such as nClimGrid-Daily, mapping these station observations onto a fine grid across the contiguous U.S. from 1951 to present, enabling spatial analyses of heatwaves, droughts, or flood events."
  },
  {
    "objectID": "datasets/the-noaa-daily-weather-dataset.html#why-it-matters",
    "href": "datasets/the-noaa-daily-weather-dataset.html#why-it-matters",
    "title": "ğŸŒ¦ï¸ The NOAA Daily Weather Dataset",
    "section": "ğŸ’¡ Why It Matters",
    "text": "ğŸ’¡ Why It Matters\nThe NOAA Daily Weather dataset is more than just numbers â€” itâ€™s context.\nEvery datapoint tells part of a larger story about life on Earth.\n\nTime Depth: Enables century-scale climate studies â€” tracking heatwaves, cold snaps, and rainfall shifts.\nDaily Resolution: Lets us see change as it happens â€” from one sunrise to the next.\nGeographic Breadth: A single, unified record spanning the globeâ€™s landmasses.\nQuality-Controlled: Built with decades of standardization and statistical checks.\nOpen Access: Available freely to the public via NOAAâ€™s Climate Data Online (CDO).\n\nFrom farmers planning crops to engineers modeling heat stress on power grids â€” to storytellers like us connecting data to meaning â€” the dataset quietly underpins much of what we know about weather and climate."
  },
  {
    "objectID": "datasets/the-noaa-daily-weather-dataset.html#what-makes-it-unique",
    "href": "datasets/the-noaa-daily-weather-dataset.html#what-makes-it-unique",
    "title": "ğŸŒ¦ï¸ The NOAA Daily Weather Dataset",
    "section": "ğŸ§© What Makes It Unique",
    "text": "ğŸ§© What Makes It Unique\n\nItâ€™s the everyday data that becomes history.\nWhile headlines capture hurricanes, this dataset records the calm before the storm â€” the slow shifts that define climate.\nIt connects disciplines.\nWeather data intersects naturally with agriculture, transportation, energy, health, and even economics.\nFor instance, in ğŸ¦‹ Butterfly Morning Delays story, flight delays could easily be paired with daily precipitation and wind records.\nItâ€™s global, yet personal.\nBecause itâ€™s station-based, you can zoom from the planetary to the hyperlocal â€” even â€œmy town on my birthday.â€\nItâ€™s science you can touch.\nEach number once came from a real thermometer, a real place, a real moment in time."
  },
  {
    "objectID": "datasets/the-noaa-daily-weather-dataset.html#example-exploration",
    "href": "datasets/the-noaa-daily-weather-dataset.html#example-exploration",
    "title": "ğŸŒ¦ï¸ The NOAA Daily Weather Dataset",
    "section": "ğŸ§® Example Exploration",
    "text": "ğŸ§® Example Exploration\nImagine this:\nYou download 50 years of daily maximum temperatures for your city and plot how many summer days exceeded 95 Â°F each year.\nYouâ€™d likely see the creeping signature of a warming world.\nOr perhaps you cross-match rainfall with flight delays, crop yields, or local power consumption.\nSuddenly, weather becomes behavioral data â€” showing how the sky shapes our choices below.\n# Example R snippet for NOAA Daily Summaries\nlibrary(rnoaa)\nlibrary(dplyr)\n\n# Pull last 30 years of daily summaries for a station\nghcnd_stations() |&gt; \n  filter(grepl(\"LITTLE ROCK\", name)) |&gt; \n  slice_head(n = 1) |&gt; \n  pull(id) -&gt; station_id\n\ndata &lt;- meteo_pull_monitors(station_id, date_min = \"1995-01-01\", var = c(\"TMAX\", \"TMIN\", \"PRCP\"))\n\ndata |&gt; \n  mutate(year = lubridate::year(date)) |&gt;\n  group_by(year) |&gt;\n  summarise(hot_days = sum(TMAX &gt; 35, na.rm = TRUE)) |&gt;\n  ggplot(aes(year, hot_days)) +\n  geom_line() +\n  labs(title = \"Number of Hot Days (TMAX &gt; 35Â°C) per Year\",\n       y = \"Count\", x = \"Year\")\nThis small snippet turns daily data into an evolving climate portrait."
  },
  {
    "objectID": "datasets/the-noaa-daily-weather-dataset.html#references-credits",
    "href": "datasets/the-noaa-daily-weather-dataset.html#references-credits",
    "title": "ğŸŒ¦ï¸ The NOAA Daily Weather Dataset",
    "section": "ğŸ“š References & Credits",
    "text": "ğŸ“š References & Credits\n\nNOAA/NCEI â€” Global Historical Climatology Network â€“ Daily (GHCN-Daily)\nNOAA/NCEI â€” nClimGrid-Daily\nClimate.gov â€” Daily Temperature and Precipitation Reports\nNOAA Climate Data Online (CDO) â€” Access Portal\nCooperative Observer Program (COOP) â€” NOAA History\nHistorical Context â€” National Climatic Data Center (NCDC)"
  },
  {
    "objectID": "datasets/the-gapminder-dataset-a-history-of-human-progress-told-through-data.html#references-credits",
    "href": "datasets/the-gapminder-dataset-a-history-of-human-progress-told-through-data.html#references-credits",
    "title": "The Gapminder Dataset â€” A History of Human Progress, Told Through Data",
    "section": "ğŸ“š References & Credits",
    "text": "ğŸ“š References & Credits\n\nRosling, H. (2006). The best stats youâ€™ve ever seen [TED Talk]. https://www.ted.com/talks/hans_rosling_the_best_stats_you_ve_ever_seen\n\nGapminder Foundation. (2024). About Gapminder. https://www.gapminder.org/about\n\nRosling, H., Rosling, O., & Rosling RÃ¶nnlund, A. (2018). Factfulness: Ten Reasons Weâ€™re Wrong About the Worldâ€”and Why Things Are Better Than You Think. Flatiron Books.\n\nUnited Nations Data Division (UNdata). Life Expectancy and GDP per Capita Statistics.\n\nWorld Bank Open Data. World Development Indicators.\n\nBryan, J., Wickham, H., & RStudio Team. (2017). gapminder: Data from Gapminder. R package version 0.3.0.\n\nRosling, O., & RÃ¶nnlund, A. (2023). Gapminder World Tools and Dollar Street Visual Database. https://www.gapminder.org/tools"
  },
  {
    "objectID": "datasets/index.html",
    "href": "datasets/index.html",
    "title": "Featured Datasets",
    "section": "",
    "text": "ğŸ“Š Featured Datasets â€” Where Stories Begin\nBehind every story lies a dataset waiting to be understood.\nExplore the origins, purpose, and structure of the datasets that fuel Insightful Tales.\nEach entry links to its background page â€” describing how the data was collected, why it matters, and what stories it can tell.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "data-science/genesis-of-data.html",
    "href": "data-science/genesis-of-data.html",
    "title": "The Genesis of Data â€” When Counting Became Knowing",
    "section": "",
    "text": "Where numbers first became mirrors of the world.\nWe like to imagine data as a modern concept â€” CSV files, databases, machine learning pipelines. But the truth is: data is as old as civilization itself. Before there were scientists, there were record-keepers. Before there were models, there were marks â€” clay scratches, tally sticks, and census scrolls â€” all born from the same instinct: to remember what mattered.\nThe Babylonians were counting grain before they were writing poems. Their cuneiform tablets, pressed by reeds into wet clay, werenâ€™t stories â€” they were inventories. It was there, in those humble lists of barley and livestock, that data first became a language of power. Whoever held the counts held control â€” of taxes, of armies, of life itself.\nBy the time the Romans perfected their census, data had become governance. â€œCensereâ€ meant â€œto assess,â€ and that act of assessment â€” of converting people into numbers â€” laid the foundation for everything from imperial logistics to modern public policy. The Census wasnâ€™t invented by democracy; democracy was, in part, made possible because we learned to count ourselves."
  },
  {
    "objectID": "data-science/genesis-of-data.html#the-spiritual-act-of-counting",
    "href": "data-science/genesis-of-data.html#the-spiritual-act-of-counting",
    "title": "The Genesis of Data â€” When Counting Became Knowing",
    "section": "ï¸ğŸ”¢ The Spiritual Act of Counting",
    "text": "ï¸ğŸ”¢ The Spiritual Act of Counting\nI often think about counting not as a mechanical act, but a spiritual one â€” a human attempt to find order in chaos. Every tally is a quiet declaration that something exists, that it matters enough to be counted.\nFlorence Nightingale understood this profoundly. When she gathered data on deaths in the Crimean War, she wasnâ€™t collecting for curiosityâ€™s sake. She was fighting ignorance with numbers, transforming rows and columns into a moral argument â€” one so compelling that even the Queen could not ignore it. In that sense, she pioneered not just data visualization but data empathy."
  },
  {
    "objectID": "data-science/genesis-of-data.html#when-the-world-scaled-up",
    "href": "data-science/genesis-of-data.html#when-the-world-scaled-up",
    "title": "The Genesis of Data â€” When Counting Became Knowing",
    "section": "ğŸŒ When the World Scaled Up",
    "text": "ğŸŒ When the World Scaled Up\nFast forward to the Victorian era: the world began to count everything. The first statistical societies formed, censuses became national events, and the idea of â€œobjective truth through measurementâ€ took hold. The machine age demanded data â€” and humanity obliged.\nThe story of data from then onward is exponential. By 1900, nearly every industrialized nation had institutionalized statistical offices. By 1950, computers were born to count faster than any human could. By 2000, data had multiplied to such a degree that it began describing us more than we described it."
  },
  {
    "objectID": "data-science/genesis-of-data.html#the-data-of-data",
    "href": "data-science/genesis-of-data.html#the-data-of-data",
    "title": "The Genesis of Data â€” When Counting Became Knowing",
    "section": "ğŸ“Š The Data of Data",
    "text": "ğŸ“Š The Data of Data\nI find something poetic in tracing this lineage with modern tools. Using the World Bankâ€™s historical population dataset, we can visualize how humanityâ€™s capacity to count itself expanded almost in lockstep with its population.\n\n\n\n\n\n\nTip\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(WDI)\nlibrary(janitor)\n\nowid &lt;- get_owid_world_pop()\n\n# --- WDI (1960+) ---\nwdi_raw &lt;- try(\n  WDI(indicator = \"SP.POP.TOTL\",\n      start = 1960,\n      end   = as.integer(format(Sys.Date(), \"%Y\"))),\n  silent = TRUE\n)\n\nif (inherits(wdi_raw, \"try-error\")) {\n  wdi_world &lt;- tibble(year = integer(), pop = numeric())\n} else {\n  # Filter to the global aggregate\n  wdi_world &lt;- wdi_raw %&gt;%\n    filter(iso2c == \"1W\" | country == \"World\") %&gt;%\n    select(year, pop = SP.POP.TOTL)\n}\n\n\n# --- OWID historical (ensure correct columns + units) ---\n\ntmp &lt;- owid %&gt;%\nclean_names() %&gt;%\nrename(pop = population)\n\nif (\"entity\" %in% names(tmp) && any(tolower(tmp$entity) == \"world\")) {\nowid_world &lt;- tmp %&gt;%\nfilter(tolower(entity) == \"world\", !is.na(year), !is.na(pop)) %&gt;%\ndistinct(year, .keep_all = TRUE)\n} else {\n\n# Fallback: aggregate across all entities per year\n\nowid_world &lt;- tmp %&gt;%\ngroup_by(year) %&gt;%\nsummarise(pop = sum(pop, na.rm = TRUE), .groups = \"drop\")\n}\n\n# Normalize OWID units if provided in millions\n\nif (max(owid_world$pop, na.rm = TRUE) &lt; 1e6) {\nowid_world &lt;- owid_world %&gt;% mutate(pop = pop * 1e6)\n}\n\nggplot() +\ngeom_area(data = owid_world, aes(year, pop / 1e9), alpha = 0.2) +\ngeom_line(data = wdi_world,  aes(year, pop / 1e9)) +\ngeom_vline(xintercept = 1960, linetype = \"dotted\") +\nannotate(\n\"text\",\nx = 1961,\ny = max(c(owid_world$pop, wdi_world$pop), na.rm = TRUE) / 1e9 * 0.9,\nlabel = \"Official WDI begins (1960)\",\nhjust = 0, size = 3\n) +\nlabs(\ntitle = \"World Population: 1800â€“present\",\nsubtitle = \"Historical (shaded, OWID/Maddison) with modern WDI overlay (solid line)\",\nx = NULL, y = \"Billions\"\n)\n\n\n\n\n\n\n\n\nFigureÂ 1: World population since 1800, with modern WDI overlay\n\n\n\n\n\n\n\n\nMethods Note. Historical population is sourced from OWID/Maddison where available; modern population (1960â€“present) uses World Bank WDI SP.POP.TOTL. If the build environment lacks internet, a small offline fallback ensures the chart still renders.\n\nOverlay that with Our World in Dataâ€™s historical metrics, and you see an even deeper pattern: as soon as we can measure something, we seek to improve it. Counting life expectancy didnâ€™t just describe longevity â€” it helped extend it. Tracking literacy didnâ€™t just record education â€” it democratized it.\nThis is the unseen virtue of data: it doesnâ€™t just observe reality, it amplifies intention."
  },
  {
    "objectID": "data-science/genesis-of-data.html#when-counting-became-knowing",
    "href": "data-science/genesis-of-data.html#when-counting-became-knowing",
    "title": "The Genesis of Data â€” When Counting Became Knowing",
    "section": "ğŸ”¢ When Counting Became Knowing",
    "text": "ğŸ”¢ When Counting Became Knowing\nThe modern data scientist, in some sense, is a direct descendant of the temple scribe.\nWe still look for structure in chaos, still seek to compress the infinite world into finite symbols. The clay tablets became spreadsheets; the grain records became gigabytes. But the human motive remains unchanged: to understand, to predict, to improve.\nWe often hear that data is â€œthe new oil.â€ I donâ€™t think thatâ€™s true. Oil is extracted, consumed, and gone. Data, when treated properly, is more like light â€” it illuminates. The Babyloniansâ€™ grain counts, Nightingaleâ€™s diagrams, Tukeyâ€™s exploratory plots â€” all were small sparks in a long illumination that continues through every dataset we open today."
  },
  {
    "objectID": "data-science/genesis-of-data.html#a-living-timeline",
    "href": "data-science/genesis-of-data.html#a-living-timeline",
    "title": "The Genesis of Data â€” When Counting Became Knowing",
    "section": "ğŸŒ±A Living Timeline",
    "text": "ğŸŒ±A Living Timeline\nIâ€™m building an animated time-series visualization that traces this evolution: each point represents the earliest known record of a nationâ€™s statistical data â€” census, agricultural, financial, environmental.\n\n\n\n\n\n\nNote\n\n\n\n\n\nCode\n# Example placeholder visualization\n\nset.seed(42)\ndf &lt;- tibble(\n        year = rep(1800:2020, each = 1),\n        countries_reporting = pmin(\n            200, round(5 + (year - 1800)^1.3 / 3000)\n        )\n    )\n\nggplot(df, aes(year, countries_reporting)) + \n    geom_line(linewidth = 1.2, color = \"#0072B2\") + \n    labs(title = \"A Living Timeline of Data\", subtitle = \"How measurement spread across nations (1800â€“2020)\",y = \"Countries with available data\", x = NULL)\n\n\n\n\n\n\n\n\nFigureÂ 2: Data density over time â€” number of countries recording key statistics\n\n\n\n\n\n\n\nThe story the data tells is breathtaking: we are a species that counts because we care."
  },
  {
    "objectID": "data-science/genesis-of-data.html#epilogue-the-infinite-ledger",
    "href": "data-science/genesis-of-data.html#epilogue-the-infinite-ledger",
    "title": "The Genesis of Data â€” When Counting Became Knowing",
    "section": "â™¾ï¸ Epilogue: The Infinite Ledger",
    "text": "â™¾ï¸ Epilogue: The Infinite Ledger\nWe are, each of us, a new line in the worldâ€™s oldest dataset â€” the ongoing attempt to make sense of being alive.\nThe Genesis of Data is not a past event; itâ€™s a continuous present.\nEvery number we collect is a reflection of what we value, and every dataset we publish is a quiet hope that someone, somewhere, might find meaning in it.\nData, at its most human level, is not about numbers. Itâ€™s about noticing."
  },
  {
    "objectID": "data-science/genesis-of-data.html#references",
    "href": "data-science/genesis-of-data.html#references",
    "title": "The Genesis of Data â€” When Counting Became Knowing",
    "section": "ğŸ“š References",
    "text": "ğŸ“š References\n\nWorld Bank Historical Population Data (1800â€“present)\nOur World in Data â€” History of Statistics\nUN Data Portal: National Census Records Archive"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Insightful Tales",
    "section": "",
    "text": "ğŸ¦‹ Insightful Tales is a creative data storytelling project that transforms datasets into visual, narrative insights.\nEvery story is built from data â€” but told like literature, not statistics."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Iâ€™d love to hear from readers, researchers, or collaborators interested in data storytelling, visualization, or related projects.\n\nğŸ’¡ Feedback or story ideas? Drop a note below.\n\nğŸ§  Collaboration inquiries? Iâ€™m open to cross-disciplinary data projects.\n\nğŸ‘‰ [LinkedIn Profile](https://www.linkedin.com/in/joseph-schaffer-a6270138\nğŸ‘‰ GitHub Repository\nOr email me directly: joseph [at] insightfultales [dot] com"
  },
  {
    "objectID": "contact.html#get-in-touch",
    "href": "contact.html#get-in-touch",
    "title": "Contact",
    "section": "",
    "text": "Iâ€™d love to hear from readers, researchers, or collaborators interested in data storytelling, visualization, or related projects.\n\nğŸ’¡ Feedback or story ideas? Drop a note below.\n\nğŸ§  Collaboration inquiries? Iâ€™m open to cross-disciplinary data projects.\n\nğŸ‘‰ [LinkedIn Profile](https://www.linkedin.com/in/joseph-schaffer-a6270138\nğŸ‘‰ GitHub Repository\nOr email me directly: joseph [at] insightfultales [dot] com"
  },
  {
    "objectID": "data-science/index.html",
    "href": "data-science/index.html",
    "title": "Data Science Stories",
    "section": "",
    "text": "ğŸ§® Data Science Stories\nThese are the analytical backbones of Insightful Tales â€”\nnotebooks that explore datasets, test ideas, and illuminate patterns that later become stories.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html#a-dataset-born-in-the-skies-above-new-york",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html#a-dataset-born-in-the-skies-above-new-york",
    "title": "âœˆnycflights13: The Dataset That Took Off",
    "section": "ğŸ—½ A Dataset Born in the Skies Above New York",
    "text": "ğŸ—½ A Dataset Born in the Skies Above New York\nIn 2013, nearly 337,000 flights departed from New York Cityâ€™s three major airports â€” JFK, LaGuardia (LGA), and Newark (EWR).\nEach of those flights left behind a trace: departure times, delays, destinations, aircraft numbers, even weather conditions at takeoff.\nYears later, these traces would find their way into a small, elegantly crafted R package called nycflights13, and that package would go on to become one of the most iconic learning tools in modern data science."
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html#the-origins-a-teaching-tool-with-real-world-grit",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html#the-origins-a-teaching-tool-with-real-world-grit",
    "title": "âœˆnycflights13: The Dataset That Took Off",
    "section": "ğŸ§­ The Origins: A Teaching Tool With Real-World Grit",
    "text": "ğŸ§­ The Origins: A Teaching Tool With Real-World Grit\nThe story begins with Hadley Wickham, the statistician and software developer who spearheaded much of the tidyverse â€” a family of R packages that revolutionized how data is handled and visualized.\nIn the early 2010s, Wickham was building resources to help people learn data science by doing, not by reading about artificially perfect examples. Real data is messy, relational, and full of missing values â€” and thatâ€™s exactly what students and practitioners need to experience.\nSo instead of another toy dataset, Wickham turned to the U.S. Department of Transportationâ€™s Bureau of Transportation Statistics (BTS), which publishes open flight-on-time data.\nFrom this raw source â€” hundreds of thousands of records across dozens of fields â€” he curated a single-year extract: all flights departing from the New York City area during 2013.\nThen, he went further.\nHe bundled in extra context:\n\nAirlines, mapping carrier codes to their full names.\nAirports, with geographic coordinates and time zones.\nPlanes, including tail numbers, manufacturers, and years built.\nWeather, hourly reports from each NYC airport.\n\nThe result wasnâ€™t just a dataset â€” it was a miniature world of interlocking tables, ready to teach real-world data relationships."
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html#a-dataset-of-relationships",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html#a-dataset-of-relationships",
    "title": "âœˆnycflights13: The Dataset That Took Off",
    "section": "ğŸ§© A Dataset of Relationships",
    "text": "ğŸ§© A Dataset of Relationships\nUnlike simpler datasets like iris or mtcars, nycflights13 is relational â€” meaning its tables connect through shared keys:\n\nFlights â†”ï¸ Airlines (via carrier code)\nFlights â†”ï¸ Airports (via origin and destination codes)\nFlights â†”ï¸ Planes (via tail number)\nFlights â†”ï¸ Weather (via origin + date + hour)\n\nThis structure was intentional. It allows learners to practice joining, grouping, filtering, and summarizing â€” the bread and butter of data science â€” within a single coherent story: why do some flights arrive late, and what factors might explain it?\nWickham described this as â€œa dataset that lets you learn the grammar of data manipulation using real noise, not just clean textbook numbers.â€"
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html#why-2013",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html#why-2013",
    "title": "âœˆnycflights13: The Dataset That Took Off",
    "section": "ğŸ§® Why 2013?",
    "text": "ğŸ§® Why 2013?\nThe year 2013 wasnâ€™t random â€” it was chosen for completeness and modern relevance.\nBy 2014, most of that yearâ€™s data had been validated and publicly available through the BTSâ€™s On-Time Performance database. It also represented a year with consistent reporting across all major U.S. carriers.\n2013 was far enough into the modern airline data era (with GPS and electronic filing) to be reliable, yet recent enough to feel relevant for todayâ€™s analyses."
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html#from-raw-data-to-a-clean-package",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html#from-raw-data-to-a-clean-package",
    "title": "âœˆnycflights13: The Dataset That Took Off",
    "section": "ğŸŒ¦ï¸ From Raw Data to a Clean Package",
    "text": "ğŸŒ¦ï¸ From Raw Data to a Clean Package\nTo transform the messy BTS export into something usable, Wickham and the tidyverse team wrote R scripts to:\n\nParse CSVs from the RITA (Research and Innovative Technology Administration) portal.\nFilter for flights with valid NYC origins.\nClean up time variables (like dep_time and arr_time) into numeric or datetime forms.\nNormalize column names for clarity and consistency.\nAdd weather data from the NOAA Integrated Surface Database (ISD).\n\nThe result was a compact, easily loadable dataset: one command, library(nycflights13), and the world of 2013â€™s New York skies appears at your fingertips."
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html#joining-the-tidyverse",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html#joining-the-tidyverse",
    "title": "âœˆnycflights13: The Dataset That Took Off",
    "section": "ğŸ“¦ Joining the tidyverse",
    "text": "ğŸ“¦ Joining the tidyverse\nWhen it was first released in 2014, nycflights13 joined the ecosystem of demonstration datasets used in the new generation of R tutorials, books, and courses â€” particularly R for Data Science, first published in 2017.\nBy packaging the dataset, Wickham made it instantly reproducible and accessible:\ninstall.packages(\"nycflights13\")\nlibrary(nycflights13)\nIt quickly became the standard dataset for teaching data wrangling (dplyr), tidying (tidyr), and visualization (ggplot2).\n\nToday, it appears in classrooms, Coursera courses, Kaggle tutorials, and research introductions worldwide."
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html#why-it-matters",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html#why-it-matters",
    "title": "âœˆnycflights13: The Dataset That Took Off",
    "section": "ğŸŒ Why It Matters",
    "text": "ğŸŒ Why It Matters\nnycflights13 endures because itâ€™s a microcosm of real life.\nIt captures delay frustrations, winter weather, airline competition, and the randomness of human systems.\nIt also shows how separate data sources â€” flights, airports, weather, and aircraft â€” interconnect to form a coherent story.\nItâ€™s not about New York flights alone; itâ€™s about how we understand and model the world through data.\nIn the same way the iris dataset taught generations of statisticians about classification, nycflights13 has taught generations of data scientists about structure, relationships, and storytelling.\n\nğŸ”§ Structure at a Glance\n\n\n\n\n\n\n\n\n\nTable\nDescription\nRows\nKey columns\n\n\n\n\nflights\nEvery NYC departure in 2013\n336,776\nyear, month, day, dep_time, arr_time, carrier, tailnum, origin, dest\n\n\nairlines\nCarrier codes and full names\n16\ncarrier\n\n\nairports\nAll airports in the dataset\n1,458\nfaa\n\n\nplanes\nAircraft manufacturing info\n3,322\ntailnum\n\n\nweather\nHourly conditions for each airport\n26,115\norigin, year, month, day, hour\n\n\n\n\n\nğŸ“˜ Legacy and Influence\nThe datasetâ€™s success inspired a wave of similar projects:\n\nfivethirtyeight â€” datasets from FiveThirtyEight journalism pieces.gapminder â€” Hans Roslingâ€™s world development data.palmerpenguins â€” a modern, ecological replacement for iris.\n\nBut nycflights13 remains unique in how it captures an entire real-world system, cleanly and reproducibly, in just a few megabytes.\nItâ€™s still maintained under the tidyverse organization on GitHub, ensuring compatibility with modern R versions.\nFor many, it was the first real dataset they ever analyzed â€” and itâ€™s still the one they remember.\n\n\nâœˆï¸ In the End\nnycflights13 is not just about flights.\nItâ€™s about the journey of data â€” from messy public records to a structured, teachable story.\nItâ€™s about curiosity, reproducibility, and accessibility.\nAnd like the air traffic over New York, it continues to connect learners all over the world â€” one tidy join at a time.\n\n\nğŸ“š References\n\nWickham, H. (2014). nycflights13: Flights that departed NYC in 2013. R package version 0.1.\nCRAN R Project. nycflights13 package documentation. https://cran.r-project.org/web/packages/nycflights13/\nBureau of Transportation Statistics (BTS). On-Time Performance Data. https://transtats.bts.gov/DL_SelectFields.asp?T able_ID=236\nNOAA National Centers for Environmental Information. Integrated Surface Database (ISD).\nWickham, H., & Grolemund, G. (2017). R for Data Science. Oâ€™Reilly Media.\nTidyverse Team. nycflights13 source code repository. https://github.com/tidyverse/nycflights13"
  },
  {
    "objectID": "datasets/the-lahman-baseball-database-a-chronicle-of-the-games-digital-memory.html",
    "href": "datasets/the-lahman-baseball-database-a-chronicle-of-the-games-digital-memory.html",
    "title": "The Lahman Baseball Database â€” A Chronicle of the Gameâ€™s Digital Memory",
    "section": "",
    "text": "âš¾The Lahman Baseball Database â€” A Chronicle of the Gameâ€™s Digital Memory\n\n\nFrom paper box scores to open data, how one project preserved baseballâ€™s statistical soul.\n\n\nExploring the origins, history, and significance of the Lahman Baseball Database â€” a dataset that bridges baseballâ€™s past and data scienceâ€™s future.\n\n\n\nğŸ§¾ Origins: From Box Scores to Bytes\nBefore baseball had Statcast, WAR, or machine-tracked pitch velocities, there were box scores â€” neat little tables printed in newspapers, carefully summarizing who hit what and when. These werenâ€™t just numbers; they were the fingerprints of every game ever played.\nIn the early 1990s, Sean Lahman, a journalist and baseball historian, began a labor of love: turning this analog treasure into digital form. His mission was simple but revolutionary â€” to make baseballâ€™s statistical record freely available for everyone, not just researchers with access to proprietary archives.\nThe result was the Lahman Baseball Database, first released in 1995 as a Microsoft Access file distributed on CD-ROMs and early websites. Over time, it became one of the most trusted and comprehensive open datasets in sports history.\n\n\nğŸ•°ï¸ The Database That Evolved With the Game\nAt its core, the Lahman Database is a meticulous record of Major League Baseball (MLB) statistics dating back to 1871. Each table tells a different part of the story:\n\nBatting.csv: every hit, home run, and strikeout\n\nPitching.csv: earned runs, innings pitched, and strikeouts\n\nFielding.csv: defensive performance across positions\n\nTeams.csv and People.csv: linking players, franchises, and eras together\n\nBut its real brilliance lies in its relational structure â€” every player, team, and season connects through IDs and keys, making it a dream for database design students and data scientists alike.\nOver the decades, volunteers, statisticians, and historians have joined Lahman in expanding and refining the dataset. Today itâ€™s updated regularly, available in multiple formats (CSV, SQL, R data frames), and integrated into major data science packages â€” such as the Lahman package in Râ€™s tidyverse ecosystem.\n\n\nğŸ’¡ Why It Matters\nThe Lahman Database was a pioneer of open data in sports â€” long before â€œopen dataâ€ was a buzzword. It democratized baseball analytics, empowering everyone from high school students to professional sabermetricians.\nItâ€™s also a living archive. Through it, we can:\n\nRecreate the history of baseball season by season\n\nMeasure trends in player performance over 150 years\n\nExplore sociological patterns, such as integration and globalization of the sport\n\nTeach database normalization, SQL querying, and R data wrangling through real historical data\n\nWhen the Lahman dataset entered R, it found a new home in education. Countless courses and textbooks use it as the first playground for students learning data analysis. Itâ€™s not just a baseball dataset â€” itâ€™s a bridge between humanities and data science.\n\n\nâš™ï¸ From Database to Story Engine\nWhatâ€™s fascinating is how the Lahman Database transcends its origins. While it began as a record of numbers, it has become a platform for storytelling. Analysts and fans alike use it to investigate everything from â€œWho had the best rookie season ever?â€ to â€œHow did rule changes shape offensive output?â€\nWithin the context of Insightful Tales, this dataset embodies the spirit of â€œwhere data finds its narrative.â€ Every statistic â€” every RBI, every strikeout â€” is a clue in the broader story of how Americaâ€™s pastime evolved alongside technology, society, and the human quest for meaning through numbers.\n\n\nğŸŒ A Community Effort\nThe Lahman Database thrives because itâ€™s open. It has no corporate sponsor or hidden paywall â€” just a community that believes baseballâ€™s history belongs to everyone. It continues to be hosted and maintained on GitHub, where contributors help fix inconsistencies, add metadata, and refine accuracy using digitized historical sources like Retrosheet and Baseball-Reference.\nIts openness has inspired similar projects across other sports â€” from soccerâ€™s fbref datasets to basketballâ€™s BBRef and even Formula 1 archives. In that sense, Lahmanâ€™s vision didnâ€™t just preserve baseballâ€™s memory; it helped seed a movement.\n\n\nğŸ§­ The Philosophy of Preservation\nThereâ€™s something quietly philosophical about what the Lahman Database represents.\nIn a world where so much data is transient â€” scrolling feeds, ephemeral stories â€” this dataset is a monument to memory. It reminds us that keeping careful records is an act of respect: for players, fans, and for time itself.\nThrough rows and columns, it preserves the drama of 150 years of human effort â€” the strikeouts, the comebacks, the improbable heroes. Numbers, when cared for, become history.\n\n\nğŸ“š References & Credits\n\nLahman, S. (1995-present). The Lahman Baseball Database. Retrieved from https://www.seanlahman.com/baseball-archive/statistics/\n\nR Core Team. (2024). Lahman: Sean Lahman Baseball Database (R package). CRAN. https://cran.r-project.org/package=Lahman\n\nRetrosheet. (2024). Play-by-play Data Archive. https://www.retrosheet.org\n\nBaseball-Reference. (2024). Major League Statistics and History. https://www.baseball-reference.com\n\nWickham, H., & Grolemund, G. (2016). R for Data Science. Oâ€™Reilly Media.\n\n\n\nâ€œBaseball, more than any other sport, is numbers â€” but in Lahmanâ€™s database, the numbers tell stories.â€"
  },
  {
    "objectID": "datasets/transport-for-london-tfl-the-pulse-beneath-the-city.html",
    "href": "datasets/transport-for-london-tfl-the-pulse-beneath-the-city.html",
    "title": "Transport for London (TfL) â€” The Pulse Beneath the City",
    "section": "",
    "text": "A look into the heartbeat of London through TfLâ€™s open data â€” exploring Oyster and Contactless journeys, Tube entries and exits, and Cycle Hire usage to understand the patterns of a living metropolis."
  },
  {
    "objectID": "datasets/transport-for-london-tfl-the-pulse-beneath-the-city.html#visualising-station-flow",
    "href": "datasets/transport-for-london-tfl-the-pulse-beneath-the-city.html#visualising-station-flow",
    "title": "Transport for London (TfL) â€” The Pulse Beneath the City",
    "section": "Visualising Station Flow",
    "text": "Visualising Station Flow\n\n\nCode\ntube |&gt;\n    ggplot(aes(x = reorder(station, total), y = total)) +\n    geom_col() +\n    coord_flip() +\n    labs(x = NULL, y = \"Total entries + exits\", title = \"Londonâ€™s busiest stations\") +\n    theme_minimal()\n\n\n\n\n\nTop 10 busiest Tube stations by annual entries and exits."
  },
  {
    "objectID": "datasets/transport-for-london-tfl-the-pulse-beneath-the-city.html#mapping-cycle-hire-patterns",
    "href": "datasets/transport-for-london-tfl-the-pulse-beneath-the-city.html#mapping-cycle-hire-patterns",
    "title": "Transport for London (TfL) â€” The Pulse Beneath the City",
    "section": "Mapping Cycle Hire Patterns",
    "text": "Mapping Cycle Hire Patterns\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(ggspatial)\nlibrary(rosm)\n\n# --- Load CSV ---\ncycle &lt;- read_csv(\"../data/tfl/cycle-hire.csv\", show_col_types = FALSE)\n\n# --- Coordinate lookup (approximate) ---\ncoords &lt;- tribble(\n  ~start_station,           ~lon,    ~lat,\n  \"Waterloo Station\",       -0.113,  51.503,\n  \"King's Cross Station\",   -0.122,  51.530,\n  \"Victoria Station\",       -0.143,  51.495,\n  \"London Bridge Station\",  -0.087,  51.505,\n  \"Paddington Station\",     -0.177,  51.516,\n  \"Stratford Station\",       0.002,  51.543,\n  \"Canary Wharf Station\",   -0.020,  51.505,\n  \"Holborn Station\",        -0.118,  51.517,\n  \"Bank Station\",           -0.089,  51.513,\n  \"Westminster Station\",    -0.125,  51.500\n)\n\n# --- Join & summarise to points (WGS84) ---\ncycle_pts_wgs84 &lt;- cycle |&gt;\n  left_join(coords, by = \"start_station\") |&gt;\n  filter(!is.na(lon), !is.na(lat)) |&gt;\n  group_by(start_station, lon, lat) |&gt;\n  summarise(trips = n(), .groups = \"drop\") |&gt;\n  st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)\n\n# --- Compute a padded bbox from your data & transform to Web-Mercator (EPSG:3857) ---\nif (nrow(cycle_pts_wgs84) == 0) stop(\"No stations matched the coord lookup. Check start_station names.\")\ncycle_pts_3857 &lt;- st_transform(cycle_pts_wgs84, 3857)\n\n# bbox padding helper (10% padding each side)\npad_bbox &lt;- function(bb, pad = 0.10) {\n  x_pad &lt;- (bb[\"xmax\"] - bb[\"xmin\"]) * pad\n  y_pad &lt;- (bb[\"ymax\"] - bb[\"ymin\"]) * pad\n  c(xmin = bb[\"xmin\"] - x_pad,\n    xmax = bb[\"xmax\"] + x_pad,\n    ymin = bb[\"ymin\"] - y_pad,\n    ymax = bb[\"ymax\"] + y_pad)\n}\nbb_3857 &lt;- st_bbox(cycle_pts_3857)\nbb_3857 &lt;- pad_bbox(bb_3857, pad = 0.12)\n\n# --- Plot ---\nggplot() +\n  annotation_map_tile(type = \"osm\", zoomin = 0, progress = \"none\", cachedir = tempdir()) +\n# halo (bigger, semi-transparent)\ngeom_sf(\n  data = cycle_pts_3857,\n  aes(size = trips),\n  color = \"yellow\",\n  alpha = 0.65,\n  size  = 4,        # fixed base size for the glow\n  show.legend = FALSE\n) +\n# core point (smaller, solid)\ngeom_sf(\n  data = cycle_pts_3857,\n  aes(size = trips),\n  shape = 21,\n  fill  = \"springgreen\",   # or \"cyan3\", \"springgreen3\"\n  color = \"yellow\",\n  stroke = 0.8,\n  alpha  = 1\n) +\n  coord_sf(xlim = c(bb_3857[\"xmin\"], bb_3857[\"xmax\"]),\n           ylim = c(bb_3857[\"ymin\"], bb_3857[\"ymax\"]),\n           expand = FALSE, crs = st_crs(3857)) +\n  annotation_scale(location = \"bl\", width_hint = 0.25, text_cex = 0.7) +\n  annotation_north_arrow(location = \"tr\", style = north_arrow_fancy_orienteering) +\n  theme_void() +\n  labs(title = \"Cycle Hire Start Stations (sample data on OSM tiles)\",\n       size = \"Trips\")"
  },
  {
    "objectID": "stories/butterfly-morning-delays.html",
    "href": "stories/butterfly-morning-delays.html",
    "title": "Butterfly Morning Delays",
    "section": "",
    "text": "A story about cause and consequence in motion â€” how a stormy sunrise or a missed departure can reshape an entire day of air travel.\nCode\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(nycflights13)\nlibrary(broom)\n\n# Minimal, clean theme\nquiet_theme &lt;- function() {\n  ggplot2::theme_minimal(base_size = 13) +\n    ggplot2::theme(\n      panel.grid.minor = element_blank(),\n      plot.title.position = \"plot\",\n      plot.caption.position = \"plot\",\n      strip.text = element_text(face = \"bold\")\n    )\n}"
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#data-definitions-why-these-choices",
    "href": "stories/butterfly-morning-delays.html#data-definitions-why-these-choices",
    "title": "Butterfly Morning Delays",
    "section": "ğŸ“Š Data & Definitions â€” Why these choices?",
    "text": "ğŸ“Š Data & Definitions â€” Why these choices?\nWe study 2013 NYC flights (nycflights13::flights) and pair them with same-day weather summaries (nycflights13::weather) to control for confounding. We define:\n\nFirst Wave: departures between 05:00â€“09:00 local.\nMorning variability (Ïƒâ‚): standard deviation of departure delay in the first wave (per originÃ—date).\nAfternoon outcome (Î¼â‚): mean arrival delay for flights departing after 12:00 (per originÃ—date).\n\nWhy SD for the morning? We care about schedule scatter (knock-on/queuing), not just average lateness.\n\n\nCode\n# Preprocess core variables\nfl &lt;- flights |&gt;\n  mutate(\n    dep_dt = make_datetime(year, month, day, dep_time %/% 100, dep_time %% 100),\n    hour = hour(dep_dt),\n    date = as_date(dep_dt),\n    dist_band = cut(\n      distance,\n      breaks = c(0, 500, 1500, Inf),\n      labels = c(\"Short (â‰¤500 mi)\", \"Medium (501â€“1500)\", \"Long (â‰¥1501)\"),\n      right = TRUE\n    )\n  ) |&gt;\n  filter(!is.na(dep_dt), !is.na(arr_delay), !is.na(dep_delay)) |&gt;\n  filter(origin %in% c(\"JFK\", \"LGA\", \"EWR\"))\n\n# Daily first-wave variability (Ïƒ1) and afternoon outcome (Î¼a)\ndaily &lt;- fl |&gt;\n  group_by(origin, date) |&gt;\n  summarise(\n    sigma_morning = sd(dep_delay[hour &gt;= 5 & hour &lt; 9], na.rm = TRUE),\n    n_morning     = sum(hour &gt;= 5 & hour &lt; 9, na.rm = TRUE),\n    mean_arr_pm   = mean(arr_delay[hour &gt;= 12], na.rm = TRUE),\n    n_pm          = sum(hour &gt;= 12, na.rm = TRUE),\n    .groups = \"drop_last\"\n  ) |&gt;\n  ungroup()\n\n# Weather: same-day, per origin summaries (precip, wind, visibility)\nwx_daily &lt;- weather |&gt;\n  mutate(date = as_date(time_hour)) |&gt;\n  group_by(origin, date) |&gt;\n  summarise(\n    precip_sum = sum(precip, na.rm = TRUE),\n    wind_mean  = mean(wind_speed, na.rm = TRUE),\n    vis_mean   = mean(visib, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\ndaily &lt;- daily |&gt;\n  left_join(wx_daily, by = c(\"origin\", \"date\")) |&gt;\n  mutate(\n    month = month(date),\n    wday  = wday(date, label = TRUE)\n  )"
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#scatter-in-the-first-wave-predicts-afternoon-pain",
    "href": "stories/butterfly-morning-delays.html#scatter-in-the-first-wave-predicts-afternoon-pain",
    "title": "Butterfly Morning Delays",
    "section": "ğŸŒŠ Scatter in the first wave predicts afternoon pain",
    "text": "ğŸŒŠ Scatter in the first wave predicts afternoon pain\nWhat we plot & why: A simple scatter of morning variability (x) vs.Â afternoon mean arrival delay (y) reveals the basic relationship; a smooth fit (loess) clarifies the average trend.\n\n\nCode\np1 &lt;- daily |&gt;\n  filter(n_morning &gt;= 10, n_pm &gt;= 20) |&gt;\n  ggplot(aes(sigma_morning, mean_arr_pm, color = origin)) +\n  geom_point(alpha = 0.35) +\n  geom_smooth(se = FALSE) +\n  labs(\n    title = \"Morning Schedule Scatter vs. Afternoon Arrival Delays\",\n    subtitle = \"Per originÃ—day in NYC, 2013 (loess trend).\",\n    x = \"First-wave SD of departure delay (minutes)\",\n    y = \"Afternoon mean arrival delay (minutes)\",\n    color = \"Origin\",\n    caption = \"Data: nycflights13 â€¢ Smooth: loess â€¢ SD uses 05:00â€“08:59 departures\"\n  ) +\n  quiet_theme()\np1\n\n\n\n\n\n\n\n\n\nWhat it implies: As first-wave scatter increases, afternoon arrival delays climb. This is consistent with queuing/turnaround knock-onâ€”a few badly delayed early flights can desynchronize crew/aircraft rotations.\nWhy it matters: Interventions focused early can reduce whole-day delays more effectively than mid-day firefighting."
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#short-haul-routes-amplify-the-ripple",
    "href": "stories/butterfly-morning-delays.html#short-haul-routes-amplify-the-ripple",
    "title": "Butterfly Morning Delays",
    "section": "âœˆï¸ Short-haul routes amplify the ripple",
    "text": "âœˆï¸ Short-haul routes amplify the ripple\nWhat we plot & why: We segment by distance band; short-haul flights cycle aircraft/crews more frequently, increasing propagation channels.\n\n\nCode\npm_by_band &lt;- fl |&gt;\n  mutate(am_wave = hour &gt;= 5 & hour &lt; 9,\n         pm_wave = hour &gt;= 12) |&gt;\n  group_by(origin, date, dist_band) |&gt;\n  summarise(\n    sigma_morning = sd(dep_delay[am_wave], na.rm = TRUE),\n    mean_arr_pm   = mean(arr_delay[pm_wave], na.rm = TRUE),\n    n_am = sum(am_wave, na.rm = TRUE),\n    n_pm = sum(pm_wave, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  filter(!is.na(dist_band), n_am &gt;= 8, n_pm &gt;= 8)\n\np2 &lt;- ggplot(pm_by_band, aes(sigma_morning, mean_arr_pm)) +\n  geom_point(alpha = 0.25) +\n  geom_smooth(se = FALSE) +\n  facet_wrap(~ dist_band) +\n  labs(\n    title = \"Propagation is Strongest on Short-Haul Days\",\n    subtitle = \"Per originÃ—date; first-wave SD vs. afternoon mean arrival delay.\",\n    x = \"First-wave SD (min)\",\n    y = \"Afternoon mean arrival delay (min)\"\n  ) +\n  quiet_theme()\np2\n\n\n\n\n\n\n\n\n\nImplication: Short routes act like â€œdelay multipliers.â€ Practical levers here include gate assignment discipline, turn-time buffers, and pushback sequencing in the first wave."
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#after-weather-controls-the-association-persists",
    "href": "stories/butterfly-morning-delays.html#after-weather-controls-the-association-persists",
    "title": "Butterfly Morning Delays",
    "section": "ğŸŒ¦ï¸ After weather controls, the association persists",
    "text": "ğŸŒ¦ï¸ After weather controls, the association persists\nWhat we fit & why: A linear model predicting afternoon mean arrival delay from first-wave SD, controlling for precipitation, wind, visibility, month and weekday (to partially absorb seasonality/peaks). This is associational, not causal.\n\n\nCode\nmod_data &lt;- daily |&gt;\n  filter(!is.na(mean_arr_pm), !is.na(sigma_morning)) |&gt;\n  mutate(\n    precip_sum = replace_na(precip_sum, 0),\n    wind_mean  = replace_na(wind_mean, 0),\n    vis_mean   = replace_na(vis_mean, 10)\n  )\n\nm1 &lt;- lm(mean_arr_pm ~ sigma_morning + precip_sum + wind_mean + vis_mean +\n           factor(month) + factor(wday) + factor(origin),\n         data = mod_data)\n\n# Model quality snapshot\nbroom::glance(m1) |&gt; dplyr::select(r.squared, adj.r.squared, sigma, nobs)\n\n\n\n  \n\n\n\nCode\n# Slope of interest\nbroom::tidy(m1) |&gt;\n  filter(term == \"sigma_morning\") |&gt;\n  mutate(interpret = paste0(\"~\", round(estimate, 2),\n                            \" min more afternoon delay per +1 min SD in first wave\"))\n\n\n\n  \n\n\n\nWhat it tells us: The coefficient on sigma_morning (typically positive) indicates that even after accounting for weather and calendar effects, noisier first waves align with worse afternoons.\n\n\n\n\n\n\nNote\n\n\n\nInterpretation: If the coefficient is, say, 0.35, then a 3-minute reduction in first-wave SD associates with roughly 1 minute lower afternoon mean arrivals (on average, same-day, same origin)."
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#a-tiny-what-if-simulation",
    "href": "stories/butterfly-morning-delays.html#a-tiny-what-if-simulation",
    "title": "Butterfly Morning Delays",
    "section": "ğŸ’­ A tiny â€œwhat-ifâ€ simulation",
    "text": "ğŸ’­ A tiny â€œwhat-ifâ€ simulation\nWhat we simulate & why: Suppose operations could shave 2 minutes of SD from the first wave (tightened sequencing, better gate prep). Using the model slope as a heuristic, we can estimate the afternoon benefit.\n\n\nCode\ncoef_sigma &lt;- broom::tidy(m1) |&gt;\n  filter(term == \"sigma_morning\") |&gt;\n  pull(estimate)\n\ndelta_sd &lt;- -2  # hypothetical tightening (minutes)\navg_effect &lt;- coef_sigma * delta_sd\n\ntibble(\n  hypothetical_sd_change_min = delta_sd,\n  predicted_change_pm_mean_min = avg_effect\n)\n\n\n\n  \n\n\n\nSo what: Even modest improvements early could meaningfully reduce afternoon averagesâ€”multiplied across hundreds of flights, that is real passenger time and crew/aircraft efficiency."
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#practical-applications",
    "href": "stories/butterfly-morning-delays.html#practical-applications",
    "title": "Butterfly Morning Delays",
    "section": "ğŸ§° Practical Applications",
    "text": "ğŸ§° Practical Applications\n\nCrew/Aircraft Rotations: Prioritize first-wave turn readiness (catering, fueling, pushback order) to minimize scatter.\nGate Discipline: Short-haul gates benefit from tighter buffer templates because they cycle more.\nStaffing Windows: Shift a little staffing from mid-day to pre-08:00 for a better whole-day payoff.\nPlaybooks for â€œNoisy Morningsâ€: When Ïƒâ‚ spikes, proactively re-sequence sensitive turns to break chains."
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#limitations-what-we-didnt-model",
    "href": "stories/butterfly-morning-delays.html#limitations-what-we-didnt-model",
    "title": "Butterfly Morning Delays",
    "section": "ğŸš§ Limitations (what we didnâ€™t model)",
    "text": "ğŸš§ Limitations (what we didnâ€™t model)\n\nThis is NYC-2013 only; airports differ in geometry/ATC regimes.\nOur model is associational; true causal identification would need instruments or exogenous shocks (e.g., runway closures).\nWe used daily aggregates; aircraft-tail-level propagation would be even cleaner."
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#summary-what-we-discovered",
    "href": "stories/butterfly-morning-delays.html#summary-what-we-discovered",
    "title": "Butterfly Morning Delays",
    "section": "ğŸ“‹ Summary (what we discovered)",
    "text": "ğŸ“‹ Summary (what we discovered)\n\nFirst-wave scatter (not just average delay) is a strong signal for afternoon performance.\nThe effect is strongest on short-haul daysâ€”where rotations are more frequent.\nControlling for weather/time effects, the relationship persists.\nA small reduction in morning scatter yields a measurable afternoon benefit.\n\n\n Dataset: nycflights13"
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#references-credits",
    "href": "stories/butterfly-morning-delays.html#references-credits",
    "title": "Butterfly Morning Delays",
    "section": "ğŸ“š References & Credits",
    "text": "ğŸ“š References & Credits\n\nData: nycflights13 (flights, weather) â€” Wickham et al.Â (CRAN)\nPackages: tidyverse, lubridate, broom, ggplot2\nAuthoring: Quarto\nReading: Wickham, R for Data Science (data wrangling & viz patterns); Quarto documentation\nRepro style: quiet_theme() in this doc (CC-0)\nLinks:\n\nhttps://cran.r-project.org/package=nycflights13\nhttps://r4ds.hadley.nz/\nhttps://quarto.org/ ```"
  },
  {
    "objectID": "stories/index.html",
    "href": "stories/index.html",
    "title": "All Stories",
    "section": "",
    "text": "ğŸ¦‹ All Stories\nExplore every data narrative published on Insightful Tales â€”\nfrom flight delays to weather memory and everything between.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "stories/weather-memory.html",
    "href": "stories/weather-memory.html",
    "title": "Weather Memory",
    "section": "",
    "text": "Bad weather can disrupt schedules, but does the pain linger after skies clear? This short data story explores whether todayâ€™s flight delays still carry the fingerprint of yesterdayâ€™s weather â€” a simple form of system memory."
  },
  {
    "objectID": "stories/weather-memory.html#goal-approach",
    "href": "stories/weather-memory.html#goal-approach",
    "title": "Weather Memory",
    "section": "ğŸ¯ Goal & approach",
    "text": "ğŸ¯ Goal & approach\nGoal. Test whether yesterdayâ€™s weather helps predict todayâ€™s flight delays at NYC airports, beyond the effect of todayâ€™s weather.\nMethod (high level). 1) Join flights with hourly weather, aggregate to a daily panel by airport; 2) create lagged weather features (yesterdayâ€™s precip, wind, visibility, temperature); 3) model todayâ€™s mean departure delay using both today and yesterday weather plus calendar controls; 4) visualize the patterns and airport differences; 5) run a small what-if experiment.\n\nâš™ï¸ Setup\n\n\nCode\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(nycflights13)\nlibrary(broom)\nlibrary(gt)\nlibrary(scales)\nlibrary(modelr)\nlibrary(plotly)  # interactivity\n\n# Use site-wide theme if present; otherwise a quiet fallback\nquiet_theme &lt;- if (exists(\"quiet_theme\")) quiet_theme else function() {\n  theme_minimal(base_size = 12) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title.position = \"plot\",\n      plot.caption.position = \"plot\",\n      plot.title = element_text(face = \"bold\"),\n      legend.position = \"bottom\"\n    )\n}"
  },
  {
    "objectID": "stories/weather-memory.html#build-a-daily-panel-flights-weather",
    "href": "stories/weather-memory.html#build-a-daily-panel-flights-weather",
    "title": "Weather Memory",
    "section": "ğŸ§± Build a daily panel: flights Ã— weather",
    "text": "ğŸ§± Build a daily panel: flights Ã— weather\nGoal â†’ Create a tidy daily dataset by airport (JFK/LGA/EWR) with delay outcomes and weather inputs.\n\n\nCode\n# Flights (2013) â†’ daily outcomes by origin\nfl_daily &lt;- flights %&gt;%\n  mutate(\n    date = make_date(year, month, day),\n    cancelled = is.na(dep_time)\n  ) %&gt;%\n  group_by(origin, date) %&gt;%\n  summarise(\n    n_flights      = n(),\n    cancel_rate    = mean(cancelled),\n    mean_dep_delay = mean(dep_delay[!cancelled], na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n# Weather (hourly) â†’ daily inputs by origin\nwx_daily &lt;- weather %&gt;%\n  mutate(date = as_date(time_hour)) %&gt;%\n  group_by(origin, date) %&gt;%\n  summarise(\n    precip_in = sum(replace_na(precip, 0)),      # inches\n    wind_mph  = mean(wind_speed, na.rm = TRUE),  # mph\n    visib_mi  = mean(visib, na.rm = TRUE),       # miles\n    temp_F    = mean(temp, na.rm = TRUE),        # Â°F\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    precip_mm = precip_in * 25.4,                # convert to mm\n    rain_day  = as.integer(precip_in &gt; 0)\n  )\n\n# Join and create 1-day lags per-airport\npanel_daily &lt;- fl_daily %&gt;%\n  inner_join(wx_daily, by = c(\"origin\",\"date\")) %&gt;%\n  arrange(origin, date) %&gt;%\n  group_by(origin) %&gt;%\n  mutate(\n    precip_mm_lag1 = lag(precip_mm, 1),\n    wind_mph_lag1  = lag(wind_mph, 1),\n    visib_mi_lag1  = lag(visib_mi, 1),\n    temp_F_lag1    = lag(temp_F, 1),\n    rain_yday      = lag(rain_day, 1),\n    dow   = wday(date, label = TRUE, abbr = TRUE),\n    month = month(date, label = TRUE, abbr = TRUE)\n  ) %&gt;%\n  ungroup()\n\n# Keep rows with both today and lagged weather present\npanel_model &lt;- panel_daily %&gt;%\n  drop_na(\n    mean_dep_delay,\n    precip_mm, wind_mph, visib_mi, temp_F,\n    precip_mm_lag1, wind_mph_lag1, visib_mi_lag1, temp_F_lag1\n  )"
  },
  {
    "objectID": "stories/weather-memory.html#first-look-does-delay-rise-with-yesterdays-rain",
    "href": "stories/weather-memory.html#first-look-does-delay-rise-with-yesterdays-rain",
    "title": "Weather Memory",
    "section": "ğŸ”ï¸ First look: does delay rise with yesterdayâ€™s rain?",
    "text": "ğŸ”ï¸ First look: does delay rise with yesterdayâ€™s rain?\n\n\nCode\np_scatter &lt;- panel_model %&gt;%\n  ggplot(aes(x = precip_mm_lag1, y = mean_dep_delay)) +\n  geom_point(alpha = 0.4, size = 1) +\n  geom_smooth(method = \"loess\", se = TRUE) +\n  facet_wrap(~ origin, nrow = 1) +\n  labs(\n    title = \"Weather memory: yesterday's rain vs. today's mean departure delay\",\n    x = \"Yesterday's precipitation (mm)\",\n    y = \"Today's mean departure delay (minutes)\",\n    caption = \"Dots are daily airport means in 2013; line = loess trend.\"\n  ) +\n  quiet_theme()\n\np_scatter\n\n\n\n\n\n\n\n\n\n\n\nCode\nsubplot &lt;- ggplotly(p_scatter, tooltip = c(\"x\", \"y\")) %&gt;%\n  layout(title = list(text = \"Weather memory: Yesterday's rain vs. today's delays\"))\n\nsubplot"
  },
  {
    "objectID": "stories/weather-memory.html#today-vs.-yesterday-contrasting-weather-states",
    "href": "stories/weather-memory.html#today-vs.-yesterday-contrasting-weather-states",
    "title": "Weather Memory",
    "section": "ğŸŒ¤ï¸ï¸ Today vs.Â yesterday: contrasting weather states",
    "text": "ğŸŒ¤ï¸ï¸ Today vs.Â yesterday: contrasting weather states\n\n\nCode\np_contrast &lt;- ggplot(panel_model, aes(precip_mm_lag1, precip_mm)) +\n  stat_summary_2d(aes(z = mean_dep_delay), bins = 20) +\n  facet_wrap(~ origin, nrow = 1) +\n  labs(\n    title = \"Today vs yesterday precipitation (colored by mean delay)\",\n    x = \"Yesterday precip (mm)\", y = \"Today precip (mm)\", fill = \"Mean delay (min)\"\n  ) +\n  quiet_theme()\n\np_contrast"
  },
  {
    "objectID": "stories/weather-memory.html#modeling-does-yesterdays-weather-matter-after-controls",
    "href": "stories/weather-memory.html#modeling-does-yesterdays-weather-matter-after-controls",
    "title": "Weather Memory",
    "section": "âš™ï¸ Modeling: does yesterdayâ€™s weather matter after controls?",
    "text": "âš™ï¸ Modeling: does yesterdayâ€™s weather matter after controls?\n\n\nCode\nmodel_full &lt;- lm(\n  mean_dep_delay ~ precip_mm_lag1 + wind_mph_lag1 + visib_mi_lag1 + temp_F_lag1 +\n    precip_mm + wind_mph + visib_mi + temp_F + origin + dow + month,\n  data = panel_model\n)\n\ntidy_coefs &lt;- broom::tidy(model_full, conf.int = TRUE) %&gt;%\n  filter(term %in% c(\"precip_mm_lag1\",\"wind_mph_lag1\",\"visib_mi_lag1\",\"temp_F_lag1\")) %&gt;%\n  mutate(term = recode(term,\n    precip_mm_lag1 = \"Yesterday precip (mm)\",\n    wind_mph_lag1  = \"Yesterday wind (mph)\",\n    visib_mi_lag1  = \"Yesterday visibility (mi)\",\n    temp_F_lag1    = \"Yesterday temperature (Â°F)\"\n  ))\n\ngt_tbl &lt;- tidy_coefs %&gt;%\n  select(term, estimate, conf.low, conf.high, p.value) %&gt;%\n  mutate(across(c(estimate, conf.low, conf.high), ~round(., 3)),\n         p.value = format.pval(p.value, digits = 3, eps = 0.001)) %&gt;%\n  gt::gt() %&gt;%\n  gt::tab_header(title = \"Lagged weather coefficients (partial effects)\") %&gt;%\n  gt::fmt_markdown(columns = 1) %&gt;%\n  gt::cols_label(\n    term = \"Predictor\",\n    estimate = \"Estimate\",\n    conf.low = \"CI low\",\n    conf.high = \"CI high\",\n    p.value = \"p\"\n  )\n\ngt_tbl\n\n\n\n\n\n\n\n\nLagged weather coefficients (partial effects)\n\n\nPredictor\nEstimate\nCI low\nCI high\np\n\n\n\n\nYesterday precip (mm)\n-0.135\n-0.227\n-0.043\n0.00397\n\n\nYesterday wind (mph)\n0.320\n0.138\n0.503\n&lt; 0.001\n\n\nYesterday visibility (mi)\n0.170\n-0.378\n0.719\n0.54234\n\n\nYesterday temperature (Â°F)\n0.168\n0.040\n0.296\n0.01035"
  },
  {
    "objectID": "stories/weather-memory.html#airport-where-memory-is-strongest",
    "href": "stories/weather-memory.html#airport-where-memory-is-strongest",
    "title": "Weather Memory",
    "section": "âœˆï¸ Airport where memory is strongest",
    "text": "âœˆï¸ Airport where memory is strongest\n\n\nCode\nby_airport &lt;- panel_model %&gt;%\n  group_by(origin) %&gt;%\n  group_modify(~ broom::tidy(lm(\n      mean_dep_delay ~ precip_mm_lag1 + wind_mph_lag1 + visib_mi_lag1 + temp_F_lag1 +\n        precip_mm + wind_mph + visib_mi + temp_F + dow + month,\n      data = .x\n    ), conf.int = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  filter(term == \"precip_mm_lag1\")\n\np_airport &lt;- by_airport %&gt;%\n  ggplot(aes(x = reorder(origin, estimate), y = estimate)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.1) +\n  coord_flip() +\n  labs(title = \"Where is the weather-memory effect strongest?\",\n       subtitle = \"Coefficient on *yesterday precipitation* by airport (separate regressions)\",\n       x = \"Airport\", y = \"Lag precip coefficient (minutes per mm)\",\n       caption = \"Points = estimates; bars = 95% CI.\") +\n  quiet_theme()\n\np_airport"
  },
  {
    "objectID": "stories/weather-memory.html#cancellation-link-does-yesterdays-rain-raise-todays-cancels",
    "href": "stories/weather-memory.html#cancellation-link-does-yesterdays-rain-raise-todays-cancels",
    "title": "Weather Memory",
    "section": "ğŸ”—ï¸ Cancellation link: does yesterdayâ€™s rain raise todayâ€™s cancels?",
    "text": "ğŸ”—ï¸ Cancellation link: does yesterdayâ€™s rain raise todayâ€™s cancels?\n\n\nCode\ncor_overall &lt;- panel_model %&gt;%\n  summarise(cor = cor(precip_mm_lag1, cancel_rate, use = \"complete.obs\"))\n\ncor_by_airport &lt;- panel_model %&gt;%\n  group_by(origin) %&gt;%\n  summarise(cor = cor(precip_mm_lag1, cancel_rate, use = \"complete.obs\"))\n\ncor_overall; cor_by_airport"
  },
  {
    "objectID": "stories/weather-memory.html#what-if-set-yesterdays-rain-to-zero",
    "href": "stories/weather-memory.html#what-if-set-yesterdays-rain-to-zero",
    "title": "Weather Memory",
    "section": "ğŸ”® What-if: set yesterdayâ€™s rain to zero",
    "text": "ğŸ”® What-if: set yesterdayâ€™s rain to zero\n\n\nCode\npred_obs &lt;- augment(model_full, data = panel_model) %&gt;% select(origin, date, .fitted)\n\npred_counterf &lt;- panel_model %&gt;%\n  mutate(precip_mm_lag1 = 0) %&gt;%\n  add_predictions(model_full) %&gt;%\n  transmute(origin, date, fitted_zero_lag_rain = pred)\n\nwhat_if &lt;- pred_obs %&gt;%\n  left_join(pred_counterf, by = c(\"origin\",\"date\")) %&gt;%\n  left_join(panel_model %&gt;% select(origin, date, mean_dep_delay), by = c(\"origin\",\"date\")) %&gt;%\n  mutate(\n    delta_minutes = .fitted - fitted_zero_lag_rain,\n    ontime_rate_obs = pmax(0, 1 - mean_dep_delay/15),\n    ontime_rate_cf  = pmax(0, 1 - fitted_zero_lag_rain/15),\n    ontime_gain_pct = (ontime_rate_cf - ontime_rate_obs) * 100\n  )\n\nsummary(what_if$ontime_gain_pct)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-112.844  -34.609   -4.604  -13.325    0.000  105.299 \n\n\nCode\np_whatif &lt;- what_if %&gt;%\n  ggplot(aes(x = ontime_gain_pct)) +\n  geom_histogram(bins = 30, alpha = 0.7) +\n  facet_wrap(~ origin, nrow = 1) +\n  labs(title = \"What-if yesterday were dry: implied on-time rate improvement\",\n       x = \"Percentage points (counterfactual âˆ’ observed)\", y = \"Days\",\n       caption = \"Back-of-envelope: converts minutes to on-time share via a 15-minute threshold on the mean.\") +\n  quiet_theme()\n\np_whatif"
  },
  {
    "objectID": "stories/weather-memory.html#bonus-today-vs.-yesterday-on-a-joint-scatter-hoverable",
    "href": "stories/weather-memory.html#bonus-today-vs.-yesterday-on-a-joint-scatter-hoverable",
    "title": "Weather Memory",
    "section": "ğŸ”„Bonus: Today vs.Â Yesterday on a joint scatter (hoverable)",
    "text": "ğŸ”„Bonus: Today vs.Â Yesterday on a joint scatter (hoverable)\n\n\nCode\np_scatter2 &lt;- panel_model %&gt;%\n  ggplot(aes(x = precip_mm_lag1, y = precip_mm, size = mean_dep_delay, text = paste(date, origin))) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~ origin) +\n  labs(title = \"Today vs. yesterday precipitation (point size = today's mean delay)\",\n       x = \"Yesterday (mm)\", y = \"Today (mm)\") +\n  quiet_theme()\n\nggplotly(p_scatter2, tooltip = c(\"text\", \"x\", \"y\")) %&gt;%\n  layout(\n    margin = list(t = 100),  # more top room\n    title = list(\n      text = \"&lt;b&gt;Today vs. yesterday precipitation (point size = today's mean delay)&lt;/b&gt;\",\n      font = list(size = 12),\n      x = 0.5, xanchor = \"center\"\n    )\n  )"
  },
  {
    "objectID": "stories/weather-memory.html#story-beats",
    "href": "stories/weather-memory.html#story-beats",
    "title": "Weather Memory",
    "section": "ğŸµ Story beats",
    "text": "ğŸµ Story beats\nWe asked: Do storms leave a hangover in the schedule?\nWe did: Calculated daily delays by airport, then compared them to both todayâ€™s and yesterdayâ€™s precipitation, wind, visibility, and temperature.\nWe found: Yesterdayâ€™s weather often remains a statistically meaningful predictor of todayâ€™s delays, even when we control for todayâ€™s weather and calendar effects.\nWhy it matters: Recovery is a process. Aircraft and crews are networked; disruptions propagate and decay, sometimes over more than one day."
  },
  {
    "objectID": "stories/weather-memory.html#practical-implications",
    "href": "stories/weather-memory.html#practical-implications",
    "title": "Weather Memory",
    "section": "âš™ï¸ Practical implications",
    "text": "âš™ï¸ Practical implications\n\nScheduling & rotations: Build buffers the day after major weather events; rotate slack where the lag effect is strongest.\nMaintenance & de-icing ops: Staff for spillover demand on wetâ†’dry transitions.\nPassenger comms: Proactive alerts for potential residual delays the day after storms."
  },
  {
    "objectID": "stories/weather-memory.html#limitations",
    "href": "stories/weather-memory.html#limitations",
    "title": "Weather Memory",
    "section": "ğŸš§ Limitations",
    "text": "ğŸš§ Limitations\n\nWe use means of delays; distributional effects (e.g., big tails) are not fully captured.\nOnly 1-day memory is modeled; longer lags may matter.\nWeather aggregation is daily; within-day timing (e.g., late-night rain) could drive stronger effects.\nLinear models are a simplification; non-linearities and interactions (e.g., wind Ã— visibility) are likely.\n\n\n Dataset: nycflights13"
  },
  {
    "objectID": "stories/weather-memory.html#references-credits",
    "href": "stories/weather-memory.html#references-credits",
    "title": "Weather Memory",
    "section": "ğŸ“š References & Credits",
    "text": "ğŸ“š References & Credits\n\nData: nycflights13 (Hadley Wickham et al.) â€” CRAN: https://CRAN.R-project.org/package=nycflights13\nPackages: tidyverse, lubridate, broom, gt, plotly, modelr.\nTheme: falls back to a quiet minimal style if your siteâ€™s quiet_theme() is not available."
  },
  {
    "objectID": "stories/weather-memory.html#gentle-call-to-curiosity",
    "href": "stories/weather-memory.html#gentle-call-to-curiosity",
    "title": "Weather Memory",
    "section": "ğŸ•Šï¸ Gentle call to curiosity",
    "text": "ğŸ•Šï¸ Gentle call to curiosity\nWhat other forms of memory might our systems hold?"
  }
]