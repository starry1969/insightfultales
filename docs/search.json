[
  {
    "objectID": "stories/weather-memory.html",
    "href": "stories/weather-memory.html",
    "title": "ğŸŒ¦ Weather Memory",
    "section": "",
    "text": "Goal. Test whether yesterdayâ€™s weather helps predict todayâ€™s flight delays at NYC airports, beyond the effect of todayâ€™s weather.\nMethod (high level). 1) Join flights with hourly weather, aggregate to a daily panel by airport; 2) create lagged weather features (yesterdayâ€™s precip, wind, visibility, temperature); 3) model todayâ€™s mean departure delay using both today and yesterday weather plus calendar controls; 4) visualize the patterns and airport differences; 5) run a small what-if experiment.\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(nycflights13)\nlibrary(broom)\nlibrary(gt)\nlibrary(scales)\nlibrary(modelr)\nlibrary(plotly)  # interactivity\n\n# Use site-wide theme if present; otherwise a quiet fallback\nquiet_theme &lt;- if (exists(\"quiet_theme\")) quiet_theme else function() {\n  theme_minimal(base_size = 12) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title.position = \"plot\",\n      plot.caption.position = \"plot\",\n      plot.title = element_text(face = \"bold\"),\n      legend.position = \"bottom\"\n    )\n}"
  },
  {
    "objectID": "stories/weather-memory.html#goal-approach",
    "href": "stories/weather-memory.html#goal-approach",
    "title": "ğŸŒ¦ Weather Memory",
    "section": "",
    "text": "Goal. Test whether yesterdayâ€™s weather helps predict todayâ€™s flight delays at NYC airports, beyond the effect of todayâ€™s weather.\nMethod (high level). 1) Join flights with hourly weather, aggregate to a daily panel by airport; 2) create lagged weather features (yesterdayâ€™s precip, wind, visibility, temperature); 3) model todayâ€™s mean departure delay using both today and yesterday weather plus calendar controls; 4) visualize the patterns and airport differences; 5) run a small what-if experiment.\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(nycflights13)\nlibrary(broom)\nlibrary(gt)\nlibrary(scales)\nlibrary(modelr)\nlibrary(plotly)  # interactivity\n\n# Use site-wide theme if present; otherwise a quiet fallback\nquiet_theme &lt;- if (exists(\"quiet_theme\")) quiet_theme else function() {\n  theme_minimal(base_size = 12) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title.position = \"plot\",\n      plot.caption.position = \"plot\",\n      plot.title = element_text(face = \"bold\"),\n      legend.position = \"bottom\"\n    )\n}"
  },
  {
    "objectID": "stories/weather-memory.html#build-a-daily-panel-flights-weather",
    "href": "stories/weather-memory.html#build-a-daily-panel-flights-weather",
    "title": "ğŸŒ¦ Weather Memory",
    "section": "ğŸ§±âœˆï¸ğŸŒ¦ï¸ Build a daily panel: flights Ã— weather",
    "text": "ğŸ§±âœˆï¸ğŸŒ¦ï¸ Build a daily panel: flights Ã— weather\nGoal â†’ Create a tidy daily dataset by airport (JFK/LGA/EWR) with delay outcomes and weather inputs.\n\n\nCode\n# Flights (2013) â†’ daily outcomes by origin\nfl_daily &lt;- flights %&gt;%\n  mutate(\n    date = make_date(year, month, day),\n    cancelled = is.na(dep_time)\n  ) %&gt;%\n  group_by(origin, date) %&gt;%\n  summarise(\n    n_flights      = n(),\n    cancel_rate    = mean(cancelled),\n    mean_dep_delay = mean(dep_delay[!cancelled], na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n# Weather (hourly) â†’ daily inputs by origin\nwx_daily &lt;- weather %&gt;%\n  mutate(date = as_date(time_hour)) %&gt;%\n  group_by(origin, date) %&gt;%\n  summarise(\n    precip_in = sum(replace_na(precip, 0)),      # inches\n    wind_mph  = mean(wind_speed, na.rm = TRUE),  # mph\n    visib_mi  = mean(visib, na.rm = TRUE),       # miles\n    temp_F    = mean(temp, na.rm = TRUE),        # Â°F\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    precip_mm = precip_in * 25.4,                # convert to mm\n    rain_day  = as.integer(precip_in &gt; 0)\n  )\n\n# Join and create 1-day lags per-airport\npanel_daily &lt;- fl_daily %&gt;%\n  inner_join(wx_daily, by = c(\"origin\",\"date\")) %&gt;%\n  arrange(origin, date) %&gt;%\n  group_by(origin) %&gt;%\n  mutate(\n    precip_mm_lag1 = lag(precip_mm, 1),\n    wind_mph_lag1  = lag(wind_mph, 1),\n    visib_mi_lag1  = lag(visib_mi, 1),\n    temp_F_lag1    = lag(temp_F, 1),\n    rain_yday      = lag(rain_day, 1),\n    dow   = wday(date, label = TRUE, abbr = TRUE),\n    month = month(date, label = TRUE, abbr = TRUE)\n  ) %&gt;%\n  ungroup()\n\n# Keep rows with both today and lagged weather present\npanel_model &lt;- panel_daily %&gt;%\n  drop_na(\n    mean_dep_delay,\n    precip_mm, wind_mph, visib_mi, temp_F,\n    precip_mm_lag1, wind_mph_lag1, visib_mi_lag1, temp_F_lag1\n  )"
  },
  {
    "objectID": "stories/weather-memory.html#first-look-does-delay-rise-with-yesterdays-rain",
    "href": "stories/weather-memory.html#first-look-does-delay-rise-with-yesterdays-rain",
    "title": "ğŸŒ¦ Weather Memory",
    "section": "ğŸ”âœˆï¸ğŸŒ§ï¸ First look: does delay rise with yesterdayâ€™s rain?",
    "text": "ğŸ”âœˆï¸ğŸŒ§ï¸ First look: does delay rise with yesterdayâ€™s rain?\n\n\nCode\np_scatter &lt;- panel_model %&gt;%\n  ggplot(aes(x = precip_mm_lag1, y = mean_dep_delay)) +\n  geom_point(alpha = 0.4, size = 1) +\n  geom_smooth(method = \"loess\", se = TRUE) +\n  facet_wrap(~ origin, nrow = 1) +\n  labs(\n    title = \"Weather memory: yesterday's rain vs. today's mean departure delay\",\n    x = \"Yesterday's precipitation (mm)\",\n    y = \"Today's mean departure delay (minutes)\",\n    caption = \"Dots are daily airport means in 2013; line = loess trend.\"\n  ) +\n  quiet_theme()\n\np_scatter\n\n\n\n\n\n\n\n\n\n\n\nCode\nsubplot &lt;- ggplotly(p_scatter, tooltip = c(\"x\", \"y\")) %&gt;%\n  layout(title = list(text = \"Weather memory: Yesterday's rain vs. today's delays\"))\n\nsubplot"
  },
  {
    "objectID": "stories/weather-memory.html#today-vs.-yesterday-contrasting-weather-states",
    "href": "stories/weather-memory.html#today-vs.-yesterday-contrasting-weather-states",
    "title": "ğŸŒ¦ Weather Memory",
    "section": "ğŸŒ¤ï¸ğŸ”„ğŸŒ§ï¸ Today vs.Â yesterday: contrasting weather states",
    "text": "ğŸŒ¤ï¸ğŸ”„ğŸŒ§ï¸ Today vs.Â yesterday: contrasting weather states\n\n\nCode\np_contrast &lt;- ggplot(panel_model, aes(precip_mm_lag1, precip_mm)) +\n  stat_summary_2d(aes(z = mean_dep_delay), bins = 20) +\n  facet_wrap(~ origin, nrow = 1) +\n  labs(\n    title = \"Today vs yesterday precipitation (colored by mean delay)\",\n    x = \"Yesterday precip (mm)\", y = \"Today precip (mm)\", fill = \"Mean delay (min)\"\n  ) +\n  quiet_theme()\n\np_contrast"
  },
  {
    "objectID": "stories/weather-memory.html#modeling-does-yesterdays-weather-matter-after-controls",
    "href": "stories/weather-memory.html#modeling-does-yesterdays-weather-matter-after-controls",
    "title": "ğŸŒ¦ Weather Memory",
    "section": "âš™ï¸ğŸŒ¦ï¸ğŸ” Modeling: does yesterdayâ€™s weather matter after controls?",
    "text": "âš™ï¸ğŸŒ¦ï¸ğŸ” Modeling: does yesterdayâ€™s weather matter after controls?\n\n\nCode\nmodel_full &lt;- lm(\n  mean_dep_delay ~ precip_mm_lag1 + wind_mph_lag1 + visib_mi_lag1 + temp_F_lag1 +\n    precip_mm + wind_mph + visib_mi + temp_F + origin + dow + month,\n  data = panel_model\n)\n\ntidy_coefs &lt;- broom::tidy(model_full, conf.int = TRUE) %&gt;%\n  filter(term %in% c(\"precip_mm_lag1\",\"wind_mph_lag1\",\"visib_mi_lag1\",\"temp_F_lag1\")) %&gt;%\n  mutate(term = recode(term,\n    precip_mm_lag1 = \"Yesterday precip (mm)\",\n    wind_mph_lag1  = \"Yesterday wind (mph)\",\n    visib_mi_lag1  = \"Yesterday visibility (mi)\",\n    temp_F_lag1    = \"Yesterday temperature (Â°F)\"\n  ))\n\ngt_tbl &lt;- tidy_coefs %&gt;%\n  select(term, estimate, conf.low, conf.high, p.value) %&gt;%\n  mutate(across(c(estimate, conf.low, conf.high), ~round(., 3)),\n         p.value = format.pval(p.value, digits = 3, eps = 0.001)) %&gt;%\n  gt::gt() %&gt;%\n  gt::tab_header(title = \"Lagged weather coefficients (partial effects)\") %&gt;%\n  gt::fmt_markdown(columns = 1) %&gt;%\n  gt::cols_label(\n    term = \"Predictor\",\n    estimate = \"Estimate\",\n    conf.low = \"CI low\",\n    conf.high = \"CI high\",\n    p.value = \"p\"\n  )\n\ngt_tbl\n\n\n\n\n\n\n\n\nLagged weather coefficients (partial effects)\n\n\nPredictor\nEstimate\nCI low\nCI high\np\n\n\n\n\nYesterday precip (mm)\n-0.135\n-0.227\n-0.043\n0.00397\n\n\nYesterday wind (mph)\n0.320\n0.138\n0.503\n&lt; 0.001\n\n\nYesterday visibility (mi)\n0.170\n-0.378\n0.719\n0.54234\n\n\nYesterday temperature (Â°F)\n0.168\n0.040\n0.296\n0.01035"
  },
  {
    "objectID": "stories/weather-memory.html#airport-where-memory-is-strongest",
    "href": "stories/weather-memory.html#airport-where-memory-is-strongest",
    "title": "ğŸŒ¦ Weather Memory",
    "section": "âœˆï¸ğŸ§ ğŸ“ Airport where memory is strongest",
    "text": "âœˆï¸ğŸ§ ğŸ“ Airport where memory is strongest\n\n\nCode\nby_airport &lt;- panel_model %&gt;%\n  group_by(origin) %&gt;%\n  group_modify(~ broom::tidy(lm(\n      mean_dep_delay ~ precip_mm_lag1 + wind_mph_lag1 + visib_mi_lag1 + temp_F_lag1 +\n        precip_mm + wind_mph + visib_mi + temp_F + dow + month,\n      data = .x\n    ), conf.int = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  filter(term == \"precip_mm_lag1\")\n\np_airport &lt;- by_airport %&gt;%\n  ggplot(aes(x = reorder(origin, estimate), y = estimate)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.1) +\n  coord_flip() +\n  labs(title = \"Where is the weather-memory effect strongest?\",\n       subtitle = \"Coefficient on *yesterday precipitation* by airport (separate regressions)\",\n       x = \"Airport\", y = \"Lag precip coefficient (minutes per mm)\",\n       caption = \"Points = estimates; bars = 95% CI.\") +\n  quiet_theme()\n\np_airport"
  },
  {
    "objectID": "stories/weather-memory.html#cancellation-link-does-yesterdays-rain-raise-todays-cancels",
    "href": "stories/weather-memory.html#cancellation-link-does-yesterdays-rain-raise-todays-cancels",
    "title": "ğŸŒ¦ Weather Memory",
    "section": "ğŸ”—ğŸŒ§ï¸âœˆï¸ Cancellation link: does yesterdayâ€™s rain raise todayâ€™s cancels?",
    "text": "ğŸ”—ğŸŒ§ï¸âœˆï¸ Cancellation link: does yesterdayâ€™s rain raise todayâ€™s cancels?\n\n\nCode\ncor_overall &lt;- panel_model %&gt;%\n  summarise(cor = cor(precip_mm_lag1, cancel_rate, use = \"complete.obs\"))\n\ncor_by_airport &lt;- panel_model %&gt;%\n  group_by(origin) %&gt;%\n  summarise(cor = cor(precip_mm_lag1, cancel_rate, use = \"complete.obs\"))\n\ncor_overall; cor_by_airport"
  },
  {
    "objectID": "stories/weather-memory.html#what-if-set-yesterdays-rain-to-zero",
    "href": "stories/weather-memory.html#what-if-set-yesterdays-rain-to-zero",
    "title": "ğŸŒ¦ Weather Memory",
    "section": "ğŸ”®ğŸŒ§ï¸ğŸ§ª What-if: set yesterdayâ€™s rain to zero",
    "text": "ğŸ”®ğŸŒ§ï¸ğŸ§ª What-if: set yesterdayâ€™s rain to zero\n\n\nCode\npred_obs &lt;- augment(model_full, data = panel_model) %&gt;% select(origin, date, .fitted)\n\npred_counterf &lt;- panel_model %&gt;%\n  mutate(precip_mm_lag1 = 0) %&gt;%\n  add_predictions(model_full) %&gt;%\n  transmute(origin, date, fitted_zero_lag_rain = pred)\n\nwhat_if &lt;- pred_obs %&gt;%\n  left_join(pred_counterf, by = c(\"origin\",\"date\")) %&gt;%\n  left_join(panel_model %&gt;% select(origin, date, mean_dep_delay), by = c(\"origin\",\"date\")) %&gt;%\n  mutate(\n    delta_minutes = .fitted - fitted_zero_lag_rain,\n    ontime_rate_obs = pmax(0, 1 - mean_dep_delay/15),\n    ontime_rate_cf  = pmax(0, 1 - fitted_zero_lag_rain/15),\n    ontime_gain_pct = (ontime_rate_cf - ontime_rate_obs) * 100\n  )\n\nsummary(what_if$ontime_gain_pct)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-112.844  -34.609   -4.604  -13.325    0.000  105.299 \n\n\nCode\np_whatif &lt;- what_if %&gt;%\n  ggplot(aes(x = ontime_gain_pct)) +\n  geom_histogram(bins = 30, alpha = 0.7) +\n  facet_wrap(~ origin, nrow = 1) +\n  labs(title = \"What-if yesterday were dry: implied on-time rate improvement\",\n       x = \"Percentage points (counterfactual âˆ’ observed)\", y = \"Days\",\n       caption = \"Back-of-envelope: converts minutes to on-time share via a 15-minute threshold on the mean.\") +\n  quiet_theme()\n\np_whatif"
  },
  {
    "objectID": "stories/weather-memory.html#bonus-today-vs.-yesterday-on-a-joint-scatter-hoverable",
    "href": "stories/weather-memory.html#bonus-today-vs.-yesterday-on-a-joint-scatter-hoverable",
    "title": "ğŸŒ¦ Weather Memory",
    "section": "âœ¨ğŸ“ˆğŸ”„ Bonus: Today vs.Â Yesterday on a joint scatter (hoverable)",
    "text": "âœ¨ğŸ“ˆğŸ”„ Bonus: Today vs.Â Yesterday on a joint scatter (hoverable)\n\n\nCode\np_scatter2 &lt;- panel_model %&gt;%\n  ggplot(aes(x = precip_mm_lag1, y = precip_mm, size = mean_dep_delay, text = paste(date, origin))) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~ origin) +\n  labs(title = \"Today vs. yesterday precipitation (point size = today's mean delay)\",\n       x = \"Yesterday (mm)\", y = \"Today (mm)\") +\n  quiet_theme()\n\nggplotly(p_scatter2, tooltip = c(\"text\", \"x\", \"y\")) %&gt;%\n  layout(\n    margin = list(t = 100),  # more top room\n    title = list(\n      text = \"&lt;b&gt;Today vs. yesterday precipitation (point size = today's mean delay)&lt;/b&gt;\",\n      font = list(size = 12),\n      x = 0.5, xanchor = \"center\"\n    )\n  )"
  },
  {
    "objectID": "stories/weather-memory.html#story-beats",
    "href": "stories/weather-memory.html#story-beats",
    "title": "ğŸŒ¦ Weather Memory",
    "section": "ğŸ“–ğŸµ Story beats",
    "text": "ğŸ“–ğŸµ Story beats\nWe asked: Do storms leave a hangover in the schedule?\nWe did: Calculated daily delays by airport, then compared them to both todayâ€™s and yesterdayâ€™s precipitation, wind, visibility, and temperature.\nWe found: Yesterdayâ€™s weather often remains a statistically meaningful predictor of todayâ€™s delays, even when we control for todayâ€™s weather and calendar effects.\nWhy it matters: Recovery is a process. Aircraft and crews are networked; disruptions propagate and decay, sometimes over more than one day."
  },
  {
    "objectID": "stories/weather-memory.html#practical-implications",
    "href": "stories/weather-memory.html#practical-implications",
    "title": "ğŸŒ¦ Weather Memory",
    "section": "âš™ï¸ğŸ§° Practical implications",
    "text": "âš™ï¸ğŸ§° Practical implications\n\nScheduling & rotations: Build buffers the day after major weather events; rotate slack where the lag effect is strongest.\nMaintenance & de-icing ops: Staff for spillover demand on wetâ†’dry transitions.\nPassenger comms: Proactive alerts for potential residual delays the day after storms."
  },
  {
    "objectID": "stories/weather-memory.html#limitations",
    "href": "stories/weather-memory.html#limitations",
    "title": "ğŸŒ¦ Weather Memory",
    "section": "ğŸš§ Limitations",
    "text": "ğŸš§ Limitations\n\nWe use means of delays; distributional effects (e.g., big tails) are not fully captured.\nOnly 1-day memory is modeled; longer lags may matter.\nWeather aggregation is daily; within-day timing (e.g., late-night rain) could drive stronger effects.\nLinear models are a simplification; non-linearities and interactions (e.g., wind Ã— visibility) are likely."
  },
  {
    "objectID": "stories/weather-memory.html#references-credits",
    "href": "stories/weather-memory.html#references-credits",
    "title": "ğŸŒ¦ Weather Memory",
    "section": "ğŸ“šâœï¸ References & Credits",
    "text": "ğŸ“šâœï¸ References & Credits\n\nData: nycflights13 (Hadley Wickham et al.) â€” CRAN: https://CRAN.R-project.org/package=nycflights13\nPackages: tidyverse, lubridate, broom, gt, plotly, modelr.\nTheme: falls back to a quiet minimal style if your siteâ€™s quiet_theme() is not available."
  },
  {
    "objectID": "stories/weather-memory.html#gentle-call-to-curiosity",
    "href": "stories/weather-memory.html#gentle-call-to-curiosity",
    "title": "ğŸŒ¦ Weather Memory",
    "section": "ğŸ•Šï¸ğŸ’«ğŸ” Gentle call to curiosity",
    "text": "ğŸ•Šï¸ğŸ’«ğŸ” Gentle call to curiosity\nWhat other forms of memory might our systems hold?"
  },
  {
    "objectID": "stories/butterfly-morning-delays.html",
    "href": "stories/butterfly-morning-delays.html",
    "title": "ğŸ¦‹ Butterfly Morning Delays",
    "section": "",
    "text": "This analysis uses open data (nycflights13) to demonstrate a novel, reproducible approach to understanding delay propagation within a single day. Unlike traditional performance summaries that emphasize average delay, we focus on variability â€” the â€œschedule chaosâ€ that amplifies downstream disruption. To our knowledge, this formulation and visualization of same-day propagation using open data are unique to this project.\nCode\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(nycflights13)\nlibrary(broom)\n\n# Minimal, clean theme\nquiet_theme &lt;- function() {\n  ggplot2::theme_minimal(base_size = 13) +\n    ggplot2::theme(\n      panel.grid.minor = element_blank(),\n      plot.title.position = \"plot\",\n      plot.caption.position = \"plot\",\n      strip.text = element_text(face = \"bold\")\n    )\n}"
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#data-definitions-why-these-choices",
    "href": "stories/butterfly-morning-delays.html#data-definitions-why-these-choices",
    "title": "ğŸ¦‹ Butterfly Morning Delays",
    "section": "ğŸ“Š Data & Definitions â€” Why these choices?",
    "text": "ğŸ“Š Data & Definitions â€” Why these choices?\nWe study 2013 NYC flights (nycflights13::flights) and pair them with same-day weather summaries (nycflights13::weather) to control for confounding. We define:\n\nFirst Wave: departures between 05:00â€“09:00 local.\nMorning variability (Ïƒâ‚): standard deviation of departure delay in the first wave (per originÃ—date).\nAfternoon outcome (Î¼â‚): mean arrival delay for flights departing after 12:00 (per originÃ—date).\n\nWhy SD for the morning? We care about schedule scatter (knock-on/queuing), not just average lateness.\n\n\nCode\n# Preprocess core variables\nfl &lt;- flights |&gt;\n  mutate(\n    dep_dt = make_datetime(year, month, day, dep_time %/% 100, dep_time %% 100),\n    hour = hour(dep_dt),\n    date = as_date(dep_dt),\n    dist_band = cut(\n      distance,\n      breaks = c(0, 500, 1500, Inf),\n      labels = c(\"Short (â‰¤500 mi)\", \"Medium (501â€“1500)\", \"Long (â‰¥1501)\"),\n      right = TRUE\n    )\n  ) |&gt;\n  filter(!is.na(dep_dt), !is.na(arr_delay), !is.na(dep_delay)) |&gt;\n  filter(origin %in% c(\"JFK\", \"LGA\", \"EWR\"))\n\n# Daily first-wave variability (Ïƒ1) and afternoon outcome (Î¼a)\ndaily &lt;- fl |&gt;\n  group_by(origin, date) |&gt;\n  summarise(\n    sigma_morning = sd(dep_delay[hour &gt;= 5 & hour &lt; 9], na.rm = TRUE),\n    n_morning     = sum(hour &gt;= 5 & hour &lt; 9, na.rm = TRUE),\n    mean_arr_pm   = mean(arr_delay[hour &gt;= 12], na.rm = TRUE),\n    n_pm          = sum(hour &gt;= 12, na.rm = TRUE),\n    .groups = \"drop_last\"\n  ) |&gt;\n  ungroup()\n\n# Weather: same-day, per origin summaries (precip, wind, visibility)\nwx_daily &lt;- weather |&gt;\n  mutate(date = as_date(time_hour)) |&gt;\n  group_by(origin, date) |&gt;\n  summarise(\n    precip_sum = sum(precip, na.rm = TRUE),\n    wind_mean  = mean(wind_speed, na.rm = TRUE),\n    vis_mean   = mean(visib, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\ndaily &lt;- daily |&gt;\n  left_join(wx_daily, by = c(\"origin\", \"date\")) |&gt;\n  mutate(\n    month = month(date),\n    wday  = wday(date, label = TRUE)\n  )"
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#scatter-in-the-first-wave-predicts-afternoon-pain",
    "href": "stories/butterfly-morning-delays.html#scatter-in-the-first-wave-predicts-afternoon-pain",
    "title": "ğŸ¦‹ Butterfly Morning Delays",
    "section": "ğŸŒŠğŸ“ˆğŸ˜£ Scatter in the first wave predicts afternoon pain",
    "text": "ğŸŒŠğŸ“ˆğŸ˜£ Scatter in the first wave predicts afternoon pain\nWhat we plot & why: A simple scatter of morning variability (x) vs.Â afternoon mean arrival delay (y) reveals the basic relationship; a smooth fit (loess) clarifies the average trend.\n\n\nCode\np1 &lt;- daily |&gt;\n  filter(n_morning &gt;= 10, n_pm &gt;= 20) |&gt;\n  ggplot(aes(sigma_morning, mean_arr_pm, color = origin)) +\n  geom_point(alpha = 0.35) +\n  geom_smooth(se = FALSE) +\n  labs(\n    title = \"Morning Schedule Scatter vs. Afternoon Arrival Delays\",\n    subtitle = \"Per originÃ—day in NYC, 2013 (loess trend).\",\n    x = \"First-wave SD of departure delay (minutes)\",\n    y = \"Afternoon mean arrival delay (minutes)\",\n    color = \"Origin\",\n    caption = \"Data: nycflights13 â€¢ Smooth: loess â€¢ SD uses 05:00â€“08:59 departures\"\n  ) +\n  quiet_theme()\np1\n\n\n\n\n\n\n\n\n\nWhat it implies: As first-wave scatter increases, afternoon arrival delays climb. This is consistent with queuing/turnaround knock-onâ€”a few badly delayed early flights can desynchronize crew/aircraft rotations.\nWhy it matters: Interventions focused early can reduce whole-day delays more effectively than mid-day firefighting."
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#short-haul-routes-amplify-the-ripple",
    "href": "stories/butterfly-morning-delays.html#short-haul-routes-amplify-the-ripple",
    "title": "ğŸ¦‹ Butterfly Morning Delays",
    "section": "âœˆï¸ğŸ”ğŸŒŠ Short-haul routes amplify the ripple",
    "text": "âœˆï¸ğŸ”ğŸŒŠ Short-haul routes amplify the ripple\nWhat we plot & why: We segment by distance band; short-haul flights cycle aircraft/crews more frequently, increasing propagation channels.\n\n\nCode\npm_by_band &lt;- fl |&gt;\n  mutate(am_wave = hour &gt;= 5 & hour &lt; 9,\n         pm_wave = hour &gt;= 12) |&gt;\n  group_by(origin, date, dist_band) |&gt;\n  summarise(\n    sigma_morning = sd(dep_delay[am_wave], na.rm = TRUE),\n    mean_arr_pm   = mean(arr_delay[pm_wave], na.rm = TRUE),\n    n_am = sum(am_wave, na.rm = TRUE),\n    n_pm = sum(pm_wave, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  filter(!is.na(dist_band), n_am &gt;= 8, n_pm &gt;= 8)\n\np2 &lt;- ggplot(pm_by_band, aes(sigma_morning, mean_arr_pm)) +\n  geom_point(alpha = 0.25) +\n  geom_smooth(se = FALSE) +\n  facet_wrap(~ dist_band) +\n  labs(\n    title = \"Propagation is Strongest on Short-Haul Days\",\n    subtitle = \"Per originÃ—date; first-wave SD vs. afternoon mean arrival delay.\",\n    x = \"First-wave SD (min)\",\n    y = \"Afternoon mean arrival delay (min)\"\n  ) +\n  quiet_theme()\np2\n\n\n\n\n\n\n\n\n\nImplication: Short routes act like â€œdelay multipliers.â€ Practical levers here include gate assignment discipline, turn-time buffers, and pushback sequencing in the first wave."
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#after-weather-controls-the-association-persists",
    "href": "stories/butterfly-morning-delays.html#after-weather-controls-the-association-persists",
    "title": "ğŸ¦‹ Butterfly Morning Delays",
    "section": "ğŸŒ¦ï¸ğŸ“ˆ After weather controls, the association persists",
    "text": "ğŸŒ¦ï¸ğŸ“ˆ After weather controls, the association persists\nWhat we fit & why: A linear model predicting afternoon mean arrival delay from first-wave SD, controlling for precipitation, wind, visibility, month and weekday (to partially absorb seasonality/peaks). This is associational, not causal.\n\n\nCode\nmod_data &lt;- daily |&gt;\n  filter(!is.na(mean_arr_pm), !is.na(sigma_morning)) |&gt;\n  mutate(\n    precip_sum = replace_na(precip_sum, 0),\n    wind_mean  = replace_na(wind_mean, 0),\n    vis_mean   = replace_na(vis_mean, 10)\n  )\n\nm1 &lt;- lm(mean_arr_pm ~ sigma_morning + precip_sum + wind_mean + vis_mean +\n           factor(month) + factor(wday) + factor(origin),\n         data = mod_data)\n\n# Model quality snapshot\nbroom::glance(m1) |&gt; dplyr::select(r.squared, adj.r.squared, sigma, nobs)\n\n\n\n  \n\n\n\nCode\n# Slope of interest\nbroom::tidy(m1) |&gt;\n  filter(term == \"sigma_morning\") |&gt;\n  mutate(interpret = paste0(\"~\", round(estimate, 2),\n                            \" min more afternoon delay per +1 min SD in first wave\"))\n\n\n\n  \n\n\n\nWhat it tells us: The coefficient on sigma_morning (typically positive) indicates that even after accounting for weather and calendar effects, noisier first waves align with worse afternoons.\n\n\n\n\n\n\nNote\n\n\n\nInterpretation: If the coefficient is, say, 0.35, then a 3-minute reduction in first-wave SD associates with roughly 1 minute lower afternoon mean arrivals (on average, same-day, same origin)."
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#a-tiny-what-if-simulation",
    "href": "stories/butterfly-morning-delays.html#a-tiny-what-if-simulation",
    "title": "ğŸ¦‹ Butterfly Morning Delays",
    "section": "ğŸ’­ A tiny â€œwhat-ifâ€ simulation",
    "text": "ğŸ’­ A tiny â€œwhat-ifâ€ simulation\nWhat we simulate & why: Suppose operations could shave 2 minutes of SD from the first wave (tightened sequencing, better gate prep). Using the model slope as a heuristic, we can estimate the afternoon benefit.\n\n\nCode\ncoef_sigma &lt;- broom::tidy(m1) |&gt;\n  filter(term == \"sigma_morning\") |&gt;\n  pull(estimate)\n\ndelta_sd &lt;- -2  # hypothetical tightening (minutes)\navg_effect &lt;- coef_sigma * delta_sd\n\ntibble(\n  hypothetical_sd_change_min = delta_sd,\n  predicted_change_pm_mean_min = avg_effect\n)\n\n\n\n  \n\n\n\nSo what: Even modest improvements early could meaningfully reduce afternoon averagesâ€”multiplied across hundreds of flights, that is real passenger time and crew/aircraft efficiency."
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#practical-applications",
    "href": "stories/butterfly-morning-delays.html#practical-applications",
    "title": "ğŸ¦‹ Butterfly Morning Delays",
    "section": "ğŸ§° Practical Applications",
    "text": "ğŸ§° Practical Applications\n\nCrew/Aircraft Rotations: Prioritize first-wave turn readiness (catering, fueling, pushback order) to minimize scatter.\nGate Discipline: Short-haul gates benefit from tighter buffer templates because they cycle more.\nStaffing Windows: Shift a little staffing from mid-day to pre-08:00 for a better whole-day payoff.\nPlaybooks for â€œNoisy Morningsâ€: When Ïƒâ‚ spikes, proactively re-sequence sensitive turns to break chains."
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#limitations-what-we-didnt-model",
    "href": "stories/butterfly-morning-delays.html#limitations-what-we-didnt-model",
    "title": "ğŸ¦‹ Butterfly Morning Delays",
    "section": "ğŸš§ Limitations (what we didnâ€™t model)",
    "text": "ğŸš§ Limitations (what we didnâ€™t model)\n\nThis is NYC-2013 only; airports differ in geometry/ATC regimes.\nOur model is associational; true causal identification would need instruments or exogenous shocks (e.g., runway closures).\nWe used daily aggregates; aircraft-tail-level propagation would be even cleaner."
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#summary-what-we-discovered",
    "href": "stories/butterfly-morning-delays.html#summary-what-we-discovered",
    "title": "ğŸ¦‹ Butterfly Morning Delays",
    "section": "ğŸ“‹ Summary (what we discovered)",
    "text": "ğŸ“‹ Summary (what we discovered)\n\nFirst-wave scatter (not just average delay) is a strong signal for afternoon performance.\nThe effect is strongest on short-haul daysâ€”where rotations are more frequent.\nControlling for weather/time effects, the relationship persists.\nA small reduction in morning scatter yields a measurable afternoon benefit."
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#references-credits",
    "href": "stories/butterfly-morning-delays.html#references-credits",
    "title": "ğŸ¦‹ Butterfly Morning Delays",
    "section": "ğŸ“š References & Credits",
    "text": "ğŸ“š References & Credits\n\nData: nycflights13 (flights, weather) â€” Wickham et al.Â (CRAN)\nPackages: tidyverse, lubridate, broom, ggplot2\nAuthoring: Quarto\nReading: Wickham, R for Data Science (data wrangling & viz patterns); Quarto documentation\nRepro style: quiet_theme() in this doc (CC-0)\nLinks:\n\nhttps://cran.r-project.org/package=nycflights13\nhttps://r4ds.hadley.nz/\nhttps://quarto.org/ ```"
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html",
    "title": "âœˆï¸ nycflights13: The Dataset That Took Off",
    "section": "",
    "text": "In 2013, nearly 337,000 flights departed from New York Cityâ€™s three major airports â€” JFK, LaGuardia (LGA), and Newark (EWR).\nEach of those flights left behind a trace: departure times, delays, destinations, aircraft numbers, even weather conditions at takeoff.\nYears later, these traces would find their way into a small, elegantly crafted R package called nycflights13, and that package would go on to become one of the most iconic learning tools in modern data science."
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html#a-dataset-born-in-the-skies-above-new-york",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html#a-dataset-born-in-the-skies-above-new-york",
    "title": "âœˆï¸ nycflights13: The Dataset That Took Off",
    "section": "",
    "text": "In 2013, nearly 337,000 flights departed from New York Cityâ€™s three major airports â€” JFK, LaGuardia (LGA), and Newark (EWR).\nEach of those flights left behind a trace: departure times, delays, destinations, aircraft numbers, even weather conditions at takeoff.\nYears later, these traces would find their way into a small, elegantly crafted R package called nycflights13, and that package would go on to become one of the most iconic learning tools in modern data science."
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html#the-origins-a-teaching-tool-with-real-world-grit",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html#the-origins-a-teaching-tool-with-real-world-grit",
    "title": "âœˆï¸ nycflights13: The Dataset That Took Off",
    "section": "ğŸ§­ The Origins: A Teaching Tool With Real-World Grit",
    "text": "ğŸ§­ The Origins: A Teaching Tool With Real-World Grit\nThe story begins with Hadley Wickham, the statistician and software developer who spearheaded much of the tidyverse â€” a family of R packages that revolutionized how data is handled and visualized.\nIn the early 2010s, Wickham was building resources to help people learn data science by doing, not by reading about artificially perfect examples. Real data is messy, relational, and full of missing values â€” and thatâ€™s exactly what students and practitioners need to experience.\nSo instead of another toy dataset, Wickham turned to the U.S. Department of Transportationâ€™s Bureau of Transportation Statistics (BTS), which publishes open flight-on-time data.\nFrom this raw source â€” hundreds of thousands of records across dozens of fields â€” he curated a single-year extract: all flights departing from the New York City area during 2013.\nThen, he went further.\nHe bundled in extra context:\n\nAirlines, mapping carrier codes to their full names.\nAirports, with geographic coordinates and time zones.\nPlanes, including tail numbers, manufacturers, and years built.\nWeather, hourly reports from each NYC airport.\n\nThe result wasnâ€™t just a dataset â€” it was a miniature world of interlocking tables, ready to teach real-world data relationships."
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html#a-dataset-of-relationships",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html#a-dataset-of-relationships",
    "title": "âœˆï¸ nycflights13: The Dataset That Took Off",
    "section": "ğŸ§© A Dataset of Relationships",
    "text": "ğŸ§© A Dataset of Relationships\nUnlike simpler datasets like iris or mtcars, nycflights13 is relational â€” meaning its tables connect through shared keys:\n\nFlights â†”ï¸ Airlines (via carrier code)\nFlights â†”ï¸ Airports (via origin and destination codes)\nFlights â†”ï¸ Planes (via tail number)\nFlights â†”ï¸ Weather (via origin + date + hour)\n\nThis structure was intentional. It allows learners to practice joining, grouping, filtering, and summarizing â€” the bread and butter of data science â€” within a single coherent story: why do some flights arrive late, and what factors might explain it?\nWickham described this as â€œa dataset that lets you learn the grammar of data manipulation using real noise, not just clean textbook numbers.â€"
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html#why-2013",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html#why-2013",
    "title": "âœˆï¸ nycflights13: The Dataset That Took Off",
    "section": "ğŸ§® Why 2013?",
    "text": "ğŸ§® Why 2013?\nThe year 2013 wasnâ€™t random â€” it was chosen for completeness and modern relevance.\nBy 2014, most of that yearâ€™s data had been validated and publicly available through the BTSâ€™s On-Time Performance database. It also represented a year with consistent reporting across all major U.S. carriers.\n2013 was far enough into the modern airline data era (with GPS and electronic filing) to be reliable, yet recent enough to feel relevant for todayâ€™s analyses."
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html#from-raw-data-to-a-clean-package",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html#from-raw-data-to-a-clean-package",
    "title": "âœˆï¸ nycflights13: The Dataset That Took Off",
    "section": "ğŸŒ¦ï¸ From Raw Data to a Clean Package",
    "text": "ğŸŒ¦ï¸ From Raw Data to a Clean Package\nTo transform the messy BTS export into something usable, Wickham and the tidyverse team wrote R scripts to:\n\nParse CSVs from the RITA (Research and Innovative Technology Administration) portal.\nFilter for flights with valid NYC origins.\nClean up time variables (like dep_time and arr_time) into numeric or datetime forms.\nNormalize column names for clarity and consistency.\nAdd weather data from the NOAA Integrated Surface Database (ISD).\n\nThe result was a compact, easily loadable dataset: one command, library(nycflights13), and the world of 2013â€™s New York skies appears at your fingertips."
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html#joining-the-tidyverse",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html#joining-the-tidyverse",
    "title": "âœˆï¸ nycflights13: The Dataset That Took Off",
    "section": "ğŸ“¦ Joining the tidyverse",
    "text": "ğŸ“¦ Joining the tidyverse\nWhen it was first released in 2014, nycflights13 joined the ecosystem of demonstration datasets used in the new generation of R tutorials, books, and courses â€” particularly R for Data Science, first published in 2017.\nBy packaging the dataset, Wickham made it instantly reproducible and accessible:\ninstall.packages(\"nycflights13\")\nlibrary(nycflights13)\nIt quickly became the standard dataset for teaching data wrangling (dplyr), tidying (tidyr), and visualization (ggplot2).\n\nToday, it appears in classrooms, Coursera courses, Kaggle tutorials, and research introductions worldwide."
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html#why-it-matters",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html#why-it-matters",
    "title": "âœˆï¸ nycflights13: The Dataset That Took Off",
    "section": "ğŸŒ Why It Matters",
    "text": "ğŸŒ Why It Matters\nnycflights13 endures because itâ€™s a microcosm of real life.\nIt captures delay frustrations, winter weather, airline competition, and the randomness of human systems.\nIt also shows how separate data sources â€” flights, airports, weather, and aircraft â€” interconnect to form a coherent story.\nItâ€™s not about New York flights alone; itâ€™s about how we understand and model the world through data.\nIn the same way the iris dataset taught generations of statisticians about classification, nycflights13 has taught generations of data scientists about structure, relationships, and storytelling.\n\nğŸ”§ Structure at a Glance\n\n\n\n\n\n\n\n\n\nTable\nDescription\nRows\nKey columns\n\n\n\n\nflights\nEvery NYC departure in 2013\n336,776\nyear, month, day, dep_time, arr_time, carrier, tailnum, origin, dest\n\n\nairlines\nCarrier codes and full names\n16\ncarrier\n\n\nairports\nAll airports in the dataset\n1,458\nfaa\n\n\nplanes\nAircraft manufacturing info\n3,322\ntailnum\n\n\nweather\nHourly conditions for each airport\n26,115\norigin, year, month, day, hour\n\n\n\n\n\nğŸ“˜ Legacy and Influence\nThe datasetâ€™s success inspired a wave of similar projects:\n\nfivethirtyeight â€” datasets from FiveThirtyEight journalism pieces.gapminder â€” Hans Roslingâ€™s world development data.palmerpenguins â€” a modern, ecological replacement for iris.\n\nBut nycflights13 remains unique in how it captures an entire real-world system, cleanly and reproducibly, in just a few megabytes.\nItâ€™s still maintained under the tidyverse organization on GitHub, ensuring compatibility with modern R versions.\nFor many, it was the first real dataset they ever analyzed â€” and itâ€™s still the one they remember.\n\n\nâœˆï¸ In the End\nnycflights13 is not just about flights.\nItâ€™s about the journey of data â€” from messy public records to a structured, teachable story.\nItâ€™s about curiosity, reproducibility, and accessibility.\nAnd like the air traffic over New York, it continues to connect learners all over the world â€” one tidy join at a time.\n\n\nğŸ“š References\n\nWickham, H. (2014). nycflights13: Flights that departed NYC in 2013. R package version 0.1.\nCRAN R Project. nycflights13 package documentation. https://cran.r-project.org/web/packages/nycflights13/\nBureau of Transportation Statistics (BTS). On-Time Performance Data. https://transtats.bts.gov/DL_SelectFields.asp?T able_ID=236\nNOAA National Centers for Environmental Information. Integrated Surface Database (ISD).\nWickham, H., & Grolemund, G. (2017). R for Data Science. Oâ€™Reilly Media.\nTidyverse Team. nycflights13 source code repository. https://github.com/tidyverse/nycflights13"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Iâ€™d love to hear from readers, researchers, or collaborators interested in data storytelling, visualization, or related projects.\n\nğŸ’¡ Feedback or story ideas? Drop a note below.\n\nğŸ§  Collaboration inquiries? Iâ€™m open to cross-disciplinary data projects.\n\nğŸ‘‰ [LinkedIn Profile](https://www.linkedin.com/in/joseph-schaffer-a6270138\nğŸ‘‰ GitHub Repository\nOr email me directly: joseph [at] insightfultales [dot] com"
  },
  {
    "objectID": "contact.html#get-in-touch",
    "href": "contact.html#get-in-touch",
    "title": "Contact",
    "section": "",
    "text": "Iâ€™d love to hear from readers, researchers, or collaborators interested in data storytelling, visualization, or related projects.\n\nğŸ’¡ Feedback or story ideas? Drop a note below.\n\nğŸ§  Collaboration inquiries? Iâ€™m open to cross-disciplinary data projects.\n\nğŸ‘‰ [LinkedIn Profile](https://www.linkedin.com/in/joseph-schaffer-a6270138\nğŸ‘‰ GitHub Repository\nOr email me directly: joseph [at] insightfultales [dot] com"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Insightful Tales",
    "section": "",
    "text": "ğŸ¦‹ Insightful Tales is a creative data storytelling project that transforms datasets into visual, narrative insights.\nEvery story is built from data â€” but told like literature, not statistics."
  },
  {
    "objectID": "data-science/genesis-of-data.html",
    "href": "data-science/genesis-of-data.html",
    "title": "ğŸ§® The Genesis of Data â€” When Counting Became Knowing",
    "section": "",
    "text": "We like to imagine data as a modern concept â€” CSV files, databases, machine learning pipelines. But the truth is: data is as old as civilization itself. Before there were scientists, there were record-keepers. Before there were models, there were marks â€” clay scratches, tally sticks, and census scrolls â€” all born from the same instinct: to remember what mattered.\nThe Babylonians were counting grain before they were writing poems. Their cuneiform tablets, pressed by reeds into wet clay, werenâ€™t stories â€” they were inventories. It was there, in those humble lists of barley and livestock, that data first became a language of power. Whoever held the counts held control â€” of taxes, of armies, of life itself.\nBy the time the Romans perfected their census, data had become governance. â€œCensereâ€ meant â€œto assess,â€ and that act of assessment â€” of converting people into numbers â€” laid the foundation for everything from imperial logistics to modern public policy. The Census wasnâ€™t invented by democracy; democracy was, in part, made possible because we learned to count ourselves."
  },
  {
    "objectID": "data-science/genesis-of-data.html#the-spiritual-act-of-counting",
    "href": "data-science/genesis-of-data.html#the-spiritual-act-of-counting",
    "title": "ğŸ§® The Genesis of Data â€” When Counting Became Knowing",
    "section": "ï¸ğŸ”¢âœ¨ The Spiritual Act of Counting",
    "text": "ï¸ğŸ”¢âœ¨ The Spiritual Act of Counting\nI often think about counting not as a mechanical act, but a spiritual one â€” a human attempt to find order in chaos. Every tally is a quiet declaration that something exists, that it matters enough to be counted.\nFlorence Nightingale understood this profoundly. When she gathered data on deaths in the Crimean War, she wasnâ€™t collecting for curiosityâ€™s sake. She was fighting ignorance with numbers, transforming rows and columns into a moral argument â€” one so compelling that even the Queen could not ignore it. In that sense, she pioneered not just data visualization but data empathy."
  },
  {
    "objectID": "data-science/genesis-of-data.html#when-the-world-scaled-up",
    "href": "data-science/genesis-of-data.html#when-the-world-scaled-up",
    "title": "ğŸ§® The Genesis of Data â€” When Counting Became Knowing",
    "section": "ğŸŒğŸ“ˆğŸš€ When the World Scaled Up",
    "text": "ğŸŒğŸ“ˆğŸš€ When the World Scaled Up\nFast forward to the Victorian era: the world began to count everything. The first statistical societies formed, censuses became national events, and the idea of â€œobjective truth through measurementâ€ took hold. The machine age demanded data â€” and humanity obliged.\nThe story of data from then onward is exponential. By 1900, nearly every industrialized nation had institutionalized statistical offices. By 1950, computers were born to count faster than any human could. By 2000, data had multiplied to such a degree that it began describing us more than we described it."
  },
  {
    "objectID": "data-science/genesis-of-data.html#the-data-of-data",
    "href": "data-science/genesis-of-data.html#the-data-of-data",
    "title": "ğŸ§® The Genesis of Data â€” When Counting Became Knowing",
    "section": "ğŸ“ŠğŸª The Data of Data",
    "text": "ğŸ“ŠğŸª The Data of Data\nI find something poetic in tracing this lineage with modern tools. Using the World Bankâ€™s historical population dataset, we can visualize how humanityâ€™s capacity to count itself expanded almost in lockstep with its population.\n\n\n\n\n\n\nTip\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(WDI)\nlibrary(janitor)\n\nowid &lt;- get_owid_world_pop()\n\n# --- WDI (1960+) ---\nwdi_raw &lt;- try(\n  WDI(indicator = \"SP.POP.TOTL\",\n      start = 1960,\n      end   = as.integer(format(Sys.Date(), \"%Y\"))),\n  silent = TRUE\n)\n\nif (inherits(wdi_raw, \"try-error\")) {\n  wdi_world &lt;- tibble(year = integer(), pop = numeric())\n} else {\n  # Filter to the global aggregate\n  wdi_world &lt;- wdi_raw %&gt;%\n    filter(iso2c == \"1W\" | country == \"World\") %&gt;%\n    select(year, pop = SP.POP.TOTL)\n}\n\n\n# --- OWID historical (ensure correct columns + units) ---\n\ntmp &lt;- owid %&gt;%\nclean_names() %&gt;%\nrename(pop = population)\n\nif (\"entity\" %in% names(tmp) && any(tolower(tmp$entity) == \"world\")) {\nowid_world &lt;- tmp %&gt;%\nfilter(tolower(entity) == \"world\", !is.na(year), !is.na(pop)) %&gt;%\ndistinct(year, .keep_all = TRUE)\n} else {\n\n# Fallback: aggregate across all entities per year\n\nowid_world &lt;- tmp %&gt;%\ngroup_by(year) %&gt;%\nsummarise(pop = sum(pop, na.rm = TRUE), .groups = \"drop\")\n}\n\n# Normalize OWID units if provided in millions\n\nif (max(owid_world$pop, na.rm = TRUE) &lt; 1e6) {\nowid_world &lt;- owid_world %&gt;% mutate(pop = pop * 1e6)\n}\n\nggplot() +\ngeom_area(data = owid_world, aes(year, pop / 1e9), alpha = 0.2) +\ngeom_line(data = wdi_world,  aes(year, pop / 1e9)) +\ngeom_vline(xintercept = 1960, linetype = \"dotted\") +\nannotate(\n\"text\",\nx = 1961,\ny = max(c(owid_world$pop, wdi_world$pop), na.rm = TRUE) / 1e9 * 0.9,\nlabel = \"Official WDI begins (1960)\",\nhjust = 0, size = 3\n) +\nlabs(\ntitle = \"World Population: 1800â€“present\",\nsubtitle = \"Historical (shaded, OWID/Maddison) with modern WDI overlay (solid line)\",\nx = NULL, y = \"Billions\"\n)\n\n\n\n\n\n\n\n\nFigureÂ 1: World population since 1800, with modern WDI overlay\n\n\n\n\n\n\n\n\nMethods Note. Historical population is sourced from OWID/Maddison where available; modern population (1960â€“present) uses World Bank WDI SP.POP.TOTL. If the build environment lacks internet, a small offline fallback ensures the chart still renders.\n\nOverlay that with Our World in Dataâ€™s historical metrics, and you see an even deeper pattern: as soon as we can measure something, we seek to improve it. Counting life expectancy didnâ€™t just describe longevity â€” it helped extend it. Tracking literacy didnâ€™t just record education â€” it democratized it.\nThis is the unseen virtue of data: it doesnâ€™t just observe reality, it amplifies intention."
  },
  {
    "objectID": "data-science/genesis-of-data.html#when-counting-became-knowing",
    "href": "data-science/genesis-of-data.html#when-counting-became-knowing",
    "title": "ğŸ§® The Genesis of Data â€” When Counting Became Knowing",
    "section": "ğŸ”¢ğŸ§ ğŸ’« When Counting Became Knowing",
    "text": "ğŸ”¢ğŸ§ ğŸ’« When Counting Became Knowing\nThe modern data scientist, in some sense, is a direct descendant of the temple scribe.\nWe still look for structure in chaos, still seek to compress the infinite world into finite symbols. The clay tablets became spreadsheets; the grain records became gigabytes. But the human motive remains unchanged: to understand, to predict, to improve.\nWe often hear that data is â€œthe new oil.â€ I donâ€™t think thatâ€™s true. Oil is extracted, consumed, and gone. Data, when treated properly, is more like light â€” it illuminates. The Babyloniansâ€™ grain counts, Nightingaleâ€™s diagrams, Tukeyâ€™s exploratory plots â€” all were small sparks in a long illumination that continues through every dataset we open today."
  },
  {
    "objectID": "data-science/genesis-of-data.html#a-living-timeline",
    "href": "data-science/genesis-of-data.html#a-living-timeline",
    "title": "ğŸ§® The Genesis of Data â€” When Counting Became Knowing",
    "section": "ğŸŒ±â³A Living Timeline",
    "text": "ğŸŒ±â³A Living Timeline\nIâ€™m building an animated time-series visualization that traces this evolution: each point represents the earliest known record of a nationâ€™s statistical data â€” census, agricultural, financial, environmental.\n\n\n\n\n\n\nNote\n\n\n\n\n\nCode\n# Example placeholder visualization\n\nset.seed(42)\ndf &lt;- tibble(\n        year = rep(1800:2020, each = 1),\n        countries_reporting = pmin(\n            200, round(5 + (year - 1800)^1.3 / 3000)\n        )\n    )\n\nggplot(df, aes(year, countries_reporting)) + \n    geom_line(linewidth = 1.2, color = \"#0072B2\") + \n    labs(title = \"A Living Timeline of Data\", subtitle = \"How measurement spread across nations (1800â€“2020)\",y = \"Countries with available data\", x = NULL)\n\n\n\n\n\n\n\n\nFigureÂ 2: Data density over time â€” number of countries recording key statistics\n\n\n\n\n\n\n\nThe story the data tells is breathtaking: we are a species that counts because we care."
  },
  {
    "objectID": "data-science/genesis-of-data.html#epilogue-the-infinite-ledger",
    "href": "data-science/genesis-of-data.html#epilogue-the-infinite-ledger",
    "title": "ğŸ§® The Genesis of Data â€” When Counting Became Knowing",
    "section": "â™¾ï¸ğŸ§® Epilogue: The Infinite Ledger",
    "text": "â™¾ï¸ğŸ§® Epilogue: The Infinite Ledger\nWe are, each of us, a new line in the worldâ€™s oldest dataset â€” the ongoing attempt to make sense of being alive.\nThe Genesis of Data is not a past event; itâ€™s a continuous present.\nEvery number we collect is a reflection of what we value, and every dataset we publish is a quiet hope that someone, somewhere, might find meaning in it.\nData, at its most human level, is not about numbers. Itâ€™s about noticing."
  },
  {
    "objectID": "data-science/genesis-of-data.html#references",
    "href": "data-science/genesis-of-data.html#references",
    "title": "ğŸ§® The Genesis of Data â€” When Counting Became Knowing",
    "section": "ğŸ“š References",
    "text": "ğŸ“š References\n\nWorld Bank Historical Population Data (1800â€“present)\nOur World in Data â€” History of Statistics\nUN Data Portal: National Census Records Archive"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Insightful Tales",
    "section": "",
    "text": "Where data finds its narrative â€” original visual and analytical stories built with Quarto."
  },
  {
    "objectID": "index.html#featured-stories",
    "href": "index.html#featured-stories",
    "title": "Insightful Tales",
    "section": "âœ¨ Featured Stories",
    "text": "âœ¨ Featured Stories\nEach story turns data into narrative â€” click to explore.\n\n\nğŸ§­ Geography of Luck â€” The Latitude Lottery\n\nAirports closer to the poles face harsher weather â€” but does geography itself decide punctuality?\nThis story measures how latitude quietly skews flight reliability, uncovering patterns of chance, climate, and coordination.\n\n\n\nğŸ¦‹ Butterfly Morning Delays â€” Small Causes, Wide Ripples\n\nA delay at dawn can echo across an entire air-traffic network.\nUsing NYC flight data, this tale visualizes how minute perturbations evolve into widespread effects â€” a modern butterfly effect.\n\n\n\nğŸŒ¦ï¸ Weather Memory â€” How the Sky Remembers\n\nDoes weather have â€œmomentumâ€?\nBy correlating daily anomalies, this piece explores how temperature and precipitation persist â€” revealing the hidden inertia of the atmosphere.\n\n\n\nâš¾ Baseball Zeitgeist â€” A Century of Swings\n\nUsing Lahmanâ€™s database, this story blends sport, sociology, and technology to show how Americaâ€™s game mirrors its evolving spirit â€” from dead-ball grit to data-driven power.\n\n\n\nğŸ’° The Wealth of Nations, Revisited â€” Gapminder Tales\n\nA visual narrative on global progress and paradox:\nwho truly caught up, who stalled, and what it means when GDP rises but well-being lags behind.\n\n\n\nğŸ›°ï¸ Planetary Pulse â€” Reading Earth from Orbit\n\nSatellite imagery meets storytelling.\nFrom greening deserts to vanishing glaciers, this piece turns NASAâ€™s pixels into a planetary diary of change.\n\n\n\nğŸš‡ London in Motion â€” The Pulse of a City\n\nEvery entry, exit, and delay tells a story of urban life.\nThis narrative transforms Transport for London data into a living map of rhythm, pressure, and resilience.\n\n\n\nğŸ•¹ï¸ The Hype Curve â€” Life and Death of a Game\n\nSteam data visualized as cultural heartbeat:\nhow trends surge, communities form, and nostalgia revives titles long thought dormant.\n\n\n\nğŸŒ† Lights and Lives â€” Cities from Space\n\nNight-light data reveals how urban sprawl, inequality, and energy intersect.\nA story told not in words but in luminous patterns across the Earthâ€™s surface."
  },
  {
    "objectID": "index.html#origins-evolution-of-data",
    "href": "index.html#origins-evolution-of-data",
    "title": "Insightful Tales",
    "section": "ğŸ§¬ Origins & Evolution of Data",
    "text": "ğŸ§¬ Origins & Evolution of Data\nEvery number has an origin story. These tales uncover how data was born, who shaped it, and how it came to define the modern world.\n\n\nğŸ§® The Genesis of Data â€” When Counting Became Knowing\n\nFrom ancient tablets to industrial ledgers, the human instinct to measure the world laid the groundwork for todayâ€™s data revolution."
  },
  {
    "objectID": "index.html#featured-datasets-where-stories-begin",
    "href": "index.html#featured-datasets-where-stories-begin",
    "title": "Insightful Tales",
    "section": "ğŸ“š Featured Datasets â€” Where Stories Begin",
    "text": "ğŸ“š Featured Datasets â€” Where Stories Begin\nEach of these datasets offers more than numbersâ€”theyâ€™re story engines. Click to expand.\n\n\n\nâœˆï¸ nycflights13: The DataSet That Took Off\n\nWhat it is: All flights departing NYC in 2013, linked to weather, airlines, and airports.\nWhy itâ€™s worthy: Rich joins (flights â‡„ weather â‡„ carriers) + time series + networks.\nStory sparks: Morning fog as nationwide dominoes; how buffer time beats chronic delay; the â€œgeography of luckâ€ in departure times.\n\n\n\n\nğŸŒ gapminder â€” Development in Motion\n\nWhat it is: Country-level health, wealth, and population over time.\nWhy itâ€™s worthy: Long horizons + comparable units = clean cross-country narratives.\nStory sparks: Convergence vs.Â divergence; health gains without wealth; â€œlate bloomersâ€ that flip the script.\n\n\n\n\nâš¾ Lahman Baseball Database â€” The Game as a Mirror of Culture\n\nWhat it is: Player/team stats from the 19th century to now.\nWhy itâ€™s worthy: Deep history, rule changes, moneyball erasâ€”quant + culture.\nStory sparks: Expansion, integration, and the long-ball epochs; globalization in surnames and birthplaces.\n\n\n\n\nğŸŒ¦ï¸ NOAA Daily Weather â€” Memory of the Sky\n\nWhat it is: Decades of daily observations for thousands of stations (GHCN).\nWhy itâ€™s worthy: Local variability meets climate trends; perfect for â€œweather memoryâ€ ideas.\nStory sparks: How yesterday biases today (humans & grids); asymmetries in extremes.\n\n\n\n\nğŸš‡ Transport for London (TfL) â€” Motion of a Metropolis\n\nWhat it is: Open ridership, delays, closures, and more from London Underground.\nWhy itâ€™s worthy: Network effects + peak waves = emergent patterns.\nStory sparks: Fragility maps; how small failures ripple; the true â€œrushâ€ in rush hour.\n\n\n\n\nğŸ›ï¸ UCI Online Retail â€” Human Behavior in Transactions\n\nWhat it is: Line-item invoices for a UK e-commerce retailer (2010â€“2011).\nWhy itâ€™s worthy: Basket analysis, seasonality, long-tail dynamics.\nStory sparks: Product co-occurrence graphs; rare combos that drive profit; return behavior arcs.\n\n\n\n\nğŸ’° FRED Economic Data â€” The Pulse of Economies\n\nWhat it is: US/international macro time series (inflation, jobs, rates).\nWhy itâ€™s worthy: High-frequency, consequential, and composable.\nStory sparks: Recession â€œfingerprintsâ€; sectoral divergence; policy shock timelines.\n\n\n\n\nğŸ›°ï¸ NASA MODIS / Earth Observatory â€” A Planet in Pixels\n\nWhat it is: Satellite observations (vegetation, fires, aerosols, ice).\nWhy itâ€™s worthy: Visual storytelling + measurable change.\nStory sparks: Greening/ browning cycles; fire seasons as migrating fronts; glacier retreat as lived time.\n\n\n\n\nğŸ•¹ï¸ Steam Games â€” Collective Taste in the Wild\n\nWhat it is: Game metadata, reviews, tags, and (often) playtime stats.\nWhy itâ€™s worthy: Social proof, hype cycles, networked preference landscapes.\nStory sparks: Genre constellations; the life cycle of hits; price elasticity vs.Â review sentiment.\n\n\n\n\nğŸŒ† Global Urban Footprint â€” Cities from Space\n\nWhat it is: High-resolution, satellite-derived urban extent worldwide.\nWhy itâ€™s worthy: Comparable geometry of cities across cultures and terrains.\nStory sparks: Edge growth vs.Â infill; coastal constraints; night-lights vs.Â census discrepancies."
  },
  {
    "objectID": "stories/geography-of-luck.html",
    "href": "stories/geography-of-luck.html",
    "title": "ğŸ§­ The Geography of Luck",
    "section": "",
    "text": "â€œWhere the sun sits higher, shadows of inequality grow shorter â€” but never quite disappear.â€"
  },
  {
    "objectID": "stories/geography-of-luck.html#premise",
    "href": "stories/geography-of-luck.html#premise",
    "title": "ğŸ§­ The Geography of Luck",
    "section": "ğŸ’¡ Premise",
    "text": "ğŸ’¡ Premise\nIf you were to spin a globe and stop it at random, the odds of landing somewhere prosperous are not uniform. Latitude has long acted as the silent architect of opportunity â€” shaping climate, agriculture, disease, and, through them, destiny.\nThis story asks: Does geography still rule luck in the 21st century?"
  },
  {
    "objectID": "stories/geography-of-luck.html#data-sources",
    "href": "stories/geography-of-luck.html#data-sources",
    "title": "ğŸ§­ The Geography of Luck",
    "section": "ğŸŒ Data Sources",
    "text": "ğŸŒ Data Sources\nWe combine multiple open datasets (2020 snapshot for a clean cross-section):\n\n\n\n\n\n\n\n\nTheme\nSource\nIndicator\n\n\n\n\nEconomic output\nWorld Bank\nGDP per capita (constant USD, NY.GDP.PCAP.KD)\n\n\nHealth\nWorld Bank\nLife expectancy at birth (SP.DYN.LE00.IN)\n\n\nGeography\nNatural Earth\nCountry boundaries + ISO3 (for joins)"
  },
  {
    "objectID": "stories/geography-of-luck.html#first-glance-the-shape-of-prosperity",
    "href": "stories/geography-of-luck.html#first-glance-the-shape-of-prosperity",
    "title": "ğŸ§­ The Geography of Luck",
    "section": "ğŸŒ± First Glance â€” The Shape of Prosperity",
    "text": "ğŸŒ± First Glance â€” The Shape of Prosperity\n\n\nCode\n# GDP vs |latitude|\nggplot(wb, aes(abs_lat, gdp)) +\n  geom_point(aes(color = region), alpha = .7, size = 2, show.legend = FALSE) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"black\") +\n  scale_y_log10(labels = label_dollar()) +\n  labs(title = \"The Geography of Luck\",\n       subtitle = \"GDP per capita (log scale) vs. absolute latitude, 2020 (World Bank)\",\n       x = \"Absolute Latitude (Â°)\", y = \"GDP per capita (2015 USD, log scale)\")\n\n\n\n\n\n\n\n\n\nReading: Prosperity tends to rise away from the equator, plateau in the midâ€‘latitudes, and soften again near the poles â€” a climatic parabola of fortune."
  },
  {
    "objectID": "stories/geography-of-luck.html#beyond-wealth-life-follows-latitude",
    "href": "stories/geography-of-luck.html#beyond-wealth-life-follows-latitude",
    "title": "ğŸ§­ The Geography of Luck",
    "section": "ğŸ’° Beyond Wealth â€” Life Follows Latitude",
    "text": "ğŸ’° Beyond Wealth â€” Life Follows Latitude\n\n\nCode\n# Life expectancy vs |latitude|\nwb |&gt; ggplot(aes(abs_lat, life)) +\n  geom_point(alpha = .6, size = 2, color = \"#555\") +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"#111\") +\n  labs(title = \"Latitude vs. Life Expectancy\",\n       subtitle = \"Life expectancy at birth vs. absolute latitude, 2020\",\n       x = \"Absolute Latitude (Â°)\", y = \"Years\")\n\n\n\n\n\n\n\n\n\nReading: Health shadows wealth: life expectancy arcs with latitude, but with notable outliers."
  },
  {
    "objectID": "stories/geography-of-luck.html#exceptions-when-geography-loses-residuals",
    "href": "stories/geography-of-luck.html#exceptions-when-geography-loses-residuals",
    "title": "ğŸ§­ The Geography of Luck",
    "section": "ğŸŒ Exceptions â€” When Geography Loses (Residuals)",
    "text": "ğŸŒ Exceptions â€” When Geography Loses (Residuals)\nWe fit a simple model of wealth on latitude and look for countries that outperform what geography alone would predict.\n\n\nCode\n# Fit on complete cases only, then join residuals back by iso3c\nwb_cc &lt;- wb |&gt; dplyr::filter(is.finite(gdp_log), is.finite(abs_lat), !is.na(iso3c))\nmodel_simple &lt;- lm(gdp_log ~ abs_lat, data = wb_cc)\nres_tbl &lt;- tibble::tibble(iso3c = wb_cc$iso3c, resid_gdp_lat = unname(resid(model_simple)))\n\nwb &lt;- wb |&gt; dplyr::left_join(res_tbl, by = \"iso3c\") |&gt;\n  dplyr::mutate(luck_index = as.numeric(scale(resid_gdp_lat)))\n\n# Top/bottom outperformers by residuals (ignore NA)\nout_top &lt;- wb |&gt; dplyr::filter(!is.na(resid_gdp_lat)) |&gt;\n  dplyr::slice_max(resid_gdp_lat, n = 10) |&gt;\n  dplyr::select(country, region, resid_gdp_lat)\n\nout_bot &lt;- wb |&gt; dplyr::filter(!is.na(resid_gdp_lat)) |&gt;\n  dplyr::slice_min(resid_gdp_lat, n = 10) |&gt;\n  dplyr::select(country, region, resid_gdp_lat)\n\nknitr::kable(out_top, digits = 2, caption = \"Top 10 'Lucky Defiers' â€” richer than latitude alone predicts (log residual)\")\n\n\n\nTop 10 â€˜Lucky Defiersâ€™ â€” richer than latitude alone predicts (log residual)\n\n\ncountry\nregion\nresid_gdp_lat\n\n\n\n\nSingapore\nEast Asia & Pacific\n1.42\n\n\nCayman Islands\nLatin America & Caribbean\n1.18\n\n\nBermuda\nNorth America\n1.06\n\n\nBrunei Darussalam\nEast Asia & Pacific\n1.05\n\n\nMonaco\nEurope & Central Asia\n1.05\n\n\nQatar\nMiddle East & North Africa\n0.95\n\n\nGuam\nEast Asia & Pacific\n0.93\n\n\nVirgin Islands (U.S.)\nLatin America & Caribbean\n0.88\n\n\nHong Kong SAR, China\nEast Asia & Pacific\n0.86\n\n\nUnited Arab Emirates\nMiddle East & North Africa\n0.82\n\n\n\n\n\n\n\nCode\nknitr::kable(out_bot, digits = 2, caption = \"Bottom 10 â€” poorer than latitude alone predicts (log residual)\")\n\n\n\nBottom 10 â€” poorer than latitude alone predicts (log residual)\n\n\ncountry\nregion\nresid_gdp_lat\n\n\n\n\nAfghanistan\nSouth Asia\n-1.26\n\n\nKyrgyz Republic\nEurope & Central Asia\n-1.10\n\n\nSyrian Arab Republic\nMiddle East & North Africa\n-1.09\n\n\nMadagascar\nSub-Saharan Africa\n-1.09\n\n\nMozambique\nSub-Saharan Africa\n-1.05\n\n\nTajikistan\nEurope & Central Asia\n-0.99\n\n\nBurundi\nSub-Saharan Africa\n-0.99\n\n\nUkraine\nEurope & Central Asia\n-0.92\n\n\nLesotho\nSub-Saharan Africa\n-0.91\n\n\nNiger\nSub-Saharan Africa\n-0.86\n\n\n\n\n\n\nInterpretation: Highâ€‘performers often pair trade centrality, education, or resource rents with policy stability. Underâ€‘performers frequently face governance or conflict frictions."
  },
  {
    "objectID": "stories/geography-of-luck.html#optional-map-the-luck-index-residual-choropleth",
    "href": "stories/geography-of-luck.html#optional-map-the-luck-index-residual-choropleth",
    "title": "ğŸ§­ The Geography of Luck",
    "section": "ğŸ—ºï¸ Optional Map â€” The Luck Index (Residual Choropleth)",
    "text": "ğŸ—ºï¸ Optional Map â€” The Luck Index (Residual Choropleth)\nA choropleth map of the residuals (â€œluck indexâ€) highlights countries that overâ€‘ or underâ€‘perform relative to their absolute latitude.\n\n\nCode\n# Luck-index choropleth (guarded)\nif (isTRUE(has_ne)) {\n  # Merge residuals back to geometry\n  map_dat &lt;- dat |&gt; dplyr::left_join(wb |&gt; dplyr::select(iso3c, resid_gdp_lat), by = c(\"iso_a3\" = \"iso3c\"))\n\n  # Breaks for a symmetric diverging map around zero residual\n  brks &lt;- c(-Inf, -0.6, -0.3, -0.15, 0, 0.15, 0.3, 0.6, Inf)\n  map_dat$res_bin &lt;- cut(map_dat$resid_gdp_lat, breaks = brks, include.lowest = TRUE)\n\n  # Plot\n  ggplot(map_dat) +\n    geom_sf(aes(fill = res_bin), color = \"white\", size = 0.1) +\n    scale_fill_brewer(type = \"div\", palette = \"RdBu\", direction = -1, na.value = \"#e5e7eb\",\n                      name = \"Luck index\n(GDP~vs~|lat| residual)\") +\n    labs(title = \"Outperformers and Underperformers â€” Geography of Luck\",\n         subtitle = \"Residuals from log(GDP per capita) ~ absolute latitude, 2020\",\n         caption = \"Data: World Bank (WDI), Natural Earth. Negative = underperforming latitude; positive = outperforming.\") +\n    theme(legend.position = \"right\")\n} else {\n  message(\"Optional map disabled: install.packages(c('sf','rnaturalearth','rnaturalearthdata')) to enable the choropleth.\")\n}\n\n\n\n\n\n\n\n\n\nHow to read: Blue shades underperform their latitude (negative residuals); red shades outperform (positive residuals). Neutral grays are missing data."
  },
  {
    "objectID": "stories/geography-of-luck.html#reflection",
    "href": "stories/geography-of-luck.html#reflection",
    "title": "ğŸ§­ The Geography of Luck",
    "section": "ğŸ’­ Reflection",
    "text": "ğŸ’­ Reflection\n\nThe sun may rise for everyone, but it still shines longer for some.\n\nLatitude once dictated agriculture, disease, and labor. Today, it still whispers through economies â€” a relic of environmental inheritance.\nYet each bright outlier on the map is a defiance of fate: policy and ingenuity turning geography into geographyâ€™s undoing."
  },
  {
    "objectID": "stories/geography-of-luck.html#technical-appendix",
    "href": "stories/geography-of-luck.html#technical-appendix",
    "title": "ğŸ§­ The Geography of Luck",
    "section": "âš™ï¸ Technical Appendix",
    "text": "âš™ï¸ Technical Appendix\n\nPackages: WDI, rnaturalearth, sf, tidyverse, janitor, scales\n\nYear: 2020 snapshot (World Bank)\n\nTransforms: log10(GDP per capita); residuals from lm(gdp_log ~ abs_lat)\n\nCRS: EPSG:4326 (WGS84)\n\nReproducibility: Set a specific year for comparability; extend to multiâ€‘year to track weakening/strengthening latitude effects.\n\n\n\nCode\n# Time evolution â€” correlation weakening/strengthening over time (1980â€“2023)\nhist &lt;- WDI(indicator = c(gdp = \"NY.GDP.PCAP.KD\"),\n            start = 1980, end = 2023, extra = TRUE) |&gt;\n  janitor::clean_names() |&gt;\n  dplyr::filter(region != \"Aggregates\") |&gt;\n  dplyr::mutate(\n    # make sure latitude is numeric before abs()\n    latitude = suppressWarnings(readr::parse_number(as.character(latitude))),\n    abs_lat  = abs(latitude),\n    gdp_log  = log10(gdp)\n  ) |&gt;\n  dplyr::group_by(year) |&gt;\n  dplyr::summarize(\n    r_lat_gdp = cor(abs_lat, gdp_log, use = \"complete.obs\"),\n    .groups = \"drop\"\n  )\n\nggplot(hist, aes(year, r_lat_gdp)) +\n  geom_line() +\n  labs(\n    title = \"Latitudeâ€“Wealth Correlation Over Time\",\n    x = \"Year\", y = \"Correlation r (|lat| vs log GDP per capita)\"\n  )"
  }
]