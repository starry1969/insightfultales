[
  {
    "objectID": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html",
    "href": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html",
    "title": "When Growth Forgot Happiness â€” The Hidden Curve of Human Progress",
    "section": "",
    "text": "Economic growth was supposed to lift all boats. But did more money always buy better lives? Using Gapminder, we trace the hidden curve where prosperity rose yet well-being lagged â€” and ask what truly counts as progress."
  },
  {
    "objectID": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#the-question-what-if-growth-isnt-happiness",
    "href": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#the-question-what-if-growth-isnt-happiness",
    "title": "When Growth Forgot Happiness â€” The Hidden Curve of Human Progress",
    "section": "ğŸ•Šï¸ 1. The Question â€” What if Growth Isnâ€™t Happiness?",
    "text": "ğŸ•Šï¸ 1. The Question â€” What if Growth Isnâ€™t Happiness?\nFor decades, countries chased GDP â€” the grand total of what they produced.\nBut can numbers capture joy, belonging, or peace of mind?\nThis story starts with a simple but daring question:\n\nIf GDP rises, does life always feel better?\n\nWeâ€™ll keep the math gentle and the meaning clear.\nYou donâ€™t need a statistics background â€” only curiosity."
  },
  {
    "objectID": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#building-a-fair-baseline",
    "href": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#building-a-fair-baseline",
    "title": "When Growth Forgot Happiness â€” The Hidden Curve of Human Progress",
    "section": "ğŸ”¢ 2. Building a Fair Baseline",
    "text": "ğŸ”¢ 2. Building a Fair Baseline\nWeâ€™ll connect income (GDP per person) and life expectancy,\nthen look for nations that live longer or shorter lives than their income predicts.\nThat difference â€” the gap between money and meaning â€”\nis what we call the âœ¨ Decoupling Index.\n\n\nCode\ndf &lt;- gapminder %&gt;%\n    mutate(\n        log_gdppc = log10(gdpPercap),\n        decade = (year %/% 10) * 10\n    )\n\nlo &lt;- loess(lifeExp ~ log_gdppc + year, data = df, span = 0.5, degree = 2)\n\ndf_index &lt;- df %&gt;%\n    mutate(\n        lifeExp_hat = predict(lo, newdata = df),\n        decouple = lifeExp - lifeExp_hat\n    )\n\n\nInterpretation:\n\nğŸ’™ Positive Index â†’ People live longer than income predicts.\nâ¤ï¸ Negative Index â†’ People live shorter lives than income predicts."
  },
  {
    "objectID": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#the-hidden-curve-appears",
    "href": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#the-hidden-curve-appears",
    "title": "When Growth Forgot Happiness â€” The Hidden Curve of Human Progress",
    "section": "ğŸ“ˆ 3. The Hidden Curve Appears",
    "text": "ğŸ“ˆ 3. The Hidden Curve Appears\n\n\nCode\ndf_index %&gt;%\n    ggplot(aes(x = gdpPercap, y = lifeExp, color = decouple)) +\n    geom_point(alpha = 0.6) +\n    scale_x_log10(labels = label_dollar(scale = 1)) +\n    scale_y_continuous(limits = c(25, 90)) +\n    scale_color_gradient2(\n        low = \"#a50f15\", mid = \"#f0f0f0\", high = \"#08519c\",\n        midpoint = 0,\n        name = \"Decoupling\\nIndex\"\n    ) +\n    labs(x = \"GDP per person (log scale)\", y = \"Life expectancy (years)\") +\n    theme_minimal(base_size = 13)\n\n\n\n\n\nğŸ’« The Hidden Curve â€” life vs.Â income, colored by Decoupling Index.\n\n\n\n\nâœ¨ Look closely: some nations sparkle above the line â€” others droop below.\nThe curve isnâ€™t smooth progress; itâ€™s a collage of human choices."
  },
  {
    "objectID": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#time-reveals-who-outran-money",
    "href": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#time-reveals-who-outran-money",
    "title": "When Growth Forgot Happiness â€” The Hidden Curve of Human Progress",
    "section": "â³ 4. Time Reveals Who Outran Money",
    "text": "â³ 4. Time Reveals Who Outran Money\n\n\nCode\ntrend &lt;- df_index %&gt;%\n    group_by(country) %&gt;%\n    arrange(year, .by_group = TRUE) %&gt;%\n    summarize(\n        start_decouple = first(decouple),\n        end_decouple   = last(decouple),\n        change = end_decouple - start_decouple,\n        continent = first(continent),\n        .groups = \"drop\"\n    ) %&gt;%\n    arrange(desc(change))\n\n\n\n\nCode\ntop_risers   &lt;- trend |&gt; slice_max(change, n = 10)\ntop_slippers &lt;- trend |&gt; slice_min(change, n = 10)\n\nbind_rows(\n    top_risers |&gt; mutate(type = \"ğŸŒŸ Riser\"),\n    top_slippers |&gt; mutate(type = \"ğŸ’¤ Slipper\")\n    ) |&gt;\n    select(type, country, continent, start_decouple, end_decouple, change) |&gt;\n    knitr::kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype\ncountry\ncontinent\nstart_decouple\nend_decouple\nchange\n\n\n\n\nğŸŒŸ Riser\nNicaragua\nAmericas\n-10.82\n9.60\n20.42\n\n\nğŸŒŸ Riser\nSaudi Arabia\nAsia\n-22.50\n-3.82\n18.68\n\n\nğŸŒŸ Riser\nGambia\nAfrica\n-7.86\n8.47\n16.33\n\n\nğŸŒŸ Riser\nHaiti\nAmericas\n-9.41\n5.83\n15.24\n\n\nğŸŒŸ Riser\nMadagascar\nAfrica\n-7.82\n5.63\n13.45\n\n\nğŸŒŸ Riser\nComoros\nAfrica\n-1.52\n11.85\n13.37\n\n\nğŸŒŸ Riser\nBahrain\nAsia\n-16.02\n-2.79\n13.23\n\n\nğŸŒŸ Riser\nPeru\nAmericas\n-11.47\n1.13\n12.60\n\n\nğŸŒŸ Riser\nHonduras\nAmericas\n-7.07\n4.74\n11.81\n\n\nğŸŒŸ Riser\nSenegal\nAfrica\n-7.27\n4.47\n11.74\n\n\nğŸ’¤ Slipper\nBotswana\nAfrica\n7.26\n-22.74\n-30.00\n\n\nğŸ’¤ Slipper\nSwaziland\nAfrica\n-1.15\n-27.56\n-26.41\n\n\nğŸ’¤ Slipper\nLesotho\nAfrica\n4.73\n-15.11\n-19.84\n\n\nğŸ’¤ Slipper\nEquatorial Guinea\nAfrica\n-2.94\n-21.69\n-18.74\n\n\nğŸ’¤ Slipper\nTaiwan\nAsia\n15.55\n0.18\n-15.37\n\n\nğŸ’¤ Slipper\nZimbabwe\nAfrica\n10.94\n-3.91\n-14.84\n\n\nğŸ’¤ Slipper\nZambia\nAfrica\n-0.51\n-13.23\n-12.72\n\n\nğŸ’¤ Slipper\nTrinidad and Tobago\nAmericas\n6.31\n-5.73\n-12.04\n\n\nğŸ’¤ Slipper\nSingapore\nAsia\n10.79\n-1.00\n-11.79\n\n\nğŸ’¤ Slipper\nThailand\nAsia\n11.19\n0.28\n-10.91\n\n\n\nğŸŒ Top ten risers and slippers by Decoupling Index.\n\n\nThese countries are your story leads â€” places where life pulled ahead or fell behind wealth."
  },
  {
    "objectID": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#country-micro-stories-human-over-graph",
    "href": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#country-micro-stories-human-over-graph",
    "title": "When Growth Forgot Happiness â€” The Hidden Curve of Human Progress",
    "section": "ğŸ‡§ğŸ‡¹ 5. Country Micro-Stories â€” Human Over Graph",
    "text": "ğŸ‡§ğŸ‡¹ 5. Country Micro-Stories â€” Human Over Graph\nWeâ€™ll soon explore pairs like:\n\nğŸ‡§ğŸ‡¹ Bhutan, where happiness led the economy.\nğŸ‡¯ğŸ‡µ Japan, where prosperity met stagnation.\nğŸ‡ºğŸ‡¸ United States, where health plateaued despite riches.\n\nEach line tells a tale â€” of policy, culture, and priorities.\n\n\nCode\nset.seed(1)\nCASE_RISER   &lt;- top_risers$country[1]\nCASE_SLIPPER &lt;- top_slippers$country[1]\nglue(\"Riser example: {CASE_RISER}\")\n\n\nRiser example: Nicaragua\n\n\nCode\nglue(\"Slipper example: {CASE_SLIPPER}\")\n\n\nSlipper example: Botswana"
  },
  {
    "objectID": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#the-quiet-threshold-when-money-stops-buying-life",
    "href": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#the-quiet-threshold-when-money-stops-buying-life",
    "title": "When Growth Forgot Happiness â€” The Hidden Curve of Human Progress",
    "section": "ğŸ’¡ 6. The Quiet Threshold â€” When Money Stops Buying Life",
    "text": "ğŸ’¡ 6. The Quiet Threshold â€” When Money Stops Buying Life\n\n\nCode\nref_year &lt;- 2007\ncurve_df &lt;- df_index |&gt; filter(year == ref_year)\nlo_ref &lt;- loess(lifeExp ~ log_gdppc, data = curve_df, span = 0.5)\ngridx &lt;- tibble(log_gdppc = seq(min(curve_df$log_gdppc), max(curve_df$log_gdppc), length.out = 200))\npred  &lt;- predict(lo_ref, newdata = gridx)\ncurve_plot &lt;- bind_cols(gridx, tibble(lifeExp_hat = pred)) |&gt;\n    mutate(gdpPercap = 10^log_gdppc)\n\ncurve_df |&gt;\n    ggplot(aes(gdpPercap, lifeExp)) +\n    geom_point(alpha = 0.6) +\n    geom_line(data = curve_plot, aes(gdpPercap, lifeExp_hat), linewidth = 1) +\n    scale_x_log10(labels = label_dollar(scale = 1)) +\n    labs(\n        x = \"GDP per person (log scale)\",\n        y = \"Life expectancy (years)\",\n        subtitle = \"Beyond this bend, more income buys little extra life.\"\n    ) +\n    theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\n\nBeyond a comfort zone (roughly $30 000 per person),\nthe curve flattens â€” proof that lifeâ€™s deepest gains come from what money canâ€™t buy."
  },
  {
    "objectID": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#where-we-go-next",
    "href": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#where-we-go-next",
    "title": "When Growth Forgot Happiness â€” The Hidden Curve of Human Progress",
    "section": "ğŸš€ 7. Where We Go Next",
    "text": "ğŸš€ 7. Where We Go Next\nEach finding births a future story:\n\nğŸ˜Š The Happiness Deficit â€” when wealth outpaces joy.\nğŸ”„ The Curve That Bends Twice â€” tech progress, then overwhelm.\nğŸ§® Counting What Counts â€” education, safety, belonging.\nğŸ’¬ Data of Dignity â€” measuring lives beyond the ledger.\n\nOur mission: turn data into empathy, and statistics into stories that feel alive.\n\n Dataset: Gapminder \n\n\n What do these icons mean?"
  },
  {
    "objectID": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#references-credits",
    "href": "stories/when-growth-forgot-happiness-the-hidden-curve-of-human-progress.html#references-credits",
    "title": "When Growth Forgot Happiness â€” The Hidden Curve of Human Progress",
    "section": "ğŸ“š References & Credits",
    "text": "ğŸ“š References & Credits\nData Sources\n\nğŸŒ Gapminder Foundation â€” Gapminder Data Portal\nğŸŒ World Bank Open Data â€” World Development Indicators\nğŸ‡ºğŸ‡³ UNDP â€” Human Development Reports\nğŸ˜Š World Happiness Report (UN SDSN)\n\nBackground & Inspiration\n\nğŸ“– Easterlin (1974) Does Economic Growth Improve the Human Lot?\nğŸ•Šï¸ Sen (1999) Development as Freedom\nğŸ’­ Layard (2005) Happiness: Lessons from a New Science\nğŸŒˆ Rosling (2018) Factfulness\n\nCredits\n\nâœï¸ Concept & Writing â€” Insightful Tales\nğŸ¨ Visual Design â€” Insightful Tales Studio\nğŸ“Š Data Stewardship â€” Gapminder Foundation"
  },
  {
    "objectID": "stories/the-planet-that-breathes-tracking-earths-hidden-rhythms-from-space.html",
    "href": "stories/the-planet-that-breathes-tracking-earths-hidden-rhythms-from-space.html",
    "title": "The Planet That Breathes â€” Tracking Earthâ€™s Hidden Rhythms from Space",
    "section": "",
    "text": "ğŸª The Planet That Breathes â€” Tracking Earthâ€™s Hidden Rhythms from Space\n\n\nHow satellite eyes reveal a living planet with seasons, pulses, and sudden interruptions.\n\n\nEarth pulses with life â€” forests inhale, oceans flicker, deserts blossom and sleep. Using decades of NASA satellite imagery, this story uncovers the planetâ€™s hidden rhythms, where they are steady, and where they are starting to skip a beat.\n\n\n\nğŸ«€ Earth Has a Pulse â€” We Just Never Looked for It\nEvery living thing has rhythms.\n\nYour heart beats.\nYour lungs rise and fall.\nYour body temperature cycles each day.\n\nEarth does, too â€” but its rhythms are slow, wide, and hard to see from the ground.\nSatellites give us a way to watch:\n\nforests green and brown\nlakes grow warm then cool\nsnow advance and retreat\ndust sweep across continents\nfires flash like sparks in the dark\n\nIf you take any single pixel on Earth and graph it through time, something magical happens:\nIt forms a repeating pattern â€” a heartbeat.\n\n\nğŸŒ± Rhythm 1: The Green Pulse (Vegetation Breathing)\nForests and grasslands â€œinhaleâ€ as they grow and â€œexhaleâ€ when they rest.\nWe can show this using a simple NDVI time series.\n\nVisual 1 â€” NDVI Pulse: A Forestâ€™s Yearly Breath\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n# simulate a simple, intuitive NDVI annual rhythm that ANY reader can understand\nset.seed(12)\ndays &lt;- 1:365\nndvi &lt;- 0.3 + 0.25*sin(2*pi*(days/365)) + rnorm(365, 0, 0.02)\n\ndf &lt;- tibble(day = days, NDVI = ndvi)\n\nggplot(df, aes(day, NDVI)) +\n  geom_line(size=1.2) +\n  labs(title=\"A Forestâ€™s Yearly Breath\",\n       subtitle=\"Growing season rise â†’ summer peak â†’ winter rest\",\n       x=\"Day of Year\", y=\"NDVI (green-ness)\") +\n  theme_minimal(base_size = 15)\n\n\n\n\n\n\n\n\n\nA wave that rises, peaks, falls, and rests.\nBut now comes the twistâ€¦\n\n\n\nâœ¨ Every Region Has a Natural Rhythm â€” But Not All Are Stable\nWhen we compute this rhythm for locations across Earth, patterns emerge:\n\nSome places have strong, steady pulses\nSome have pulses that weaken slowly\nSome suddenly change shape\nA few lose their rhythm entirely\n\nThis gives us a completely new lens:\nIs a regionâ€™s natural heartbeat stable or changing?\n\n\nâ„ï¸ Rhythm 2: Blink of Winter â€” Eight-Day Snow in a Single Breath\nEven winter has a heartbeat.\nEvery eight days, NASAâ€™s MODIS instrument takes a fresh snapshot of where snow covers the land. On their own, those images look static â€” pale tiles of white over dark land. But when we stitch them together, they reveal something more like a breath: snow advancing, settling, and retreating in waves.\n\n\n\nFigure â€” Blink of Winter. An eight-day rolling view of MODIS snow cover, compressed into a short loop. High latitudes flare bright as snow races south in autumn, then slowly retreats as spring light returns.\n\n\nFrom a distance, the planet looks almost mechanical: snow appears on schedule, especially in the high latitudes. But look closely and the rhythm is not perfectly regular. Some years snow lingers longer into spring. Some years it arrives earlier, or the pattern of expansion and retreat becomes patchy.\nAs Rhythm 2 in this story, this simple animation becomes one of our â€œpulse tracesâ€ â€” a way to watch the seasonal heartbeat of the cryosphere. Alongside the other rhythms on this page, it helps us see where Earthâ€™s pulses are steady, where they are drifting, and where the beat is starting to skip.\n\nHow we built the Blink of Winter GIF\nBelow is the R code used to construct the animation from raw MODIS HDF files. It:\n\nReads all MOD10A2.061 HDF files from data/snow-raw/\n\nExtracts the Eight_Day_Snow_Cover subdataset\n\nConverts each raster to a data frame for plotting\n\nRenders each time slice as a PNG using ggplot2\n\nStitches them into a looping GIF with {magick} and saves it as\nimages/planetary-pulse/snow-blink-hdf.gif\n\n\n\nCode\n###############################################\n# Blink of Winter â€“ MODIS HDF Snow GIF       #\n# Dataset: MOD10A2.061 (Eight_Day_Snow_Cover)#\n###############################################\n\nlibrary(terra)    # raster handling\nlibrary(ggplot2)  # plotting\nlibrary(dplyr)    # data wrangling\nlibrary(magick)   # GIF creation\n\n# 1. List HDF files ------------------------------------------------------------\n\n# Assumes project root contains: data/snow-raw/*.hdf\n# If your files truly live at an absolute path like \"/data/snow-raw/\",\n# change this to hdf_dir &lt;- \"/data/snow-raw\".\nhdf_dir &lt;- \"data/snow-raw\"\n\nfiles &lt;- list.files(hdf_dir, pattern = \"\\\\.hdf$\", full.names = TRUE)\nfiles &lt;- sort(files)  # sort by name =&gt; chronological for MOD10A2\n\ncat(\"Found\", length(files), \"HDF files:\\n\")\nprint(files)\n\nif (length(files) == 0) {\n  stop(\"No .hdf files found in \", hdf_dir, \". Check the folder and filenames.\")\n}\n\n# 2. Function to extract Eight_Day_Snow_Cover from one HDF --------------------\n\nget_eightday_snow &lt;- function(hdf_file) {\n  cat(\"\\n--- Processing:\", hdf_file, \"\\n\")\n  \n  d &lt;- describe(hdf_file)\n  \n  # Find the SUBDATASET_2_NAME line = Eight_Day_Snow_Cover in MOD10A2\n  s2_line &lt;- d[grep(\"SUBDATASET_2_NAME\", d)]\n  if (length(s2_line) == 0) {\n    warning(\"No SUBDATASET_2_NAME found in \", hdf_file, \" â€“ skipping.\")\n    return(NULL)\n  }\n  \n  s2_name &lt;- sub(\"  SUBDATASET_2_NAME=\", \"\", s2_line)\n  cat(\"  Using subdataset:\", s2_name, \"\\n\")\n  \n  r &lt;- rast(s2_name)\n  \n  # Quick sanity check: sample some pixels without loading the whole raster\n  vals &lt;- tryCatch(\n    terra::spatSample(r, size = 500, method = \"random\")[, 1],\n    error = function(e) NA\n  )\n  \n  if (all(is.na(vals))) {\n    warning(\"  All sampled values are NA in \", hdf_file, \" â€“ skipping this file.\")\n    return(NULL)\n  }\n  \n  r\n}\n\n# 3. Read all HDFs into a list of rasters -------------------------------------\n\nsnow_list &lt;- lapply(files, get_eightday_snow)\n\n# Remove any NULLs (files that failed)\nsnow_list &lt;- Filter(Negate(is.null), snow_list)\n\ncat(\"\\nKept\", length(snow_list), \"HDF files with valid snow data.\\n\")\n\nif (length(snow_list) == 0) {\n  stop(\"All HDF rasters were empty or unreadable. Cannot build GIF.\")\n}\n\n# 4. Stack into a SpatRaster (time series) ------------------------------------\n\nsnow_stack &lt;- rast(snow_list)\ncat(\"\\nSnow stack:\\n\")\nprint(snow_stack)\n\n# 5. Convert to data frame for ggplot -----------------------------------------\n\ncat(\"\\nConverting raster stack to long data frame for plotting...\\n\")\n\nsnow_df_list &lt;- lapply(1:nlyr(snow_stack), function(i) {\n  r  &lt;- snow_stack[[i]]\n  df &lt;- as.data.frame(r, xy = TRUE, na.rm = TRUE)\n  names(df)[3] &lt;- \"snow\"\n  df$frame &lt;- i\n  df\n})\n\nsnow_df &lt;- bind_rows(snow_df_list)\n\ncat(\"Data frame has\", nrow(snow_df), \"rows across\",\n    length(unique(snow_df$frame)), \"frames.\\n\")\n\n# 6. Create PNG frames --------------------------------------------------------\n\nout_dir &lt;- \"frames/snow_blink_hdf\"\ndir.create(out_dir, showWarnings = FALSE, recursive = TRUE)\n\ncat(\"\\nSaving PNG frames to\", out_dir, \"...\\n\")\n\nfor (f in sort(unique(snow_df$frame))) {\n  df_f &lt;- dplyr::filter(snow_df, frame == f)\n  \n  p &lt;- ggplot(df_f, aes(x, y, fill = snow)) +\n    geom_raster() +\n    # Strong contrast: darker background, brighter snow\n    scale_fill_gradient(low = \"grey10\", high = \"white\") +\n    coord_equal() +\n    labs(title = paste(\"Eight-Day Snow Cover â€” Frame\", f)) +\n    theme_void() +\n    theme(\n      plot.title = element_text(hjust = 0.5, size = 14)\n    )\n  \n  ggsave(\n    filename = sprintf(\"%s/frame_%03d.png\", out_dir, f),\n    plot   = p,\n    width  = 6,\n    height = 4,\n    dpi    = 150\n  )\n}\n\ncat(\"PNG frames written.\\n\")\n\n# 7. Stitch PNGs into animated GIF --------------------------------------------\n\ncat(\"\\nBuilding GIF from PNG frames...\\n\")\n\npng_files &lt;- list.files(out_dir, pattern = \"^frame_.*\\\\.png$\", full.names = TRUE)\npng_files &lt;- sort(png_files)\n\nif (length(png_files) == 0) {\n  stop(\"No PNG frames found in \", out_dir, \". Cannot create GIF.\")\n}\n\nimg_list  &lt;- lapply(png_files, image_read)\nanimation &lt;- image_animate(do.call(c, img_list), fps = 4, loop = 0)\n\ngif_dir  &lt;- \"images/planetary-pulse\"\ndir.create(gif_dir, showWarnings = FALSE, recursive = TRUE)\n\ngif_path &lt;- file.path(gif_dir, \"snow-blink-hdf.gif\")\nimage_write(animation, gif_path)\n\ncat(\"\\nGIF written to:\", gif_path, \"\\n\")\ncat(\"Done. â„ï¸\\n\")\n\n\n\n\n\nğŸ”¥ Rhythm 3: The Fire Pulse â€” Earthâ€™s Flickers\nFire activity has a rhythm too â€” annual fire seasons.\n\nVisual 2 â€” The Barcode of Fire\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\n\nset.seed(23)\n\nweeks &lt;- 1:52\nfire_counts &lt;- abs(rnorm(52, mean = sin(2*pi*(weeks/52))*40 + 50, sd = 20))\n\nfire_df &lt;- tibble(week = weeks, fires = fire_counts)\n\nggplot(fire_df, aes(week, fires)) +\n  geom_col(width=0.9) +\n  labs(title=\"The Barcode of Fire\",\n       subtitle=\"A simulated fire season pattern â€” calm â†’ ignition â†’ peak â†’ fade\",\n       x=\"Week of Year\", y=\"Fire Detections\") +\n  theme_minimal(base_size = 15)\n\n\n\n\n\n\n\n\n\nTall spikes â†’ fire season.\nFlat lines â†’ off season.\n\n\n\nğŸ§­ The Big Discovery â€” The Rhythm Breakers\n\nSome regions of Earth show rhythm â€œbreaksâ€â€” abrupt shifts in their natural cycles that donâ€™t match climate patterns, seasons, or geography.\n\nThese â€œrhythm breaksâ€ often align with:\n\nirrigation expansion\ndam construction\nmegafires\nrapid urbanization\nmining pits\ndesert irrigation circular fields\nsudden ecosystem collapse\n\nMost scientific papers analyze trends.\nAlmost none analyze rhythm stability.\nWe are opening a completely new door.\n\n\nğŸ”® Where This Leads â€” Future Explorations\nThis single idea â€” the stability of Earthâ€™s pulses â€” gives a roadmap for future stories:\n\nWhich forests are losing the strength of their green pulse?\nWhere are fire seasons changing shape most rapidly?\nAre winter cycles becoming less predictable?\nDo human-made interruptions leave detectable â€œscarsâ€ in the rhythm?\nWhat regions show new rhythms that never existed before?\n\nThis internal consistency creates a whole series of original, planetary tales.\n\n Dataset: NASA MODIS\n\n\n What do these icons mean? \n\n\n\nğŸ“š References & Credits\n\nData Sources\n\nNASA MODIS Terra & Aqua Imagery\nNASA Earth Observatory\nNASA FIRMS (Fire Information for Resource Management System)\nNOAA Snow & Ice Data\nUSGS Landsat Archive\n\n\n\nConcepts\n\nNormalized Difference Vegetation Index (NDVI)\nSeasonal snow cover extent\nFire hotspot detection\nLand surface temperature\n\n\n\nR Packages Used\n\ntidyverse â€” data wrangling\nggplot2 â€” visualizations\nterra or raster (future expansions)\nsf (spatial data)\n\n\n\nCredits\n\nSatellite imagery courtesy of NASA / USGS\nConceptual framing and narrative by Insightful Tales (original work)\nAll simulated plots here are illustrative; real satellite derivatives will be added in later iterations"
  },
  {
    "objectID": "stories/the-hidden-rhythm-of-baseball.html",
    "href": "stories/the-hidden-rhythm-of-baseball.html",
    "title": "The Hidden Rhythm of Baseball",
    "section": "",
    "text": "A hundred years of baseball heard, not just measured â€” where numbers whisper the story of how the game learned to breathe differently with each generation."
  },
  {
    "objectID": "stories/the-hidden-rhythm-of-baseball.html#the-game-that-breathes",
    "href": "stories/the-hidden-rhythm-of-baseball.html#the-game-that-breathes",
    "title": "The Hidden Rhythm of Baseball",
    "section": "ğŸ§¢ The Game That Breathes",
    "text": "ğŸ§¢ The Game That Breathes\nBaseball isnâ€™t timed by a clock, but it still has a tempo. Some eras feel quick and scrappy, others patient and thunderous. In this story, weâ€™ll listen to that tempo using longâ€‘run league averages. If you donâ€™t speak data â€” no problem. Think of every chart as a stethoscope on the gameâ€™s chest.\n\nğŸ§° What data are we using?\nWeâ€™ll use the wellâ€‘known Lahman database (public, free) through R. Weâ€™ll count simple things anyone can grasp: runs, home runs, walks, strikeouts, and stolen bases. From these counts, we compute commonsense rates, like â€œhome runs per plate appearance.â€\n\n\nCode\nlibrary(tidyverse)\nlibrary(Lahman)\n\n# Safe rate helper\nrate &lt;- function(n, d) ifelse(d &gt; 0, n / d, NA_real_)\n\n# Build league-level season summary from Teams\nseason &lt;- Teams %&gt;%\n  transmute(\n    year = yearID,\n    g   = G,\n    r   = R,\n    hr  = HR,\n    bb  = BB,\n    so  = SO,\n    sb  = SB,\n    ab  = AB,\n    hbp = coalesce(HBP, 0L),\n    sf  = coalesce(SF, 0L),\n    pa  = AB + BB + hbp + sf\n  ) %&gt;%\n  group_by(year) %&gt;%\n  summarise(\n    g      = sum(g,  na.rm = TRUE),\n    r_pg   = rate(sum(r,  na.rm = TRUE), g),\n    # Use summed plate appearances; if pa has gaps early on, fall back to AB+BB\n    pa_sum = sum(pa, na.rm = TRUE),\n    abbb   = sum(ab + bb, na.rm = TRUE),\n    denom  = ifelse(pa_sum &gt; 0, pa_sum, abbb),\n\n    hr_pa  = rate(sum(hr, na.rm = TRUE), denom),\n    bb_pa  = rate(sum(bb, na.rm = TRUE), denom),\n    so_pa  = rate(sum(so, na.rm = TRUE), denom),\n    sb_pa  = rate(sum(sb, na.rm = TRUE), denom)\n  ) %&gt;%\n  ungroup() %&gt;%\n  arrange(year) %&gt;%\n  mutate(\n    # 5-year right-aligned moving average using base R (no zoo needed)\n    rpg_roll5 = as.numeric(stats::filter(r_pg, rep(1/5, 5), sides = 1)),\n    tto_share = hr_pa + bb_pa + so_pa,\n    bip_share = pmax(0, 1 - tto_share)\n  )\n\n\n\nDecoder\n\nper PA â‰ˆ per trip to the plate.\nRuns per game = average runs a team scores in a game that year.\nBalls in play = anything that isnâ€™t a HR, walk, or strikeout."
  },
  {
    "objectID": "stories/the-hidden-rhythm-of-baseball.html#a-heartbeat-you-can-see-runs-per-game",
    "href": "stories/the-hidden-rhythm-of-baseball.html#a-heartbeat-you-can-see-runs-per-game",
    "title": "The Hidden Rhythm of Baseball",
    "section": "ğŸ¼ A Heartbeat You Can See: Runs Per Game",
    "text": "ğŸ¼ A Heartbeat You Can See: Runs Per Game\nIf the game were breathing, runs per game would be its chest rising and falling. Peaks = lively offense. Valleys = pitcherâ€‘dominated eras.\n\n\nCode\nseason %&gt;%\n    ggplot(aes(year, r_pg)) +\n    geom_line(alpha = 0.85) +\n    geom_line(aes(y = rpg_roll5), linewidth = 1.1) +\n    labs(\n        title = \"Runs per Team Game over Time\",\n        subtitle = \"The 5â€‘year smoother reveals gentle waves rather than a straight trend\",\n        x = NULL, y = \"Runs per game\"\n    )\n\n\n\n\n\n\n\n\n\nWhat to notice: The line waves. Offense doesnâ€™t simply climb forever â€” it surges and recedes across generations."
  },
  {
    "objectID": "stories/the-hidden-rhythm-of-baseball.html#fewer-things-happen-louder-when-they-do",
    "href": "stories/the-hidden-rhythm-of-baseball.html#fewer-things-happen-louder-when-they-do",
    "title": "The Hidden Rhythm of Baseball",
    "section": "ğŸ’¥ Fewer Things Happen, Louder When They Do",
    "text": "ğŸ’¥ Fewer Things Happen, Louder When They Do\nModern baseball increasingly ends in one of three ways: home run, walk, or strikeout. That means fewer balls are fielded. The game can feel slower between bursts of action, even while the big moments are bigger.\n\n\nCode\nseason %&gt;%\n    select(year, hr_pa, bb_pa, so_pa) %&gt;%\n    pivot_longer(-year, names_to = \"metric\", values_to = \"rate\"\n    ) %&gt;%\n    mutate(\n        metric = recode(metric,\n        hr_pa = \"Home runs per PA\",\n        bb_pa = \"Walks per PA\",\n        so_pa = \"Strikeouts per PA\")\n    ) %&gt;%\n    ggplot(aes(year, rate, linetype = metric)) +\n    geom_line() +\n    labs(\n        title = \"The Three True Outcomes over Time\",\n        subtitle = \"Share per plate appearance (league average)\",\n        x = NULL, y = \"Rate\"\n    )\n\n\n\n\n\n\n\n\n\nWhat to notice: Strikeouts climb steadily; home runs rise in bursts; walks bob up and down. Together they take up more of the action pie."
  },
  {
    "objectID": "stories/the-hidden-rhythm-of-baseball.html#thread-count-how-much-play-is-in-play",
    "href": "stories/the-hidden-rhythm-of-baseball.html#thread-count-how-much-play-is-in-play",
    "title": "The Hidden Rhythm of Baseball",
    "section": "ğŸ§¶ Thread Count: How Much Play Is In Play?",
    "text": "ğŸ§¶ Thread Count: How Much Play Is In Play?\nLetâ€™s peek at the share of balls in play â€” the portion of plate appearances that require a fielder to do something.\n\n\nCode\nseason %&gt;%\n    ggplot(aes(year, bip_share)) +\n    geom_line() +\n    labs(\n        title = \"Balls In Play as a Share of All Plate Appearances\",\n        subtitle = \"When this falls, the game has fewer fielding plays and more allâ€‘orâ€‘nothing outcomes\",\n        x = NULL, y = \"Share (0â€“1)\"\n    )\n\n\n\n\n\n\n\n\n\nWhat to notice: When this dips, the gameâ€™s rhythm changes â€” fewer sprints and throws, more waiting for the next big swing."
  },
  {
    "objectID": "stories/the-hidden-rhythm-of-baseball.html#speeds-comeback-arc",
    "href": "stories/the-hidden-rhythm-of-baseball.html#speeds-comeback-arc",
    "title": "The Hidden Rhythm of Baseball",
    "section": "ğŸƒ Speedâ€™s Comeback Arc",
    "text": "ğŸƒ Speedâ€™s Comeback Arc\nSteals add heartbeat. Their rise and fall shows how teams toggle between risk and power.\n\n\nCode\nseason %&gt;%\n    ggplot(aes(year, sb_pa)) +\n    geom_line() +\n    labs(\n        title = \"Stolen Bases per Plate Appearance\",\n        subtitle = \"Risk tolerance in motion\",\n        x = NULL, y = \"SB per PA\"\n    )\n\n\n\n\n\n\n\n\n\nWhat to notice: Cycles. The sport rediscovering speed isnâ€™t nostalgia â€” itâ€™s ecology. When defenses and pitchers neutralize power, movement becomes valuable again."
  },
  {
    "objectID": "stories/the-hidden-rhythm-of-baseball.html#a-simple-zeit-rhythm-index",
    "href": "stories/the-hidden-rhythm-of-baseball.html#a-simple-zeit-rhythm-index",
    "title": "The Hidden Rhythm of Baseball",
    "section": "ğŸŒ€ A Simple â€œZeit Rhythmâ€ Index",
    "text": "ğŸŒ€ A Simple â€œZeit Rhythmâ€ Index\nTo summarize the feel of an era, we blend four dials:\n\nPower (HR/PA)\nMiss (SO/PA)\nMovement (SB/PA)\nPlay in the Field (Balls in play share, but we subtract it â€” less BIP = more allâ€‘orâ€‘nothing feel)\n\nThe result is a single line: higher = louder, burstier baseball; lower = more contactâ€‘heavy flow.\n\n\nCode\nzi &lt;- season %&gt;%\n  mutate(across(c(hr_pa, so_pa, sb_pa, bip_share), scale)) %&gt;%\n  mutate(ZI = as.numeric(hr_pa + so_pa + sb_pa - bip_share)) %&gt;%\n  mutate(ZI_roll5 = as.numeric(stats::filter(ZI, rep(1/5, 5), sides = 1))) %&gt;%\n  select(year, ZI, ZI_roll5)\n\n\n\nzi %&gt;% \n    ggplot(aes(year, ZI)) +\n    geom_hline(yintercept = 0, linewidth = 0.3) +\n    geom_line(alpha = 0.55) +\n    geom_line(aes(y = ZI_roll5), linewidth = 1.1) +\n    labs(\n        title = \"Zeit Rhythm Index\",\n        subtitle = \"A blended, zâ€‘scored feel of power + miss + movement âˆ’ balls in play\",\n        x = NULL, y = \"Index (0 â‰ˆ average era)\"\n    )\n\n\n\n\n\n\n\n\n\nWhat to notice: The line oscillates in multiâ€‘decade waves rather than drifting randomly â€” a hint that baseball selfâ€‘corrects when any one style takes over."
  },
  {
    "objectID": "stories/the-hidden-rhythm-of-baseball.html#what-this-suggests-next",
    "href": "stories/the-hidden-rhythm-of-baseball.html#what-this-suggests-next",
    "title": "The Hidden Rhythm of Baseball",
    "section": "ğŸ”­ What This Suggests Next",
    "text": "ğŸ”­ What This Suggests Next\n\nContact renaissance: As pitchers and defenses optimize for strikeouts and power prevention, putting the ball in play may regain value â€” not as nostalgia, but as an edge.\nSmart speed: Selective stealing and hitâ€‘andâ€‘run tactics may return in pockets where they exploit pitcher/catcher timing.\nFielding theater: If leagues want more visible action, rule tweaks that increase balls in play could shift the rhythm again."
  },
  {
    "objectID": "stories/the-hidden-rhythm-of-baseball.html#reproducibility-notes",
    "href": "stories/the-hidden-rhythm-of-baseball.html#reproducibility-notes",
    "title": "The Hidden Rhythm of Baseball",
    "section": "ğŸ“¦ Reproducibility Notes",
    "text": "ğŸ“¦ Reproducibility Notes\n\nSource: The public Lahman database via the Lahman R package.\nAverages: We averaged teams into a single league line per year to hear the broad rhythm (details will vary by ballpark or league).\nApproximations: Older seasons are missing some counts (like sacrifice flies). We used simple, conservative fallbacks that donâ€™t change the big picture.\nSmoothing: A gentle 5â€‘year average makes the waves easier to see.\n\n\n Dataset: Lahman Baseball Database\n\n\n What do these icons mean?"
  },
  {
    "objectID": "stories/the-hidden-rhythm-of-baseball.html#references-credits",
    "href": "stories/the-hidden-rhythm-of-baseball.html#references-credits",
    "title": "The Hidden Rhythm of Baseball",
    "section": "ğŸ“š References & Credits",
    "text": "ğŸ“š References & Credits\n\nSean Lahman â€” Lahman Baseball Database (public historical stats).\nLahman (R package, CRAN) â€” convenient access to the database.\ntidyverse (CRAN) â€” data wrangling and plotting in R.\nzoo (CRAN) â€” rolling averages.\n\nInspirations: FanGraphs and Baseballâ€‘Reference for longstanding public analytics; the broader sabermetric community for opening up the conversation."
  },
  {
    "objectID": "stories/geography-of-luck.html",
    "href": "stories/geography-of-luck.html",
    "title": "The Geography of Luck",
    "section": "",
    "text": "What we call â€œluckâ€ often begins with latitude and longitude. This story uncovers how place, mobility, and systems intertwine to shape the fortunes of people and nations."
  },
  {
    "objectID": "stories/geography-of-luck.html#premise",
    "href": "stories/geography-of-luck.html#premise",
    "title": "The Geography of Luck",
    "section": "ğŸ’¡ Premise",
    "text": "ğŸ’¡ Premise\nIf you were to spin a globe and stop it at random, the odds of landing somewhere prosperous are not uniform. Latitude has long acted as the silent architect of opportunity â€” shaping climate, agriculture, disease, and, through them, destiny.\nThis story asks: Does geography still rule luck in the 21st century?"
  },
  {
    "objectID": "stories/geography-of-luck.html#data-sources",
    "href": "stories/geography-of-luck.html#data-sources",
    "title": "The Geography of Luck",
    "section": "ğŸŒ Data Sources",
    "text": "ğŸŒ Data Sources\nWe combine multiple open datasets (2020 snapshot for a clean cross-section):\n\n\n\n\n\n\n\n\nTheme\nSource\nIndicator\n\n\n\n\nEconomic output\nWorld Bank\nGDP per capita (constant USD, NY.GDP.PCAP.KD)\n\n\nHealth\nWorld Bank\nLife expectancy at birth (SP.DYN.LE00.IN)\n\n\nGeography\nNatural Earth\nCountry boundaries + ISO3 (for joins)"
  },
  {
    "objectID": "stories/geography-of-luck.html#first-glance-the-shape-of-prosperity",
    "href": "stories/geography-of-luck.html#first-glance-the-shape-of-prosperity",
    "title": "The Geography of Luck",
    "section": "ğŸŒ± First Glance â€” The Shape of Prosperity",
    "text": "ğŸŒ± First Glance â€” The Shape of Prosperity\n\n\nCode\n# GDP vs |latitude|\nggplot(wb, aes(abs_lat, gdp)) +\n  geom_point(aes(color = region), alpha = .7, size = 2, show.legend = FALSE) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"black\") +\n  scale_y_log10(labels = label_dollar()) +\n  labs(title = \"The Geography of Luck\",\n       subtitle = \"GDP per capita (log scale) vs. absolute latitude, 2020 (World Bank)\",\n       x = \"Absolute Latitude (Â°)\", y = \"GDP per capita (2015 USD, log scale)\")\n\n\n\n\n\n\n\n\n\nReading: Prosperity tends to rise away from the equator, plateau in the midâ€‘latitudes, and soften again near the poles â€” a climatic parabola of fortune."
  },
  {
    "objectID": "stories/geography-of-luck.html#beyond-wealth-life-follows-latitude",
    "href": "stories/geography-of-luck.html#beyond-wealth-life-follows-latitude",
    "title": "The Geography of Luck",
    "section": "ğŸ’° Beyond Wealth â€” Life Follows Latitude",
    "text": "ğŸ’° Beyond Wealth â€” Life Follows Latitude\n\n\nCode\n# Life expectancy vs |latitude|\nwb |&gt; ggplot(aes(abs_lat, life)) +\n  geom_point(alpha = .6, size = 2, color = \"#555\") +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"#111\") +\n  labs(title = \"Latitude vs. Life Expectancy\",\n       subtitle = \"Life expectancy at birth vs. absolute latitude, 2020\",\n       x = \"Absolute Latitude (Â°)\", y = \"Years\")\n\n\n\n\n\n\n\n\n\nReading: Health shadows wealth: life expectancy arcs with latitude, but with notable outliers."
  },
  {
    "objectID": "stories/geography-of-luck.html#exceptions-when-geography-loses-residuals",
    "href": "stories/geography-of-luck.html#exceptions-when-geography-loses-residuals",
    "title": "The Geography of Luck",
    "section": "ğŸŒ Exceptions â€” When Geography Loses (Residuals)",
    "text": "ğŸŒ Exceptions â€” When Geography Loses (Residuals)\nWe fit a simple model of wealth on latitude and look for countries that outperform what geography alone would predict.\n\n\nCode\n# Fit on complete cases only, then join residuals back by iso3c\nwb_cc &lt;- wb |&gt; dplyr::filter(is.finite(gdp_log), is.finite(abs_lat), !is.na(iso3c))\nmodel_simple &lt;- lm(gdp_log ~ abs_lat, data = wb_cc)\nres_tbl &lt;- tibble::tibble(iso3c = wb_cc$iso3c, resid_gdp_lat = unname(resid(model_simple)))\n\nwb &lt;- wb |&gt; dplyr::left_join(res_tbl, by = \"iso3c\") |&gt;\n  dplyr::mutate(luck_index = as.numeric(scale(resid_gdp_lat)))\n\n# Top/bottom outperformers by residuals (ignore NA)\nout_top &lt;- wb |&gt; dplyr::filter(!is.na(resid_gdp_lat)) |&gt;\n  dplyr::slice_max(resid_gdp_lat, n = 10) |&gt;\n  dplyr::select(country, region, resid_gdp_lat)\n\nout_bot &lt;- wb |&gt; dplyr::filter(!is.na(resid_gdp_lat)) |&gt;\n  dplyr::slice_min(resid_gdp_lat, n = 10) |&gt;\n  dplyr::select(country, region, resid_gdp_lat)\n\nknitr::kable(out_top, digits = 2, caption = \"Top 10 'Lucky Defiers' â€” richer than latitude alone predicts (log residual)\")\n\n\n\nTop 10 â€˜Lucky Defiersâ€™ â€” richer than latitude alone predicts (log residual)\n\n\ncountry\nregion\nresid_gdp_lat\n\n\n\n\nSingapore\nEast Asia & Pacific\n1.42\n\n\nCayman Islands\nLatin America & Caribbean\n1.18\n\n\nBermuda\nNorth America\n1.06\n\n\nBrunei Darussalam\nEast Asia & Pacific\n1.05\n\n\nMonaco\nEurope & Central Asia\n1.05\n\n\nQatar\nMiddle East & North Africa\n0.95\n\n\nGuam\nEast Asia & Pacific\n0.93\n\n\nVirgin Islands (U.S.)\nLatin America & Caribbean\n0.88\n\n\nHong Kong SAR, China\nEast Asia & Pacific\n0.86\n\n\nUnited Arab Emirates\nMiddle East & North Africa\n0.82\n\n\n\n\n\n\n\nCode\nknitr::kable(out_bot, digits = 2, caption = \"Bottom 10 â€” poorer than latitude alone predicts (log residual)\")\n\n\n\nBottom 10 â€” poorer than latitude alone predicts (log residual)\n\n\ncountry\nregion\nresid_gdp_lat\n\n\n\n\nAfghanistan\nSouth Asia\n-1.26\n\n\nKyrgyz Republic\nEurope & Central Asia\n-1.10\n\n\nSyrian Arab Republic\nMiddle East & North Africa\n-1.09\n\n\nMadagascar\nSub-Saharan Africa\n-1.09\n\n\nMozambique\nSub-Saharan Africa\n-1.05\n\n\nTajikistan\nEurope & Central Asia\n-0.99\n\n\nBurundi\nSub-Saharan Africa\n-0.99\n\n\nUkraine\nEurope & Central Asia\n-0.92\n\n\nLesotho\nSub-Saharan Africa\n-0.91\n\n\nNiger\nSub-Saharan Africa\n-0.86\n\n\n\n\n\n\nInterpretation: Highâ€‘performers often pair trade centrality, education, or resource rents with policy stability. Underâ€‘performers frequently face governance or conflict frictions."
  },
  {
    "objectID": "stories/geography-of-luck.html#optional-map-the-luck-index-residual-choropleth",
    "href": "stories/geography-of-luck.html#optional-map-the-luck-index-residual-choropleth",
    "title": "The Geography of Luck",
    "section": "ğŸ—ºï¸ Optional Map â€” The Luck Index (Residual Choropleth)",
    "text": "ğŸ—ºï¸ Optional Map â€” The Luck Index (Residual Choropleth)\nA choropleth map of the residuals (â€œluck indexâ€) highlights countries that overâ€‘ or underâ€‘perform relative to their absolute latitude.\n\n\nCode\n# Luck-index choropleth (guarded)\nif (isTRUE(has_ne)) {\n  # Merge residuals back to geometry\n  map_dat &lt;- dat |&gt; dplyr::left_join(wb |&gt; dplyr::select(iso3c, resid_gdp_lat), by = c(\"iso_a3\" = \"iso3c\"))\n\n  # Breaks for a symmetric diverging map around zero residual\n  brks &lt;- c(-Inf, -0.6, -0.3, -0.15, 0, 0.15, 0.3, 0.6, Inf)\n  map_dat$res_bin &lt;- cut(map_dat$resid_gdp_lat, breaks = brks, include.lowest = TRUE)\n\n  # Plot\n  ggplot(map_dat) +\n    geom_sf(aes(fill = res_bin), color = \"white\", size = 0.1) +\n    scale_fill_brewer(type = \"div\", palette = \"RdBu\", direction = -1, na.value = \"#e5e7eb\",\n                      name = \"Luck index\n(GDP~vs~|lat| residual)\") +\n    labs(title = \"Outperformers and Underperformers â€” Geography of Luck\",\n         subtitle = \"Residuals from log(GDP per capita) ~ absolute latitude, 2020\",\n         caption = \"Data: World Bank (WDI), Natural Earth. Negative = underperforming latitude; positive = outperforming.\") +\n    theme(legend.position = \"right\")\n} else {\n  message(\"Optional map disabled: install.packages(c('sf','rnaturalearth','rnaturalearthdata')) to enable the choropleth.\")\n}\n\n\n\n\n\n\n\n\n\nHow to read: Blue shades underperform their latitude (negative residuals); red shades outperform (positive residuals). Neutral grays are missing data."
  },
  {
    "objectID": "stories/geography-of-luck.html#reflection",
    "href": "stories/geography-of-luck.html#reflection",
    "title": "The Geography of Luck",
    "section": "ğŸ’­ Reflection",
    "text": "ğŸ’­ Reflection\n\nThe sun may rise for everyone, but it still shines longer for some.\n\nLatitude once dictated agriculture, disease, and labor. Today, it still whispers through economies â€” a relic of environmental inheritance.\nYet each bright outlier on the map is a defiance of fate: policy and ingenuity turning geography into geographyâ€™s undoing.\n\n\n Dataset: nycflights13 \n\n\n What do these icons mean?"
  },
  {
    "objectID": "stories/geography-of-luck.html#technical-appendix",
    "href": "stories/geography-of-luck.html#technical-appendix",
    "title": "The Geography of Luck",
    "section": "âš™ï¸ Technical Appendix",
    "text": "âš™ï¸ Technical Appendix\n\nPackages: WDI, rnaturalearth, sf, tidyverse, janitor, scales\n\nYear: 2020 snapshot (World Bank)\n\nTransforms: log10(GDP per capita); residuals from lm(gdp_log ~ abs_lat)\n\nCRS: EPSG:4326 (WGS84)\n\nReproducibility: Set a specific year for comparability; extend to multiâ€‘year to track weakening/strengthening latitude effects.\n\n\n\nCode\n# Time evolution â€” correlation weakening/strengthening over time (1980â€“2023)\nhist &lt;- WDI(indicator = c(gdp = \"NY.GDP.PCAP.KD\"),\n            start = 1980, end = 2023, extra = TRUE) |&gt;\n  janitor::clean_names() |&gt;\n  dplyr::filter(region != \"Aggregates\") |&gt;\n  dplyr::mutate(\n    # make sure latitude is numeric before abs()\n    latitude = suppressWarnings(readr::parse_number(as.character(latitude))),\n    abs_lat  = abs(latitude),\n    gdp_log  = log10(gdp)\n  ) |&gt;\n  dplyr::group_by(year) |&gt;\n  dplyr::summarize(\n    r_lat_gdp = cor(abs_lat, gdp_log, use = \"complete.obs\"),\n    .groups = \"drop\"\n  )\n\nggplot(hist, aes(year, r_lat_gdp)) +\n  geom_line() +\n  labs(\n    title = \"Latitudeâ€“Wealth Correlation Over Time\",\n    x = \"Year\", y = \"Correlation r (|lat| vs log GDP per capita)\"\n  )"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Insightful Tales",
    "section": "",
    "text": "Insightful Tales is where numbers find their voice. Through thoughtful storytelling, rich visuals, and clear analysis, the site turns datasets into human-focused narratives about how the world moves, grows, and remembers."
  },
  {
    "objectID": "index.html#featured-stories",
    "href": "index.html#featured-stories",
    "title": "Insightful Tales",
    "section": "âœ¨ Featured Stories",
    "text": "âœ¨ Featured Stories\nEach story turns data into narrative â€” click to explore.\n\n\nğŸ§­ Geography of Luck â€” The Latitude Lottery\n\nAirports closer to the poles face harsher weather â€” but does geography itself decide punctuality?\nThis story measures how latitude quietly skews flight reliability, uncovering patterns of chance, climate, and coordination.\n\n\n\nğŸ¦‹ Butterfly Morning Delays â€” Small Causes, Wide Ripples\n\nA delay at dawn can echo across an entire air-traffic network.\nUsing NYC flight data, this tale visualizes how minute perturbations evolve into widespread effects â€” a modern butterfly effect.\n\n\n\nğŸŒ¦ï¸ Weather Memory â€” How the Sky Remembers\n\nDoes weather have â€œmomentumâ€?\nBy correlating daily anomalies, this piece explores how temperature and precipitation persist â€” revealing the hidden inertia of the atmosphere.\n\n\n\nâš¾ Baseball Zeitgeist â€” A Century of Swings\n\nUsing Lahmanâ€™s database, this story blends sport, sociology, and technology to show how Americaâ€™s game mirrors its evolving spirit â€” from dead-ball grit to data-driven power.\n\n\n\nğŸ’° The Wealth of Nations, Revisited â€” Gapminder Tales\n\nA visual narrative on global progress and paradox: who truly caught up, who stalled, and what it means when GDP rises but well-being lags behind.\n\n\n\nğŸªï¸ Planetary Pulse â€” Reading Earth from Orbit\n\nSatellite imagery meets storytelling.\nFrom greening deserts to vanishing glaciers, this piece turns NASAâ€™s pixels into a planetary diary of change.\n\n\n\nğŸš‡ London in Motion â€” The Pulse of a City\n\nEvery entry, exit, and delay tells a story of urban life.\nThis narrative transforms Transport for London data into a living map of rhythm, pressure, and resilience.\n\n\n\nğŸ•¹ï¸ï¸ The Hype Curve â€” Life and Death of a Game\n\nSteam data visualized as cultural heartbeat:\nhow trends surge, communities form, and nostalgia revives titles long thought dormant.\n\n\n\nğŸŒ† Lights and Lives â€” Cities from Space\n\nNight-light data reveals how urban sprawl, inequality, and energy intersect.\nA story told not in words but in luminous patterns across the Earthâ€™s surface."
  },
  {
    "objectID": "index.html#origins-evolution-of-data",
    "href": "index.html#origins-evolution-of-data",
    "title": "Insightful Tales",
    "section": "ğŸ§¬ Origins & Evolution of Data",
    "text": "ğŸ§¬ Origins & Evolution of Data\nEvery number has an origin story. These tales uncover how data was born, who shaped it, and how it came to define the modern world.\n\n\nğŸ§® The Genesis of Data â€” When Counting Became Knowing\n\nFrom ancient tablets to industrial ledgers, the human instinct to measure the world laid the groundwork for todayâ€™s data revolution."
  },
  {
    "objectID": "index.html#featured-datasets-where-stories-begin",
    "href": "index.html#featured-datasets-where-stories-begin",
    "title": "Insightful Tales",
    "section": "ğŸ“š Featured Datasets â€” Where Stories Begin",
    "text": "ğŸ“š Featured Datasets â€” Where Stories Begin\nEach of these datasets offers more than numbersâ€”theyâ€™re story engines. Click to expand.\n\n\n\n nycflights13 â€” The DataSet That Took Off\n\nWhat it is: All flights departing NYC in 2013, linked to weather, airlines, and airports.\nWhy itâ€™s worthy: Rich joins (flights â‡„ weather â‡„ carriers) + time series + networks.\nStory sparks: Morning fog as nationwide dominoes; how buffer time beats chronic delay; the â€œgeography of luckâ€ in departure times.\n\n\n\n\n Gapminder Dataset â€” A History of Human Progress, Told Through Data\n\nWhat it is: Country-level health, wealth, and population over time.\nWhy itâ€™s worthy: Long horizons + comparable units = clean cross-country narratives.\nStory sparks: Convergence vs.Â divergence; health gains without wealth; â€œlate bloomersâ€ that flip the script.\n\n\n\n\nLahman Baseball Database â€” A Chronicle of the Gameâ€™s Digital Memory\n\nWhat it is: Player/team stats from the 19th century to now.\nWhy itâ€™s worthy: Deep history, rule changes, moneyball erasâ€”quant + culture.\nStory sparks: Expansion, integration, and the long-ball epochs; globalization in surnames and birthplaces.\n\n\n\n\n  NOAA Daily Weather â€” Memory of the Sky\n\nWhat it is: Decades of daily observations for thousands of stations (GHCN).\nWhy itâ€™s worthy: Local variability meets climate trends; perfect for â€œweather memoryâ€ ideas.\nStory sparks: How yesterday biases today (humans & grids); asymmetries in extremes.\n\n\n\n\n Transport for London (TfL) â€” The Pulse Beneath the City\n\nWhat it is: Open ridership, delays, closures, and more from London Underground.\nWhy itâ€™s worthy: Network effects + peak waves = emergent patterns.\nStory sparks: Fragility maps; how small failures ripple; the true â€œrushâ€ in rush hour.\n\n\n\n\n UCI Online Retail â€” Human Behavior in Transactions\n\nWhat it is: Line-item invoices for a UK e-commerce retailer (2010â€“2011).\nWhy itâ€™s worthy: Basket analysis, seasonality, long-tail dynamics.\nStory sparks: Product co-occurrence graphs; rare combos that drive profit; return behavior arcs.\n\n\n\n\n FRED Economic Data â€” The Pulse of Economies\n\nWhat it is: US/international macro time series (inflation, jobs, rates).\nWhy itâ€™s worthy: High-frequency, consequential, and composable.\nStory sparks: Recession â€œfingerprintsâ€; sectoral divergence; policy shock timelines.\n\n\n\n\n NASA MODIS / Earth Observatory â€” A Planet in Pixels\n\nWhat it is: Satellite observations (vegetation, fires, aerosols, ice).\nWhy itâ€™s worthy: Visual storytelling + measurable change.\nStory sparks: Greening/ browning cycles; fire seasons as migrating fronts; glacier retreat as lived time.\n\n\n\n\n Steam Games â€” Collective Taste in the Wild\n\nWhat it is: Game metadata, reviews, tags, and (often) playtime stats.\nWhy itâ€™s worthy: Social proof, hype cycles, networked preference landscapes.\nStory sparks: Genre constellations; the life cycle of hits; price elasticity vs.Â review sentiment.\n\n\n\n\n Global Urban Footprint â€” Cities from Space\n\nWhat it is: High-resolution, satellite-derived urban extent worldwide.\nWhy itâ€™s worthy: Comparable geometry of cities across cultures and terrains.\nStory sparks: Edge growth vs.Â infill; coastal constraints; night-lights vs.Â census discrepancies.\n\n\n\n Learn the meaning behind these dataset icons â†’"
  },
  {
    "objectID": "datasets/transport-for-london-tfl-the-pulse-beneath-the-city.html",
    "href": "datasets/transport-for-london-tfl-the-pulse-beneath-the-city.html",
    "title": "Transport for London (TfL) â€” The Pulse Beneath the City",
    "section": "",
    "text": "What does this icon mean?"
  },
  {
    "objectID": "datasets/transport-for-london-tfl-the-pulse-beneath-the-city.html#visualising-station-flow",
    "href": "datasets/transport-for-london-tfl-the-pulse-beneath-the-city.html#visualising-station-flow",
    "title": "Transport for London (TfL) â€” The Pulse Beneath the City",
    "section": "Visualising Station Flow",
    "text": "Visualising Station Flow\n\n\nCode\ntube |&gt;\n    ggplot(aes(x = reorder(station, total), y = total)) +\n    geom_col() +\n    coord_flip() +\n    labs(x = NULL, y = \"Total entries + exits\", title = \"Londonâ€™s busiest stations\") +\n    theme_minimal()\n\n\n\n\n\nTop 10 busiest Tube stations by annual entries and exits."
  },
  {
    "objectID": "datasets/transport-for-london-tfl-the-pulse-beneath-the-city.html#mapping-cycle-hire-patterns",
    "href": "datasets/transport-for-london-tfl-the-pulse-beneath-the-city.html#mapping-cycle-hire-patterns",
    "title": "Transport for London (TfL) â€” The Pulse Beneath the City",
    "section": "Mapping Cycle Hire Patterns",
    "text": "Mapping Cycle Hire Patterns\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(ggspatial)\nlibrary(rosm)\n\n# --- Load CSV ---\ncycle &lt;- read_csv(\"../data/tfl/cycle-hire.csv\", show_col_types = FALSE)\n\n# --- Coordinate lookup (approximate) ---\ncoords &lt;- tribble(\n  ~start_station,           ~lon,    ~lat,\n  \"Waterloo Station\",       -0.113,  51.503,\n  \"King's Cross Station\",   -0.122,  51.530,\n  \"Victoria Station\",       -0.143,  51.495,\n  \"London Bridge Station\",  -0.087,  51.505,\n  \"Paddington Station\",     -0.177,  51.516,\n  \"Stratford Station\",       0.002,  51.543,\n  \"Canary Wharf Station\",   -0.020,  51.505,\n  \"Holborn Station\",        -0.118,  51.517,\n  \"Bank Station\",           -0.089,  51.513,\n  \"Westminster Station\",    -0.125,  51.500\n)\n\n# --- Join & summarise to points (WGS84) ---\ncycle_pts_wgs84 &lt;- cycle |&gt;\n  left_join(coords, by = \"start_station\") |&gt;\n  filter(!is.na(lon), !is.na(lat)) |&gt;\n  group_by(start_station, lon, lat) |&gt;\n  summarise(trips = n(), .groups = \"drop\") |&gt;\n  st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)\n\n# --- Compute a padded bbox from your data & transform to Web-Mercator (EPSG:3857) ---\nif (nrow(cycle_pts_wgs84) == 0) stop(\"No stations matched the coord lookup. Check start_station names.\")\ncycle_pts_3857 &lt;- st_transform(cycle_pts_wgs84, 3857)\n\n# bbox padding helper (10% padding each side)\npad_bbox &lt;- function(bb, pad = 0.10) {\n  x_pad &lt;- (bb[\"xmax\"] - bb[\"xmin\"]) * pad\n  y_pad &lt;- (bb[\"ymax\"] - bb[\"ymin\"]) * pad\n  c(xmin = bb[\"xmin\"] - x_pad,\n    xmax = bb[\"xmax\"] + x_pad,\n    ymin = bb[\"ymin\"] - y_pad,\n    ymax = bb[\"ymax\"] + y_pad)\n}\nbb_3857 &lt;- st_bbox(cycle_pts_3857)\nbb_3857 &lt;- pad_bbox(bb_3857, pad = 0.12)\n\n# --- Plot ---\nggplot() +\n  annotation_map_tile(type = \"osm\", zoomin = 0, progress = \"none\", cachedir = tempdir()) +\n# halo (bigger, semi-transparent)\ngeom_sf(\n  data = cycle_pts_3857,\n  aes(size = trips),\n  color = \"yellow\",\n  alpha = 0.65,\n  size  = 4,        # fixed base size for the glow\n  show.legend = FALSE\n) +\n# core point (smaller, solid)\ngeom_sf(\n  data = cycle_pts_3857,\n  aes(size = trips),\n  shape = 21,\n  fill  = \"springgreen\",   # or \"cyan3\", \"springgreen3\"\n  color = \"yellow\",\n  stroke = 0.8,\n  alpha  = 1\n) +\n  coord_sf(xlim = c(bb_3857[\"xmin\"], bb_3857[\"xmax\"]),\n           ylim = c(bb_3857[\"ymin\"], bb_3857[\"ymax\"]),\n           expand = FALSE, crs = st_crs(3857)) +\n  annotation_scale(location = \"bl\", width_hint = 0.25, text_cex = 0.7) +\n  annotation_north_arrow(location = \"tr\", style = north_arrow_fancy_orienteering) +\n  theme_void() +\n  labs(title = \"Cycle Hire Start Stations (sample data on OSM tiles)\",\n       size = \"Trips\")"
  },
  {
    "objectID": "datasets/the-lahman-baseball-database-a-chronicle-of-the-games-digital-memory.html",
    "href": "datasets/the-lahman-baseball-database-a-chronicle-of-the-games-digital-memory.html",
    "title": "The Lahman Baseball Database â€” A Chronicle of the Gameâ€™s Digital Memory",
    "section": "",
    "text": "âš¾The Lahman Baseball Database â€” A Chronicle of the Gameâ€™s Digital Memory\n\n\nFrom paper box scores to open data, how one project preserved baseballâ€™s statistical soul.\n\n\nExploring the origins, history, and significance of the Lahman Baseball Database â€” a dataset that bridges baseballâ€™s past and data scienceâ€™s future.\n\n\n\n What does this icon mean? \n\n\nğŸ§¾ Origins: From Box Scores to Bytes\nBefore baseball had Statcast, WAR, or machine-tracked pitch velocities, there were box scores â€” neat little tables printed in newspapers, carefully summarizing who hit what and when. These werenâ€™t just numbers; they were the fingerprints of every game ever played.\nIn the early 1990s, Sean Lahman, a journalist and baseball historian, began a labor of love: turning this analog treasure into digital form. His mission was simple but revolutionary â€” to make baseballâ€™s statistical record freely available for everyone, not just researchers with access to proprietary archives.\nThe result was the Lahman Baseball Database, first released in 1995 as a Microsoft Access file distributed on CD-ROMs and early websites. Over time, it became one of the most trusted and comprehensive open datasets in sports history.\n\n\nğŸ•°ï¸ The Database That Evolved With the Game\nAt its core, the Lahman Database is a meticulous record of Major League Baseball (MLB) statistics dating back to 1871. Each table tells a different part of the story:\n\nBatting.csv: every hit, home run, and strikeout\n\nPitching.csv: earned runs, innings pitched, and strikeouts\n\nFielding.csv: defensive performance across positions\n\nTeams.csv and People.csv: linking players, franchises, and eras together\n\nBut its real brilliance lies in its relational structure â€” every player, team, and season connects through IDs and keys, making it a dream for database design students and data scientists alike.\nOver the decades, volunteers, statisticians, and historians have joined Lahman in expanding and refining the dataset. Today itâ€™s updated regularly, available in multiple formats (CSV, SQL, R data frames), and integrated into major data science packages â€” such as the Lahman package in Râ€™s tidyverse ecosystem.\n\n\nğŸ’¡ Why It Matters\nThe Lahman Database was a pioneer of open data in sports â€” long before â€œopen dataâ€ was a buzzword. It democratized baseball analytics, empowering everyone from high school students to professional sabermetricians.\nItâ€™s also a living archive. Through it, we can:\n\nRecreate the history of baseball season by season\n\nMeasure trends in player performance over 150 years\n\nExplore sociological patterns, such as integration and globalization of the sport\n\nTeach database normalization, SQL querying, and R data wrangling through real historical data\n\nWhen the Lahman dataset entered R, it found a new home in education. Countless courses and textbooks use it as the first playground for students learning data analysis. Itâ€™s not just a baseball dataset â€” itâ€™s a bridge between humanities and data science.\n\n\nâš™ï¸ From Database to Story Engine\nWhatâ€™s fascinating is how the Lahman Database transcends its origins. While it began as a record of numbers, it has become a platform for storytelling. Analysts and fans alike use it to investigate everything from â€œWho had the best rookie season ever?â€ to â€œHow did rule changes shape offensive output?â€\nWithin the context of Insightful Tales, this dataset embodies the spirit of â€œwhere data finds its narrative.â€ Every statistic â€” every RBI, every strikeout â€” is a clue in the broader story of how Americaâ€™s pastime evolved alongside technology, society, and the human quest for meaning through numbers.\n\n\nğŸŒ A Community Effort\nThe Lahman Database thrives because itâ€™s open. It has no corporate sponsor or hidden paywall â€” just a community that believes baseballâ€™s history belongs to everyone. It continues to be hosted and maintained on GitHub, where contributors help fix inconsistencies, add metadata, and refine accuracy using digitized historical sources like Retrosheet and Baseball-Reference.\nIts openness has inspired similar projects across other sports â€” from soccerâ€™s fbref datasets to basketballâ€™s BBRef and even Formula 1 archives. In that sense, Lahmanâ€™s vision didnâ€™t just preserve baseballâ€™s memory; it helped seed a movement.\n\n\nğŸ§­ The Philosophy of Preservation\nThereâ€™s something quietly philosophical about what the Lahman Database represents.\nIn a world where so much data is transient â€” scrolling feeds, ephemeral stories â€” this dataset is a monument to memory. It reminds us that keeping careful records is an act of respect: for players, fans, and for time itself.\nThrough rows and columns, it preserves the drama of 150 years of human effort â€” the strikeouts, the comebacks, the improbable heroes. Numbers, when cared for, become history.\n\n\nğŸ“š References & Credits\n\nLahman, S. (1995-present). The Lahman Baseball Database. Retrieved from https://www.seanlahman.com/baseball-archive/statistics/\n\nR Core Team. (2024). Lahman: Sean Lahman Baseball Database (R package). CRAN. https://cran.r-project.org/package=Lahman\n\nRetrosheet. (2024). Play-by-play Data Archive. https://www.retrosheet.org\n\nBaseball-Reference. (2024). Major League Statistics and History. https://www.baseball-reference.com\n\nWickham, H., & Grolemund, G. (2016). R for Data Science. Oâ€™Reilly Media.\n\n\n\nâ€œBaseball, more than any other sport, is numbers â€” but in Lahmanâ€™s database, the numbers tell stories.â€"
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html",
    "title": "âœˆnycflights13: The Dataset That Took Off",
    "section": "",
    "text": "What does this icon mean?"
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html#a-dataset-born-in-the-skies-above-new-york",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html#a-dataset-born-in-the-skies-above-new-york",
    "title": "âœˆnycflights13: The Dataset That Took Off",
    "section": "ğŸ—½ A Dataset Born in the Skies Above New York",
    "text": "ğŸ—½ A Dataset Born in the Skies Above New York\nIn 2013, nearly 337,000 flights departed from New York Cityâ€™s three major airports â€” JFK, LaGuardia (LGA), and Newark (EWR).\nEach of those flights left behind a trace: departure times, delays, destinations, aircraft numbers, even weather conditions at takeoff.\nYears later, these traces would find their way into a small, elegantly crafted R package called nycflights13, and that package would go on to become one of the most iconic learning tools in modern data science."
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html#the-origins-a-teaching-tool-with-real-world-grit",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html#the-origins-a-teaching-tool-with-real-world-grit",
    "title": "âœˆnycflights13: The Dataset That Took Off",
    "section": "ğŸ§­ The Origins: A Teaching Tool With Real-World Grit",
    "text": "ğŸ§­ The Origins: A Teaching Tool With Real-World Grit\nThe story begins with Hadley Wickham, the statistician and software developer who spearheaded much of the tidyverse â€” a family of R packages that revolutionized how data is handled and visualized.\nIn the early 2010s, Wickham was building resources to help people learn data science by doing, not by reading about artificially perfect examples. Real data is messy, relational, and full of missing values â€” and thatâ€™s exactly what students and practitioners need to experience.\nSo instead of another toy dataset, Wickham turned to the U.S. Department of Transportationâ€™s Bureau of Transportation Statistics (BTS), which publishes open flight-on-time data.\nFrom this raw source â€” hundreds of thousands of records across dozens of fields â€” he curated a single-year extract: all flights departing from the New York City area during 2013.\nThen, he went further.\nHe bundled in extra context:\n\nAirlines, mapping carrier codes to their full names.\nAirports, with geographic coordinates and time zones.\nPlanes, including tail numbers, manufacturers, and years built.\nWeather, hourly reports from each NYC airport.\n\nThe result wasnâ€™t just a dataset â€” it was a miniature world of interlocking tables, ready to teach real-world data relationships."
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html#a-dataset-of-relationships",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html#a-dataset-of-relationships",
    "title": "âœˆnycflights13: The Dataset That Took Off",
    "section": "ğŸ§© A Dataset of Relationships",
    "text": "ğŸ§© A Dataset of Relationships\nUnlike simpler datasets like iris or mtcars, nycflights13 is relational â€” meaning its tables connect through shared keys:\n\nFlights â†”ï¸ Airlines (via carrier code)\nFlights â†”ï¸ Airports (via origin and destination codes)\nFlights â†”ï¸ Planes (via tail number)\nFlights â†”ï¸ Weather (via origin + date + hour)\n\nThis structure was intentional. It allows learners to practice joining, grouping, filtering, and summarizing â€” the bread and butter of data science â€” within a single coherent story: why do some flights arrive late, and what factors might explain it?\nWickham described this as â€œa dataset that lets you learn the grammar of data manipulation using real noise, not just clean textbook numbers.â€"
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html#why-2013",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html#why-2013",
    "title": "âœˆnycflights13: The Dataset That Took Off",
    "section": "ğŸ§® Why 2013?",
    "text": "ğŸ§® Why 2013?\nThe year 2013 wasnâ€™t random â€” it was chosen for completeness and modern relevance.\nBy 2014, most of that yearâ€™s data had been validated and publicly available through the BTSâ€™s On-Time Performance database. It also represented a year with consistent reporting across all major U.S. carriers.\n2013 was far enough into the modern airline data era (with GPS and electronic filing) to be reliable, yet recent enough to feel relevant for todayâ€™s analyses."
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html#from-raw-data-to-a-clean-package",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html#from-raw-data-to-a-clean-package",
    "title": "âœˆnycflights13: The Dataset That Took Off",
    "section": "ğŸŒ¦ï¸ From Raw Data to a Clean Package",
    "text": "ğŸŒ¦ï¸ From Raw Data to a Clean Package\nTo transform the messy BTS export into something usable, Wickham and the tidyverse team wrote R scripts to:\n\nParse CSVs from the RITA (Research and Innovative Technology Administration) portal.\nFilter for flights with valid NYC origins.\nClean up time variables (like dep_time and arr_time) into numeric or datetime forms.\nNormalize column names for clarity and consistency.\nAdd weather data from the NOAA Integrated Surface Database (ISD).\n\nThe result was a compact, easily loadable dataset: one command, library(nycflights13), and the world of 2013â€™s New York skies appears at your fingertips."
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html#joining-the-tidyverse",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html#joining-the-tidyverse",
    "title": "âœˆnycflights13: The Dataset That Took Off",
    "section": "ğŸ“¦ Joining the tidyverse",
    "text": "ğŸ“¦ Joining the tidyverse\nWhen it was first released in 2014, nycflights13 joined the ecosystem of demonstration datasets used in the new generation of R tutorials, books, and courses â€” particularly R for Data Science, first published in 2017.\nBy packaging the dataset, Wickham made it instantly reproducible and accessible:\ninstall.packages(\"nycflights13\")\nlibrary(nycflights13)\nIt quickly became the standard dataset for teaching data wrangling (dplyr), tidying (tidyr), and visualization (ggplot2).\n\nToday, it appears in classrooms, Coursera courses, Kaggle tutorials, and research introductions worldwide."
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html#why-it-matters",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html#why-it-matters",
    "title": "âœˆnycflights13: The Dataset That Took Off",
    "section": "ğŸŒ Why It Matters",
    "text": "ğŸŒ Why It Matters\nnycflights13 endures because itâ€™s a microcosm of real life.\nIt captures delay frustrations, winter weather, airline competition, and the randomness of human systems.\nIt also shows how separate data sources â€” flights, airports, weather, and aircraft â€” interconnect to form a coherent story.\nItâ€™s not about New York flights alone; itâ€™s about how we understand and model the world through data.\nIn the same way the iris dataset taught generations of statisticians about classification, nycflights13 has taught generations of data scientists about structure, relationships, and storytelling."
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html#structure-at-a-glance",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html#structure-at-a-glance",
    "title": "âœˆnycflights13: The Dataset That Took Off",
    "section": "ğŸ”§ Structure at a Glance",
    "text": "ğŸ”§ Structure at a Glance\n\n\n\n\n\n\n\n\n\nTable\nDescription\nRows\nKey columns\n\n\n\n\nflights\nEvery NYC departure in 2013\n336,776\nyear, month, day, dep_time, arr_time, carrier, tailnum, origin, dest\n\n\nairlines\nCarrier codes and full names\n16\ncarrier\n\n\nairports\nAll airports in the dataset\n1,458\nfaa\n\n\nplanes\nAircraft manufacturing info\n3,322\ntailnum\n\n\nweather\nHourly conditions for each airport\n26,115\norigin, year, month, day, hour"
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html#legacy-and-influence",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html#legacy-and-influence",
    "title": "âœˆnycflights13: The Dataset That Took Off",
    "section": "ğŸ“˜ Legacy and Influence",
    "text": "ğŸ“˜ Legacy and Influence\nThe datasetâ€™s success inspired a wave of similar projects:\n\nfivethirtyeight â€” datasets from FiveThirtyEight journalism pieces.gapminder â€” Hans Roslingâ€™s world development data.palmerpenguins â€” a modern, ecological replacement for iris.\n\nBut nycflights13 remains unique in how it captures an entire real-world system, cleanly and reproducibly, in just a few megabytes.\nItâ€™s still maintained under the tidyverse organization on GitHub, ensuring compatibility with modern R versions.\nFor many, it was the first real dataset they ever analyzed â€” and itâ€™s still the one they remember."
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html#in-the-end",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html#in-the-end",
    "title": "âœˆnycflights13: The Dataset That Took Off",
    "section": "âœˆï¸ In the End",
    "text": "âœˆï¸ In the End\nnycflights13 is not just about flights.\nItâ€™s about the journey of data â€” from messy public records to a structured, teachable story.\nItâ€™s about curiosity, reproducibility, and accessibility.\nAnd like the air traffic over New York, it continues to connect learners all over the world â€” one tidy join at a time."
  },
  {
    "objectID": "datasets/nycflights13-the-dataset-that-took-off.html#references",
    "href": "datasets/nycflights13-the-dataset-that-took-off.html#references",
    "title": "âœˆnycflights13: The Dataset That Took Off",
    "section": "ğŸ“š References",
    "text": "ğŸ“š References\n\nWickham, H. (2014). nycflights13: Flights that departed NYC in 2013. R package version 0.1.\nCRAN R Project. nycflights13 package documentation. https://cran.r-project.org/web/packages/nycflights13/\nBureau of Transportation Statistics (BTS). On-Time Performance Data. https://transtats.bts.gov/DL_SelectFields.asp?T able_ID=236\nNOAA National Centers for Environmental Information. Integrated Surface Database (ISD).\nWickham, H., & Grolemund, G. (2017). R for Data Science. Oâ€™Reilly Media.\nTidyverse Team. nycflights13 source code repository. https://github.com/tidyverse/nycflights13"
  },
  {
    "objectID": "datasets/index.html",
    "href": "datasets/index.html",
    "title": "Featured Datasets",
    "section": "",
    "text": "ğŸ“Š Featured Datasets â€” Where Stories Begin\nBehind every story lies a dataset waiting to be understood.\nExplore the origins, purpose, and structure of the datasets that fuel Insightful Tales.\nEach entry links to its background page â€” describing how the data was collected, why it matters, and what stories it can tell.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "data-science/index.html",
    "href": "data-science/index.html",
    "title": "Data Science Stories",
    "section": "",
    "text": "ğŸ§® Data Science Stories\nThese are the analytical backbones of Insightful Tales â€”\nnotebooks that explore datasets, test ideas, and illuminate patterns that later become stories.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Iâ€™d love to hear from readers, researchers, or collaborators interested in data storytelling, visualization, or related projects.\n\nğŸ’¡ Feedback or story ideas? Drop a note below.\n\nğŸ§  Collaboration inquiries? Iâ€™m open to cross-disciplinary data projects.\n\nğŸ‘‰ [LinkedIn Profile](https://www.linkedin.com/in/joseph-schaffer-a6270138\nğŸ‘‰ GitHub Repository\nOr email me directly: joseph [at] insightfultales [dot] com"
  },
  {
    "objectID": "contact.html#get-in-touch",
    "href": "contact.html#get-in-touch",
    "title": "Contact",
    "section": "",
    "text": "Iâ€™d love to hear from readers, researchers, or collaborators interested in data storytelling, visualization, or related projects.\n\nğŸ’¡ Feedback or story ideas? Drop a note below.\n\nğŸ§  Collaboration inquiries? Iâ€™m open to cross-disciplinary data projects.\n\nğŸ‘‰ [LinkedIn Profile](https://www.linkedin.com/in/joseph-schaffer-a6270138\nğŸ‘‰ GitHub Repository\nOr email me directly: joseph [at] insightfultales [dot] com"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Insightful Tales",
    "section": "",
    "text": "ğŸ¦‹ Insightful Tales is a creative data storytelling project that transforms datasets into visual, narrative insights.\nEvery story is built from data â€” but told like literature, not statistics."
  },
  {
    "objectID": "data-science/genesis-of-data.html",
    "href": "data-science/genesis-of-data.html",
    "title": "The Genesis of Data â€” When Counting Became Knowing",
    "section": "",
    "text": "Where numbers first became mirrors of the world.\nWe like to imagine data as a modern concept â€” CSV files, databases, machine learning pipelines. But the truth is: data is as old as civilization itself. Before there were scientists, there were record-keepers. Before there were models, there were marks â€” clay scratches, tally sticks, and census scrolls â€” all born from the same instinct: to remember what mattered.\nThe Babylonians were counting grain before they were writing poems. Their cuneiform tablets, pressed by reeds into wet clay, werenâ€™t stories â€” they were inventories. It was there, in those humble lists of barley and livestock, that data first became a language of power. Whoever held the counts held control â€” of taxes, of armies, of life itself.\nBy the time the Romans perfected their census, data had become governance. â€œCensereâ€ meant â€œto assess,â€ and that act of assessment â€” of converting people into numbers â€” laid the foundation for everything from imperial logistics to modern public policy. The Census wasnâ€™t invented by democracy; democracy was, in part, made possible because we learned to count ourselves."
  },
  {
    "objectID": "data-science/genesis-of-data.html#the-spiritual-act-of-counting",
    "href": "data-science/genesis-of-data.html#the-spiritual-act-of-counting",
    "title": "The Genesis of Data â€” When Counting Became Knowing",
    "section": "ï¸ğŸ”¢ The Spiritual Act of Counting",
    "text": "ï¸ğŸ”¢ The Spiritual Act of Counting\nI often think about counting not as a mechanical act, but a spiritual one â€” a human attempt to find order in chaos. Every tally is a quiet declaration that something exists, that it matters enough to be counted.\nFlorence Nightingale understood this profoundly. When she gathered data on deaths in the Crimean War, she wasnâ€™t collecting for curiosityâ€™s sake. She was fighting ignorance with numbers, transforming rows and columns into a moral argument â€” one so compelling that even the Queen could not ignore it. In that sense, she pioneered not just data visualization but data empathy."
  },
  {
    "objectID": "data-science/genesis-of-data.html#when-the-world-scaled-up",
    "href": "data-science/genesis-of-data.html#when-the-world-scaled-up",
    "title": "The Genesis of Data â€” When Counting Became Knowing",
    "section": "ğŸŒ When the World Scaled Up",
    "text": "ğŸŒ When the World Scaled Up\nFast forward to the Victorian era: the world began to count everything. The first statistical societies formed, censuses became national events, and the idea of â€œobjective truth through measurementâ€ took hold. The machine age demanded data â€” and humanity obliged.\nThe story of data from then onward is exponential. By 1900, nearly every industrialized nation had institutionalized statistical offices. By 1950, computers were born to count faster than any human could. By 2000, data had multiplied to such a degree that it began describing us more than we described it."
  },
  {
    "objectID": "data-science/genesis-of-data.html#the-data-of-data",
    "href": "data-science/genesis-of-data.html#the-data-of-data",
    "title": "The Genesis of Data â€” When Counting Became Knowing",
    "section": "ğŸ“Š The Data of Data",
    "text": "ğŸ“Š The Data of Data\nI find something poetic in tracing this lineage with modern tools. Using the World Bankâ€™s historical population dataset, we can visualize how humanityâ€™s capacity to count itself expanded almost in lockstep with its population.\n\n\n\n\n\n\nTip\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(WDI)\nlibrary(janitor)\n\nowid &lt;- get_owid_world_pop()\n\n# --- WDI (1960+) ---\nwdi_raw &lt;- try(\n  WDI(indicator = \"SP.POP.TOTL\",\n      start = 1960,\n      end   = as.integer(format(Sys.Date(), \"%Y\"))),\n  silent = TRUE\n)\n\nif (inherits(wdi_raw, \"try-error\")) {\n  wdi_world &lt;- tibble(year = integer(), pop = numeric())\n} else {\n  # Filter to the global aggregate\n  wdi_world &lt;- wdi_raw %&gt;%\n    filter(iso2c == \"1W\" | country == \"World\") %&gt;%\n    select(year, pop = SP.POP.TOTL)\n}\n\n\n# --- OWID historical (ensure correct columns + units) ---\n\ntmp &lt;- owid %&gt;%\nclean_names() %&gt;%\nrename(pop = population)\n\nif (\"entity\" %in% names(tmp) && any(tolower(tmp$entity) == \"world\")) {\nowid_world &lt;- tmp %&gt;%\nfilter(tolower(entity) == \"world\", !is.na(year), !is.na(pop)) %&gt;%\ndistinct(year, .keep_all = TRUE)\n} else {\n\n# Fallback: aggregate across all entities per year\n\nowid_world &lt;- tmp %&gt;%\ngroup_by(year) %&gt;%\nsummarise(pop = sum(pop, na.rm = TRUE), .groups = \"drop\")\n}\n\n# Normalize OWID units if provided in millions\n\nif (max(owid_world$pop, na.rm = TRUE) &lt; 1e6) {\nowid_world &lt;- owid_world %&gt;% mutate(pop = pop * 1e6)\n}\n\nggplot() +\ngeom_area(data = owid_world, aes(year, pop / 1e9), alpha = 0.2) +\ngeom_line(data = wdi_world,  aes(year, pop / 1e9)) +\ngeom_vline(xintercept = 1960, linetype = \"dotted\") +\nannotate(\n\"text\",\nx = 1961,\ny = max(c(owid_world$pop, wdi_world$pop), na.rm = TRUE) / 1e9 * 0.9,\nlabel = \"Official WDI begins (1960)\",\nhjust = 0, size = 3\n) +\nlabs(\ntitle = \"World Population: 1800â€“present\",\nsubtitle = \"Historical (shaded, OWID/Maddison) with modern WDI overlay (solid line)\",\nx = NULL, y = \"Billions\"\n)\n\n\n\n\n\n\n\n\nFigureÂ 1: World population since 1800, with modern WDI overlay\n\n\n\n\n\n\n\n\nMethods Note. Historical population is sourced from OWID/Maddison where available; modern population (1960â€“present) uses World Bank WDI SP.POP.TOTL. If the build environment lacks internet, a small offline fallback ensures the chart still renders.\n\nOverlay that with Our World in Dataâ€™s historical metrics, and you see an even deeper pattern: as soon as we can measure something, we seek to improve it. Counting life expectancy didnâ€™t just describe longevity â€” it helped extend it. Tracking literacy didnâ€™t just record education â€” it democratized it.\nThis is the unseen virtue of data: it doesnâ€™t just observe reality, it amplifies intention."
  },
  {
    "objectID": "data-science/genesis-of-data.html#when-counting-became-knowing",
    "href": "data-science/genesis-of-data.html#when-counting-became-knowing",
    "title": "The Genesis of Data â€” When Counting Became Knowing",
    "section": "ğŸ”¢ When Counting Became Knowing",
    "text": "ğŸ”¢ When Counting Became Knowing\nThe modern data scientist, in some sense, is a direct descendant of the temple scribe.\nWe still look for structure in chaos, still seek to compress the infinite world into finite symbols. The clay tablets became spreadsheets; the grain records became gigabytes. But the human motive remains unchanged: to understand, to predict, to improve.\nWe often hear that data is â€œthe new oil.â€ I donâ€™t think thatâ€™s true. Oil is extracted, consumed, and gone. Data, when treated properly, is more like light â€” it illuminates. The Babyloniansâ€™ grain counts, Nightingaleâ€™s diagrams, Tukeyâ€™s exploratory plots â€” all were small sparks in a long illumination that continues through every dataset we open today."
  },
  {
    "objectID": "data-science/genesis-of-data.html#a-living-timeline",
    "href": "data-science/genesis-of-data.html#a-living-timeline",
    "title": "The Genesis of Data â€” When Counting Became Knowing",
    "section": "ğŸŒ±A Living Timeline",
    "text": "ğŸŒ±A Living Timeline\nIâ€™m building an animated time-series visualization that traces this evolution: each point represents the earliest known record of a nationâ€™s statistical data â€” census, agricultural, financial, environmental.\n\n\n\n\n\n\nNote\n\n\n\n\n\nCode\n# Example placeholder visualization\n\nset.seed(42)\ndf &lt;- tibble(\n        year = rep(1800:2020, each = 1),\n        countries_reporting = pmin(\n            200, round(5 + (year - 1800)^1.3 / 3000)\n        )\n    )\n\nggplot(df, aes(year, countries_reporting)) + \n    geom_line(linewidth = 1.2, color = \"#0072B2\") + \n    labs(title = \"A Living Timeline of Data\", subtitle = \"How measurement spread across nations (1800â€“2020)\",y = \"Countries with available data\", x = NULL)\n\n\n\n\n\n\n\n\nFigureÂ 2: Data density over time â€” number of countries recording key statistics\n\n\n\n\n\n\n\nThe story the data tells is breathtaking: we are a species that counts because we care."
  },
  {
    "objectID": "data-science/genesis-of-data.html#epilogue-the-infinite-ledger",
    "href": "data-science/genesis-of-data.html#epilogue-the-infinite-ledger",
    "title": "The Genesis of Data â€” When Counting Became Knowing",
    "section": "â™¾ï¸ Epilogue: The Infinite Ledger",
    "text": "â™¾ï¸ Epilogue: The Infinite Ledger\nWe are, each of us, a new line in the worldâ€™s oldest dataset â€” the ongoing attempt to make sense of being alive.\nThe Genesis of Data is not a past event; itâ€™s a continuous present.\nEvery number we collect is a reflection of what we value, and every dataset we publish is a quiet hope that someone, somewhere, might find meaning in it.\nData, at its most human level, is not about numbers. Itâ€™s about noticing."
  },
  {
    "objectID": "data-science/genesis-of-data.html#references",
    "href": "data-science/genesis-of-data.html#references",
    "title": "The Genesis of Data â€” When Counting Became Knowing",
    "section": "ğŸ“š References",
    "text": "ğŸ“š References\n\nWorld Bank Historical Population Data (1800â€“present)\nOur World in Data â€” History of Statistics\nUN Data Portal: National Census Records Archive"
  },
  {
    "objectID": "dataset-icon-language.html",
    "href": "dataset-icon-language.html",
    "title": "The Insightful Tales Dataset Icon Language",
    "section": "",
    "text": "The cyan circles and abstract shapes you see across Insightful Tales arenâ€™t just decoration. Theyâ€™re a visual language: a way of hinting at what each dataset is and how it behaves, before a single number appears on the page."
  },
  {
    "objectID": "dataset-icon-language.html#the-shared-visual-grammar",
    "href": "dataset-icon-language.html#the-shared-visual-grammar",
    "title": "The Insightful Tales Dataset Icon Language",
    "section": "ğŸ§© The Shared Visual Grammar",
    "text": "ğŸ§© The Shared Visual Grammar\nBefore we look at individual icons, hereâ€™s the common design grammar they all share.\n\nCyan circle as frame.\nEvery icon sits in a bright cyan ring. It marks the design as part of the Insightful Tales family, and suggests a bounded, structured view of the world â€” a dataset as a lens.\nAbstract rather than literal.\nNo airplanes, no globes, no logos. Each icon uses minimal geometry â€” arcs, lines, dots, blocks â€” to capture the structure of the data instead of its surface aesthetics.\nConsistent weight and geometry.\nLine weights and shapes are kept consistent, so very different domains (flights, weather, urban form, economics, games) still feel like they belong to the same library.\nContext-aware usage.\n\nSmall badge (.dataset-badge-icon) in lists and â€œFeatured Datasetsâ€.\n\nHero badge (.dataset-icon-hero) at the top of dataset pages.\n\nTiny source badge (.dataset-icon-small) in story footers like â€œDataset: nycflights13â€."
  },
  {
    "objectID": "dataset-icon-language.html#nycflights13-the-dataset-that-took-off",
    "href": "dataset-icon-language.html#nycflights13-the-dataset-that-took-off",
    "title": "The Insightful Tales Dataset Icon Language",
    "section": "âœˆï¸ nycflights13 â€” The Dataset That Took Off",
    "text": "âœˆï¸ nycflights13 â€” The Dataset That Took Off\n\n\n\nWhat you see\n\nA stylized, angular path inside the circle.\nA sense of origin and destination, with a central kink that feels like movement.\n\nWhat it represents\n\nThe path hints at flight legs and routing.\nThe angle suggests direction and schedules â€” departures and arrivals rather than a static point.\nThe circle is the air traffic system around NYC, seen as a whole.\n\nTogether, the icon says: this is a dataset of routes, timings, and connections â€” a structured, navigable map of flights."
  },
  {
    "objectID": "dataset-icon-language.html#gapminder-human-progress-in-motion",
    "href": "dataset-icon-language.html#gapminder-human-progress-in-motion",
    "title": "The Insightful Tales Dataset Icon Language",
    "section": "ğŸŒ Gapminder â€” Human Progress in Motion",
    "text": "ğŸŒ Gapminder â€” Human Progress in Motion\n\n\n\nWhat you see\n\nA smooth, rising curve with three circular markers, all inside the circle.\n\nWhat it represents\n\nThe curve is a stylized development trajectory: GDP per capita, life expectancy, or another long-run indicator.\nThe dots are countries or snapshots in time â€” clustered along the progress curve.\nThe circle stands for the world, viewed through comparable indicators.\n\nThis icon captures Gapminder as a dataset about global development trajectories â€” where different places sit along a shared arc of progress."
  },
  {
    "objectID": "dataset-icon-language.html#lahman-baseball-database-baseballs-digital-memory",
    "href": "dataset-icon-language.html#lahman-baseball-database-baseballs-digital-memory",
    "title": "The Insightful Tales Dataset Icon Language",
    "section": "âš¾ Lahman Baseball Database â€” Baseballâ€™s Digital Memory",
    "text": "âš¾ Lahman Baseball Database â€” Baseballâ€™s Digital Memory\n\n\n\nWhat you see\n\nA parabolic arc across the circle, with three dots near its peak.\n\nWhat it represents\n\nThe arc is a career or season trajectory â€” rise, peak, decline.\nThe dots are key seasons, players, or eras placed along that arc.\nThe circle is the historical record of the game.\n\nThe icon echoes what Lahman is best at: long-term, relational baseball history, where every row is part of a bigger narrative arc."
  },
  {
    "objectID": "dataset-icon-language.html#noaa-daily-weather-rhythm-of-the-days",
    "href": "dataset-icon-language.html#noaa-daily-weather-rhythm-of-the-days",
    "title": "The Insightful Tales Dataset Icon Language",
    "section": "ğŸŒ¦ NOAA Daily Weather â€” Rhythm of the Days",
    "text": "ğŸŒ¦ NOAA Daily Weather â€” Rhythm of the Days\n\n\n\nWhat you see\n\nA gently undulating wave across the circle.\nThree short vertical bars rising from the wave.\n\nWhat it represents\n\nThe wave is daily variation: highs and lows, the breathing rhythm of weather.\nThe bars are rain or notable events: a storm spike, a heavy day, an outlier.\nThe circle is the broader climate system that these days live within.\n\nThis icon signals daily records and weather persistence, ideal for stories about memory in systems and the mood of seasons."
  },
  {
    "objectID": "dataset-icon-language.html#tfl-open-data-the-network-beneath-the-city",
    "href": "dataset-icon-language.html#tfl-open-data-the-network-beneath-the-city",
    "title": "The Insightful Tales Dataset Icon Language",
    "section": "ğŸš‡ TfL Open Data â€” The Network Beneath the City",
    "text": "ğŸš‡ TfL Open Data â€” The Network Beneath the City\n\n\n\nWhat you see\n\nCrisscrossing lines.\nTwo solid circular nodes where lines meet.\n\nWhat it represents\n\nThe lines are routes â€” Tube lines, rail paths, bus corridors.\nThe nodes are interchanges and stations, where flows intersect.\nThe circle is the entire transit network as a single organism.\n\nTfLâ€™s data is about movement on a graph â€” this icon foregrounds routes, intersections, and connectivity."
  },
  {
    "objectID": "dataset-icon-language.html#uci-online-retail-baskets-and-transactions",
    "href": "dataset-icon-language.html#uci-online-retail-baskets-and-transactions",
    "title": "The Insightful Tales Dataset Icon Language",
    "section": "ğŸ›’ UCI Online Retail â€” Baskets and Transactions",
    "text": "ğŸ›’ UCI Online Retail â€” Baskets and Transactions\n\n\n\nWhat you see\n\nA rounded rectangle with horizontal lines (a â€œmini invoiceâ€ or basket).\nAn arrow-like element extending outward.\n\nWhat it represents\n\nThe rectangle is a basket or invoice: one order, many line items.\nThe horizontal lines stand in for products in that order.\nThe arrow suggests a completed transaction â€” an event in time.\nThe circle frames the marketplace as a system.\n\nThis icon marks datasets about customers, orders, and items, where the grain of the data is the transaction."
  },
  {
    "objectID": "dataset-icon-language.html#fred-economic-data-the-macro-curve",
    "href": "dataset-icon-language.html#fred-economic-data-the-macro-curve",
    "title": "The Insightful Tales Dataset Icon Language",
    "section": "ğŸ“ˆ FRED Economic Data â€” The Macro Curve",
    "text": "ğŸ“ˆ FRED Economic Data â€” The Macro Curve\n\n\n\nWhat you see\n\nA horizontal baseline.\nA rising-and-falling curve with three markers.\n\nWhat it represents\n\nThe baseline is time â€” months, quarters, years.\nThe curve is a simplified business cycle: expansions and recessions.\nThe dots are key readings or indicators: unemployment, inflation, GDP.\nThe circle is the macroeconomic landscape.\n\nFREDâ€™s icon is about time-series indicators in motion, not any single statistic."
  },
  {
    "objectID": "dataset-icon-language.html#nasa-modis-earth-observatory-earth-seen-from-orbit",
    "href": "dataset-icon-language.html#nasa-modis-earth-observatory-earth-seen-from-orbit",
    "title": "The Insightful Tales Dataset Icon Language",
    "section": "ğŸ›°ï¸ NASA MODIS / Earth Observatory â€” Earth Seen from Orbit",
    "text": "ğŸ›°ï¸ NASA MODIS / Earth Observatory â€” Earth Seen from Orbit\n\n\n\nWhat you see\n\nA lower arc.\nA high sweeping curve.\nA small square â€œpixelâ€ near the top curve.\n\nWhat it represents\n\nThe lower arc suggests the Earthâ€™s limb.\nThe upper curve is a satellite ground track or swath.\nThe square is a sensor footprint â€” one tile of an image.\nThe circle frames the Earth-observation domain.\n\nThis icon hints at remote sensing and swaths of imagery, where each pixel is a measurement of our planet."
  },
  {
    "objectID": "dataset-icon-language.html#global-urban-footprint-where-the-world-is-built",
    "href": "dataset-icon-language.html#global-urban-footprint-where-the-world-is-built",
    "title": "The Insightful Tales Dataset Icon Language",
    "section": "ğŸŒ† Global Urban Footprint â€” Where the World Is Built",
    "text": "ğŸŒ† Global Urban Footprint â€” Where the World Is Built\n\n\n\nWhat you see\n\nA gently faceted â€œgroundâ€ shape.\nSeveral small, blocky rectangles sitting on top of it.\n\nWhat it represents\n\nThe ground patch is land â€” a slice of the Earthâ€™s surface.\nThe blocks are built-up areas: urban clusters and structures.\nThe circle is the global coverage of the dataset.\n\nThe icon expresses GUF as a map of where humanity has concretized the landscape."
  },
  {
    "objectID": "dataset-icon-language.html#steam-games-libraries-and-play",
    "href": "dataset-icon-language.html#steam-games-libraries-and-play",
    "title": "The Insightful Tales Dataset Icon Language",
    "section": "ğŸ® Steam Games â€” Libraries and Play",
    "text": "ğŸ® Steam Games â€” Libraries and Play\n\n\n\nWhat you see\n\nTwo overlapping rounded rectangles.\nA circular shape with a small triangle inside it.\n\nWhat it represents\n\nThe overlapping rectangles are a stack of game tiles â€” a digital library.\nThe circle with a triangle is a play/interaction indicator.\nThe circle frame is the ecosystem of games, players, and time.\n\nSteam datasets are about collections, activity, and engagement â€” not just one title, but the whole library in motion."
  },
  {
    "objectID": "dataset-icon-language.html#how-to-read-and-use-these-icons",
    "href": "dataset-icon-language.html#how-to-read-and-use-these-icons",
    "title": "The Insightful Tales Dataset Icon Language",
    "section": "ğŸ¨ How to Read and Use These Icons",
    "text": "ğŸ¨ How to Read and Use These Icons\n\nWhen you see a small badge next to a dataset name, itâ€™s there to quickly signal the datasetâ€™s â€œshapeâ€ â€” flights, cities, climate, economics, etc.\nWhen you see a large badge at the top of a dataset page, itâ€™s a visual â€œcoverâ€ for that particular dataset, echoing the stories it can tell.\nWhen you see a small badge in a story footer (â€œDataset: â€¦â€), it helps you connect the narrative you just read with the underlying dataset page, so you can explore further.\n\nOver time, this icon language is meant to feel familiar: even before you read a title, a quick glance at the symbol will tell you what kind of world youâ€™re about to step into â€” flights, weather, cities, games, baseball, macroeconomics, Earth systems, and more."
  },
  {
    "objectID": "datasets/nasa-modis-global-environmental-time-series.html",
    "href": "datasets/nasa-modis-global-environmental-time-series.html",
    "title": "ğŸŒ NASA MODIS â€” Global Environmental Time Series",
    "section": "",
    "text": "What does this icon mean?"
  },
  {
    "objectID": "datasets/nasa-modis-global-environmental-time-series.html#what-is-modis",
    "href": "datasets/nasa-modis-global-environmental-time-series.html#what-is-modis",
    "title": "ğŸŒ NASA MODIS â€” Global Environmental Time Series",
    "section": "ğŸŒ What is MODIS?",
    "text": "ğŸŒ What is MODIS?\nMODIS (Moderate Resolution Imaging Spectroradiometer) is an instrument aboard NASAâ€™s Terra (1999-present) and Aqua (2002-present) satellites.\nIt scans the entire planet every 1â€“2 days in 36 spectral bands, capturing:\n\nvegetation\nsnow\nice\nfires\nclouds\ndust\nsea-surface temperatures\nland-surface temperatures\natmospheric aerosols\nwater vapor\n\nMore than 26 years of continuous observations make MODIS the most consistent environmental time series in Earthâ€™s history."
  },
  {
    "objectID": "datasets/nasa-modis-global-environmental-time-series.html#why-modis-is-special",
    "href": "datasets/nasa-modis-global-environmental-time-series.html#why-modis-is-special",
    "title": "ğŸŒ NASA MODIS â€” Global Environmental Time Series",
    "section": "ğŸ§© Why MODIS Is Special",
    "text": "ğŸ§© Why MODIS Is Special\nMODIS isnâ€™t one dataset â€” itâ€™s a family of global products that share:\n\nconsistent processing\nlong-term continuity\nglobal coverage\nstable calibration\npredictable update cycles\nmultiple instruments (Terra + Aqua) for redundancy\n\nThis creates the rare ability to observe:\n\nseasonal rhythms\nlong-term trends\nsudden environmental interruptions\necological â€œpulse changesâ€\nfire season shifts\nsnow melt timing\nland temperature behavior\n\nMODIS is essentially a planetary vital signs monitor."
  },
  {
    "objectID": "datasets/nasa-modis-global-environmental-time-series.html#key-modis-data-products-used-in-insightful-tales",
    "href": "datasets/nasa-modis-global-environmental-time-series.html#key-modis-data-products-used-in-insightful-tales",
    "title": "ğŸŒ NASA MODIS â€” Global Environmental Time Series",
    "section": "ğŸ“¦ Key MODIS Data Products Used in Insightful Tales",
    "text": "ğŸ“¦ Key MODIS Data Products Used in Insightful Tales\n\nğŸŒ± Vegetation (NDVI / EVI)\nMOD13 (Terra) â€¢ MYD13 (Aqua)\n\nNDVI (Normalized Difference Vegetation Index)\nEVI (Enhanced Vegetation Index)\n16-day composites, global\n\nUsed for:\n\nâ€œGreen pulseâ€ seasonal cycles\nLong-term greening/browning trends\n\n\n\nâ„ï¸ Snow & Ice\nMOD10 (Terra) â€¢ MYD10 (Aqua)\n\nDaily or 8-day snow cover\nSnow persistence and melt timing\n\nUsed for:\n\nWinter rhythm (â€œblink of snowâ€)\nSnow retreat mapping\n\n\n\nğŸ”¥ Active Fire / Burned Area\nMOD14 / MYD14\n\nThermal anomaly detections\nFire intensity & location\n\nUsed for:\n\nFire season â€œbarcodeâ€ visualization\nDetecting rhythm shifts in fire timing\n\n\n\nğŸŒ¡ï¸ Land Surface Temperature\nMOD11 / MYD11\n\nDay/night land temperature\nUrban heat island structure\n\nUsed for:\n\nCity heat pulse\nComparing natural vs built environments"
  },
  {
    "objectID": "datasets/nasa-modis-global-environmental-time-series.html#modis-in-insightful-tales",
    "href": "datasets/nasa-modis-global-environmental-time-series.html#modis-in-insightful-tales",
    "title": "ğŸŒ NASA MODIS â€” Global Environmental Time Series",
    "section": "ğŸ§­ MODIS in Insightful Tales",
    "text": "ğŸ§­ MODIS in Insightful Tales\nThis dataset powers multiple Insightful Tales stories, including:\nğŸª  The Planet That Breathes â€” Tracking Earthâ€™s Hidden Rhythms from Space \n\nFuture stories exploring:\n\ngreening vs browning regions\nwildfire rhythm changes\nurban thermal signatures\nwater cycles\nsnow pulse shortening"
  },
  {
    "objectID": "datasets/nasa-modis-global-environmental-time-series.html#example-interpretations-conceptual",
    "href": "datasets/nasa-modis-global-environmental-time-series.html#example-interpretations-conceptual",
    "title": "ğŸŒ NASA MODIS â€” Global Environmental Time Series",
    "section": "ğŸ“Š Example Interpretations (Conceptual)",
    "text": "ğŸ“Š Example Interpretations (Conceptual)\nEven without deep technical knowledge, MODIS can show:\n\nBreathing forests through annual NDVI waves\nSnow blink cycles â€” winter arriving and departing\nFire seasons emerging as predictable stripes\nHeat patterns distinguishing cities from nature\nCloud or dust â€œseasonsâ€ repeating each year\n\nMODIS data is not just scientific â€” itâ€™s surprisingly story-shaped."
  },
  {
    "objectID": "datasets/nasa-modis-global-environmental-time-series.html#how-insightful-tales-processes-modis",
    "href": "datasets/nasa-modis-global-environmental-time-series.html#how-insightful-tales-processes-modis",
    "title": "ğŸŒ NASA MODIS â€” Global Environmental Time Series",
    "section": "ğŸ” How Insightful Tales Processes MODIS",
    "text": "ğŸ” How Insightful Tales Processes MODIS\nFor each story, we typically:\n\nDownload selected MODIS products\nSimplify into easy-to-understand categories (snow/no snow, fire/no fire, etc.)\nSmooth using weekly or 8-day composites\nExtract rhythms (seasonal cycles, peaks/troughs, interruptions)\nVisualize with gentle colors and narrative annotations\nTranslate time-series into human language (â€œpulseâ€, â€œheartbeatâ€, â€œblinkâ€)\n\nThe goal: make planetary-scale change emotionally legible."
  },
  {
    "objectID": "datasets/nasa-modis-global-environmental-time-series.html#technical-details",
    "href": "datasets/nasa-modis-global-environmental-time-series.html#technical-details",
    "title": "ğŸŒ NASA MODIS â€” Global Environmental Time Series",
    "section": "ğŸ› ï¸ Technical Details",
    "text": "ğŸ› ï¸ Technical Details\n\nResolution: 250m, 500m, or 1km depending on the product\nTemporal resolution: daily, 8-day, 16-day\nGlobal coverage\nRadiometric calibration maintained across decades\nBand coverage: 36 spectral bands (400â€“14,400 nm)"
  },
  {
    "objectID": "datasets/nasa-modis-global-environmental-time-series.html#references-credits",
    "href": "datasets/nasa-modis-global-environmental-time-series.html#references-credits",
    "title": "ğŸŒ NASA MODIS â€” Global Environmental Time Series",
    "section": "ğŸ“š References & Credits",
    "text": "ğŸ“š References & Credits\n\nPrimary Sources\n\nNASA MODIS Land Products\nhttps://modis.gsfc.nasa.gov\nNASA Earthdata\nhttps://earthdata.nasa.gov\nNASA LAADS DAAC\nhttps://ladsweb.modaps.eosdis.nasa.gov\nNSIDC MODIS Snow Products\nhttps://nsidc.org/data/modis\n\n\n\nScientific Documentation\n\nâ€œMODIS Land Products Usersâ€™ Guideâ€\nâ€œMODIS Land Surface Temperature Algorithmâ€\nâ€œMODIS Vegetation Index User Guideâ€\nâ€œMODIS Snow Cover Products Documentationâ€\nâ€œMODIS Fire Products Overviewâ€\n\n\n\nCredits\n\nNASA (Terra & Aqua missions)\nUSGS (Landsat, supplemental imagery)\nNSIDC (snow products)\nConceptual explanation & narrative: Insightful Tales\nAll visualizations: created in R using the tidyverse & terra packages"
  },
  {
    "objectID": "datasets/the-gapminder-dataset-a-history-of-human-progress-told-through-data.html",
    "href": "datasets/the-gapminder-dataset-a-history-of-human-progress-told-through-data.html",
    "title": "The Gapminder Dataset â€” A History of Human Progress, Told Through Data",
    "section": "",
    "text": "What does this icon mean?"
  },
  {
    "objectID": "datasets/the-gapminder-dataset-a-history-of-human-progress-told-through-data.html#references-credits",
    "href": "datasets/the-gapminder-dataset-a-history-of-human-progress-told-through-data.html#references-credits",
    "title": "The Gapminder Dataset â€” A History of Human Progress, Told Through Data",
    "section": "ğŸ“š References & Credits",
    "text": "ğŸ“š References & Credits\n\nRosling, H. (2006). The best stats youâ€™ve ever seen [TED Talk]. https://www.ted.com/talks/hans_rosling_the_best_stats_you_ve_ever_seen\n\nGapminder Foundation. (2024). About Gapminder. https://www.gapminder.org/about\n\nRosling, H., Rosling, O., & Rosling RÃ¶nnlund, A. (2018). Factfulness: Ten Reasons Weâ€™re Wrong About the Worldâ€”and Why Things Are Better Than You Think. Flatiron Books.\n\nUnited Nations Data Division (UNdata). Life Expectancy and GDP per Capita Statistics.\n\nWorld Bank Open Data. World Development Indicators.\n\nBryan, J., Wickham, H., & RStudio Team. (2017). gapminder: Data from Gapminder. R package version 0.3.0.\n\nRosling, O., & RÃ¶nnlund, A. (2023). Gapminder World Tools and Dollar Street Visual Database. https://www.gapminder.org/tools"
  },
  {
    "objectID": "datasets/the-noaa-daily-weather-dataset.html",
    "href": "datasets/the-noaa-daily-weather-dataset.html",
    "title": "ğŸŒ¦ï¸ The NOAA Daily Weather Dataset",
    "section": "",
    "text": "What does this icon mean?"
  },
  {
    "objectID": "datasets/the-noaa-daily-weather-dataset.html#a-planets-daily-logbook",
    "href": "datasets/the-noaa-daily-weather-dataset.html#a-planets-daily-logbook",
    "title": "ğŸŒ¦ï¸ The NOAA Daily Weather Dataset",
    "section": "ğŸª¶ A Planetâ€™s Daily Logbook",
    "text": "ğŸª¶ A Planetâ€™s Daily Logbook\nIf the Earth kept a diary, the NOAA Daily Weather dataset would be its journal â€” quietly recording what happened in the sky each day, everywhere.\nFrom sweltering July afternoons in Texas to silent January snow in Maine, each entry captures our atmosphereâ€™s daily rhythm.\nAnd collectively, they reveal the long memory of weather that shapes our lives."
  },
  {
    "objectID": "datasets/the-noaa-daily-weather-dataset.html#origins-from-observers-to-databases",
    "href": "datasets/the-noaa-daily-weather-dataset.html#origins-from-observers-to-databases",
    "title": "ğŸŒ¦ï¸ The NOAA Daily Weather Dataset",
    "section": "ğŸ§­ Origins â€” From Observers to Databases",
    "text": "ğŸ§­ Origins â€” From Observers to Databases\nThe story begins in the late 19th century.\nLong before satellites and sensors, weather was measured by hand â€” Cooperative Observer Program (COOP) volunteers using thermometers and rain gauges.\nIn 1890, the U.S. Weather Bureau (now the National Weather Service) organized these thousands of observers to provide a daily national picture.\nBy the 1930s, these logs were centralized under what became the National Climatic Data Center (NCDC) â€” and eventually NOAAâ€™s National Centers for Environmental Information (NCEI).\nTheir mission was simple but monumental: preserve the planetâ€™s weather memory.\nToday, the result is the Global Historical Climatology Network â€“ Daily (GHCN-Daily), containing billions of daily records from over 100,000 land-based stations across 180 countries.\n\nâ€œClimate is what you expect. Weather is what you get.â€ â€” Robert Heinlein\nThis dataset captures both."
  },
  {
    "objectID": "datasets/the-noaa-daily-weather-dataset.html#what-it-contains",
    "href": "datasets/the-noaa-daily-weather-dataset.html#what-it-contains",
    "title": "ğŸŒ¦ï¸ The NOAA Daily Weather Dataset",
    "section": "ğŸŒ What It Contains",
    "text": "ğŸŒ What It Contains\n\n\n\n\n\n\n\n\nElement\nDescription\nExample Variable\n\n\n\n\nTemperature\nDaily maximum & minimum (Â°C or Â°F)\nTMAX, TMIN\n\n\nPrecipitation\nTotal daily precipitation\nPRCP\n\n\nSnowfall\nDaily snow accumulation\nSNOW\n\n\nSnow Depth\nDepth of snow on the ground\nSNWD\n\n\n\nEach station reports one row per day â€” a simple rhythm of measurements stretching back, in some places, over 150 years.\nNOAA also provides derived gridded versions, such as nClimGrid-Daily, mapping these station observations onto a fine grid across the contiguous U.S. from 1951 to present, enabling spatial analyses of heatwaves, droughts, or flood events."
  },
  {
    "objectID": "datasets/the-noaa-daily-weather-dataset.html#why-it-matters",
    "href": "datasets/the-noaa-daily-weather-dataset.html#why-it-matters",
    "title": "ğŸŒ¦ï¸ The NOAA Daily Weather Dataset",
    "section": "ğŸ’¡ Why It Matters",
    "text": "ğŸ’¡ Why It Matters\nThe NOAA Daily Weather dataset is more than just numbers â€” itâ€™s context.\nEvery datapoint tells part of a larger story about life on Earth.\n\nTime Depth: Enables century-scale climate studies â€” tracking heatwaves, cold snaps, and rainfall shifts.\nDaily Resolution: Lets us see change as it happens â€” from one sunrise to the next.\nGeographic Breadth: A single, unified record spanning the globeâ€™s landmasses.\nQuality-Controlled: Built with decades of standardization and statistical checks.\nOpen Access: Available freely to the public via NOAAâ€™s Climate Data Online (CDO).\n\nFrom farmers planning crops to engineers modeling heat stress on power grids â€” to storytellers like us connecting data to meaning â€” the dataset quietly underpins much of what we know about weather and climate."
  },
  {
    "objectID": "datasets/the-noaa-daily-weather-dataset.html#what-makes-it-unique",
    "href": "datasets/the-noaa-daily-weather-dataset.html#what-makes-it-unique",
    "title": "ğŸŒ¦ï¸ The NOAA Daily Weather Dataset",
    "section": "ğŸ§© What Makes It Unique",
    "text": "ğŸ§© What Makes It Unique\n\nItâ€™s the everyday data that becomes history.\nWhile headlines capture hurricanes, this dataset records the calm before the storm â€” the slow shifts that define climate.\nIt connects disciplines.\nWeather data intersects naturally with agriculture, transportation, energy, health, and even economics.\nFor instance, in ğŸ¦‹ Butterfly Morning Delays story, flight delays could easily be paired with daily precipitation and wind records.\nItâ€™s global, yet personal.\nBecause itâ€™s station-based, you can zoom from the planetary to the hyperlocal â€” even â€œmy town on my birthday.â€\nItâ€™s science you can touch.\nEach number once came from a real thermometer, a real place, a real moment in time."
  },
  {
    "objectID": "datasets/the-noaa-daily-weather-dataset.html#example-exploration",
    "href": "datasets/the-noaa-daily-weather-dataset.html#example-exploration",
    "title": "ğŸŒ¦ï¸ The NOAA Daily Weather Dataset",
    "section": "ğŸ§® Example Exploration",
    "text": "ğŸ§® Example Exploration\nImagine this:\nYou download 50 years of daily maximum temperatures for your city and plot how many summer days exceeded 95 Â°F each year.\nYouâ€™d likely see the creeping signature of a warming world.\nOr perhaps you cross-match rainfall with flight delays, crop yields, or local power consumption.\nSuddenly, weather becomes behavioral data â€” showing how the sky shapes our choices below.\n# Example R snippet for NOAA Daily Summaries\nlibrary(rnoaa)\nlibrary(dplyr)\n\n# Pull last 30 years of daily summaries for a station\nghcnd_stations() |&gt; \n  filter(grepl(\"LITTLE ROCK\", name)) |&gt; \n  slice_head(n = 1) |&gt; \n  pull(id) -&gt; station_id\n\ndata &lt;- meteo_pull_monitors(station_id, date_min = \"1995-01-01\", var = c(\"TMAX\", \"TMIN\", \"PRCP\"))\n\ndata |&gt; \n  mutate(year = lubridate::year(date)) |&gt;\n  group_by(year) |&gt;\n  summarise(hot_days = sum(TMAX &gt; 35, na.rm = TRUE)) |&gt;\n  ggplot(aes(year, hot_days)) +\n  geom_line() +\n  labs(title = \"Number of Hot Days (TMAX &gt; 35Â°C) per Year\",\n       y = \"Count\", x = \"Year\")\nThis small snippet turns daily data into an evolving climate portrait."
  },
  {
    "objectID": "datasets/the-noaa-daily-weather-dataset.html#references-credits",
    "href": "datasets/the-noaa-daily-weather-dataset.html#references-credits",
    "title": "ğŸŒ¦ï¸ The NOAA Daily Weather Dataset",
    "section": "ğŸ“š References & Credits",
    "text": "ğŸ“š References & Credits\n\nNOAA/NCEI â€” Global Historical Climatology Network â€“ Daily (GHCN-Daily)\nNOAA/NCEI â€” nClimGrid-Daily\nClimate.gov â€” Daily Temperature and Precipitation Reports\nNOAA Climate Data Online (CDO) â€” Access Portal\nCooperative Observer Program (COOP) â€” NOAA History\nHistorical Context â€” National Climatic Data Center (NCDC)"
  },
  {
    "objectID": "datasets/uci-online-retail-human-behavior-in-transactions.html",
    "href": "datasets/uci-online-retail-human-behavior-in-transactions.html",
    "title": "UCI Online Retail â€” Human Behavior in Transactions",
    "section": "",
    "text": "ğŸ›’ï¸ UCI Online Retail â€” Human Behavior in Transactions\n\n\nLine-item invoices for a UK e-commerce retailer, 2010â€“2011.\n\n\nA rich log of every product, basket, and return for a gift-oriented UK online retailer â€” a natural playground for basket analysis, seasonality, and long-tail behavior.\n\n\n\n\n What does this icon mean? \n\n\nğŸ§¾ What is this dataset?\nThe UCI Online Retail dataset is a log of 541,909 line-item invoice records from a UK-based, non-store online retailer that mainly sells unique â€œall-occasionâ€ giftware, with many customers being wholesalers.\nIt captures every item on every invoice between 1 December 2010 and 9 December 2011, including cancellations.\nIn other words, this is a year of human shopping behavior, expressed as a table: who bought what, when, how much, and from where.\n\n\n\nğŸ“¦ Whatâ€™s inside the table?\nAt its core, the dataset has one row per line item on an invoice. A typical schema (as implemented in several R/Python wrappers) looks like this:\n\nInvoiceNo â€“ 6-digit code for each transaction (if it starts with C, it indicates a cancellation/return).\n\nStockCode â€“ product code for each distinct item.\n\nDescription â€“ human-readable product name.\n\nQuantity â€“ number of units purchased (can be negative for returns).\n\nInvoiceDate â€“ date and time when the invoice was generated.\n\nUnitPrice â€“ price per unit (in sterling).\n\nCustomerID â€“ numeric customer identifier.\n\nCountry â€“ country where the customer resides.\n\nFrom just these columns, you can reconstruct:\n\nBaskets (all items sharing an InvoiceNo).\n\nCustomer histories (all rows sharing a CustomerID).\n\nTime series (grouping by InvoiceDate at various granularities).\n\nGeography (by Country).\n\n\n\n\nğŸ§  Why itâ€™s worthy of attention\nThis dataset sits at the crossroads of behavior, business, and complexity:\n\nBasket analysis playground\n\nClassic market-basket rules (â€œpeople who buy X also buy Yâ€) are almost textbook-designed for this data.\n\nYou get thousands of distinct products, allowing both obvious and surprising co-occurrence patterns.\n\nSeasonality and calendar stories\n\nThe time window includes major holidays, especially the end-of-year gift season, giving rich weekly and seasonal cycles in purchases, returns, and order sizes.\n\nLong-tail dynamics\n\nA few products sell constantly, but many sell rarely â€” a long-tail distribution that invites questions about variety, niche goods, and the economics of â€œjust-in-caseâ€ inventory.\n\nCustomer lifetime & churn signals\n\nBecause customers are identified (when CustomerID is present), you can study recency-frequency-monetary (RFM) patterns, churn, and loyalty â€” echoing the original RFM-based segmentation work that motivated this dataset.\n\n\nThis is not just â€œtransaction dataâ€; itâ€™s a behavioral trace of how people assemble baskets when buying gifts.\n\n\n\nğŸ” How the data was collected (and what it represents)\nAccording to the UCI documentation, the dataset comes from a real, UK-registered online retailer that sells â€œunique all-occasion gifts,â€ mainly to customers in the UK and Europe, with many wholesale buyers.\nKey characteristics:\n\nNon-store retailer â€“ everything happens online; no in-person point-of-sale data.\n\nGift-oriented â€“ products often have â€œgift set,â€ â€œdecor,â€ or â€œpartyâ€ flavors, which shapes basket composition and seasonality.\n\nWholesale flavor â€“ large quantities and repeat orders from some customers, useful for differentiating B2B vs B2C behavior.\n\nThe original analysis that accompanied this dataset used it to demonstrate data mining and customer segmentation for online retail, especially via RFM and clustering techniques.\n\n\n\nğŸ§º What kinds of questions can you ask?\nThis dataset is endlessly re-usable because it naturally supports multiple â€œlensesâ€:\n\nğŸ¤ Basket & co-occurrence questions\n\nWhich items tend to appear together on the same invoice?\n\nAre there â€œanchor productsâ€ that pull entire families of accessories into the cart?\n\nCan we build product co-occurrence networks where communities reveal natural â€œgift themesâ€?\n\n\n\nğŸ“… Time & seasonality questions\n\nHow does order volume vary by day of week and month?\n\nAre returns clustered at particular post-holiday dates?\n\nDo customers exhibit weekend vs weekday personalities (tiny impulse orders vs big planned baskets)?\n\n\n\nğŸ‘¥ Customer & behavior questions\n\nHow do first-time buyers differ from repeat buyers in basket size, product mix, and return behavior?\n\nCan RFM-style segmentation uncover distinct customer archetypes (e.g., â€œlast-minute gifters,â€ â€œbulk wholesalers,â€ â€œcurious one-off browsersâ€)?\n\nHow does churn risk change after a return or a negative experience hinted by high cancellation rates?\n\n\n\nğŸŒ Geography & market structure questions\n\nWhich countries rely more on wholesale-like orders (large quantities, repeated SKUs)?\n\nAre there regional favorites that almost never appear elsewhere?\n\n\n\n\n\nâœ¨ Story sparks\nHere are some story directions tailored to this dataset that lean into narrative and visuals:\n\nğŸ•¸ï¸ â€œThe Hidden Gift Graphâ€\n\nProduct co-occurrence graphs that show the hidden neighborhoods of the gift store.\n\n\nBuild a network where nodes are products and edges link items that frequently appear in the same invoice.\n\nUse community detection to find gift â€œecosystemsâ€â€”cozy tea sets with candles, party supplies that live near novelty mugs, etc.\n\nVisual angle: interactive graph + small multiples of communities, each with narrative labels like â€œCozy Evenings,â€ â€œOffice Banter,â€ â€œChildrenâ€™s Parties.â€\n\n\n\n\nğŸ§¬ â€œThe Long Tail of Little Joysâ€\n\nExploring long-tail dynamics: thousands of products that each sell only rarely, but together shape the character of the store.\n\n\nPlot the sales rank vs frequency to show the highly skewed distribution of product popularity.\n\nTell the story of a few rare, quirky products: what baskets do they appear in, and which â€œpower productsâ€ drag them into the cart?\n\nFrame this as a story about variety vs efficiency: what does it mean for a retailer to keep all these niche items alive?\n\n\n\n\nğŸ” â€œArcs of Regret: The Anatomy of Returnsâ€\n\nTracking return behavior arcs across time and customers.\n\n\nUse negative Quantities / cancellation invoices to define returns.\n\nStudy the time between purchase and return, and whether certain product types or basket patterns are more return-prone.\n\nBuild â€œemotion arcsâ€: e.g., a timeline per customer type (initial excitement with large baskets â†’ small return spike â†’ future purchase decisions).\n\n\n\n\nğŸ’¸ â€œInvisible Profit: Rare Combos That Matterâ€\n\nIdentifying rare combinations that quietly contribute outsized revenue or margin.\n\n\nDefine â€œrare but lucrative basketsâ€: baskets that occur infrequently but have high total value or margin.\n\nExplore which products serve as bridges between everyday items and niche, high-value pieces.\n\nThis can become a story about tiny configuration changesâ€”adding one complementary itemâ€”that transform an ordinary basket into a highly profitable one.\n\n\n\n\n\nğŸ§ª Practical quirks, caveats & ethics\nBefore diving in, it helps to know where the sharp edges are:\n\nMissing IDs â€“ some rows lack CustomerID; these can be used for anonymous, session-level stories but not customer-centric ones.\n\nCancellations and negative quantities â€“ returns appear as invoices with C prefixes and negative Quantity; youâ€™ll need to decide whether to remove, net out, or explicitly model them.\n\nNon-commercial use â€“ multiple sources note that this dataset is provided for research and educational purposes, not for commercial targeting or productization.\n\nNo demographic detail â€“ you see behavior, not personal identity. Thatâ€™s a feature, not a bug: it keeps the focus on baskets and patterns rather than individuals.\n\nFrom an ethics point of view, this dataset is a good sandbox for exploring behavioral patterns without drifting into invasive profiling. Still, itâ€™s worth emphasizing in any story that this is historical, anonymized data used for learning, not live tracking.\n\n\n\nğŸ§‘â€ğŸ’» Getting started in R & Python\nYou might load the dataset in different ways depending on your tools.\n\nğŸŸ¦ In R\nSeveral R packages mirror the UCI Online Retail data; one example is BAdatasets::online_retail:\n# install.packages(\"BAdatasets\")  # if needed\nlibrary(BAdatasets)\ndata(\"online_retail\")\n\nstr(online_retail)\n#&gt; 'data.frame': 541909 obs. of  8 variables:\n#&gt;  $ InvoiceNo : chr ...\n#&gt;  $ StockCode : chr ...\n#&gt;  ...\nYou can then build:\n\nBaskets: dplyr::group_by(InvoiceNo)\nCustomer histories: group_by(CustomerID)\nDaily totals: mutate(date = as.Date(InvoiceDate)) |&gt; group_by(date)\n\n\n\nğŸŸ§ In Python\nIf you download the original Excel file (Online Retail.xlsx) from UCI, loading is straightforward:\nimport pandas as pd\n\ndf = pd.read_excel(\"Online Retail.xlsx\")\n\ndf.head()\nFrom there:\n\nBuild a basket-level table by grouping on InvoiceNo.\nCreate RFM features per CustomerID.\nConstruct co-occurrence matrices (e.g., with pivot tables) for market basket analysis.\n\n\n\n\nğŸ“š References\n\nUCI Machine Learning Repository â€“ Online Retail Data Set. University of California, Irvine. Describes the original 2010â€“2011 UK non-store online retail transactions, including schema and usage context. UCI Machine Learning Repository+2UCI Machine Learning Repository+2\nChen, D., Sain, S. L., & Guo, K. (2012). Data mining for the online retail industry: A case study of RFM model-based customer segmentation using data mining. Journal of Database Marketing & Customer Strategy Management, 19(3), 197â€“208. Demonstrates RFM-based customer segmentation on this dataset. UCI Machine Learning Repository+1\nAllan, J. (2024). online_retail dataset in the BAdatasets R package. Provides a ready-to-use version of the UCI Online Retail data with clear column documentation. Rdrr.io\nSelected public analyses.\n\nEhsan Behzadi, Online Retail Data Analysis and Preprocessing (GitHub), focusing on cleaning, EDA, RFM, and clustering. GitHub\nVarious Kaggle notebooks and blog posts exploring market basket analysis, customer clustering, and preprocessing strategies on this dataset. Kaggle+1"
  },
  {
    "objectID": "stories/butterfly-morning-delays.html",
    "href": "stories/butterfly-morning-delays.html",
    "title": "Butterfly Morning Delays",
    "section": "",
    "text": "A story about cause and consequence in motion â€” how a stormy sunrise or a missed departure can reshape an entire day of air travel.\nCode\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(nycflights13)\nlibrary(broom)\n\n# Minimal, clean theme\nquiet_theme &lt;- function() {\n  ggplot2::theme_minimal(base_size = 13) +\n    ggplot2::theme(\n      panel.grid.minor = element_blank(),\n      plot.title.position = \"plot\",\n      plot.caption.position = \"plot\",\n      strip.text = element_text(face = \"bold\")\n    )\n}"
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#data-definitions-why-these-choices",
    "href": "stories/butterfly-morning-delays.html#data-definitions-why-these-choices",
    "title": "Butterfly Morning Delays",
    "section": "ğŸ“Š Data & Definitions â€” Why these choices?",
    "text": "ğŸ“Š Data & Definitions â€” Why these choices?\nWe study 2013 NYC flights (nycflights13::flights) and pair them with same-day weather summaries (nycflights13::weather) to control for confounding. We define:\n\nFirst Wave: departures between 05:00â€“09:00 local.\nMorning variability (Ïƒâ‚): standard deviation of departure delay in the first wave (per originÃ—date).\nAfternoon outcome (Î¼â‚): mean arrival delay for flights departing after 12:00 (per originÃ—date).\n\nWhy SD for the morning? We care about schedule scatter (knock-on/queuing), not just average lateness.\n\n\nCode\n# Preprocess core variables\nfl &lt;- flights |&gt;\n  mutate(\n    dep_dt = make_datetime(year, month, day, dep_time %/% 100, dep_time %% 100),\n    hour = hour(dep_dt),\n    date = as_date(dep_dt),\n    dist_band = cut(\n      distance,\n      breaks = c(0, 500, 1500, Inf),\n      labels = c(\"Short (â‰¤500 mi)\", \"Medium (501â€“1500)\", \"Long (â‰¥1501)\"),\n      right = TRUE\n    )\n  ) |&gt;\n  filter(!is.na(dep_dt), !is.na(arr_delay), !is.na(dep_delay)) |&gt;\n  filter(origin %in% c(\"JFK\", \"LGA\", \"EWR\"))\n\n# Daily first-wave variability (Ïƒ1) and afternoon outcome (Î¼a)\ndaily &lt;- fl |&gt;\n  group_by(origin, date) |&gt;\n  summarise(\n    sigma_morning = sd(dep_delay[hour &gt;= 5 & hour &lt; 9], na.rm = TRUE),\n    n_morning     = sum(hour &gt;= 5 & hour &lt; 9, na.rm = TRUE),\n    mean_arr_pm   = mean(arr_delay[hour &gt;= 12], na.rm = TRUE),\n    n_pm          = sum(hour &gt;= 12, na.rm = TRUE),\n    .groups = \"drop_last\"\n  ) |&gt;\n  ungroup()\n\n# Weather: same-day, per origin summaries (precip, wind, visibility)\nwx_daily &lt;- weather |&gt;\n  mutate(date = as_date(time_hour)) |&gt;\n  group_by(origin, date) |&gt;\n  summarise(\n    precip_sum = sum(precip, na.rm = TRUE),\n    wind_mean  = mean(wind_speed, na.rm = TRUE),\n    vis_mean   = mean(visib, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\ndaily &lt;- daily |&gt;\n  left_join(wx_daily, by = c(\"origin\", \"date\")) |&gt;\n  mutate(\n    month = month(date),\n    wday  = wday(date, label = TRUE)\n  )"
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#scatter-in-the-first-wave-predicts-afternoon-pain",
    "href": "stories/butterfly-morning-delays.html#scatter-in-the-first-wave-predicts-afternoon-pain",
    "title": "Butterfly Morning Delays",
    "section": "ğŸŒŠ Scatter in the first wave predicts afternoon pain",
    "text": "ğŸŒŠ Scatter in the first wave predicts afternoon pain\nWhat we plot & why: A simple scatter of morning variability (x) vs.Â afternoon mean arrival delay (y) reveals the basic relationship; a smooth fit (loess) clarifies the average trend.\n\n\nCode\np1 &lt;- daily |&gt;\n  filter(n_morning &gt;= 10, n_pm &gt;= 20) |&gt;\n  ggplot(aes(sigma_morning, mean_arr_pm, color = origin)) +\n  geom_point(alpha = 0.35) +\n  geom_smooth(se = FALSE) +\n  labs(\n    title = \"Morning Schedule Scatter vs. Afternoon Arrival Delays\",\n    subtitle = \"Per originÃ—day in NYC, 2013 (loess trend).\",\n    x = \"First-wave SD of departure delay (minutes)\",\n    y = \"Afternoon mean arrival delay (minutes)\",\n    color = \"Origin\",\n    caption = \"Data: nycflights13 â€¢ Smooth: loess â€¢ SD uses 05:00â€“08:59 departures\"\n  ) +\n  quiet_theme()\np1\n\n\n\n\n\n\n\n\n\nWhat it implies: As first-wave scatter increases, afternoon arrival delays climb. This is consistent with queuing/turnaround knock-onâ€”a few badly delayed early flights can desynchronize crew/aircraft rotations.\nWhy it matters: Interventions focused early can reduce whole-day delays more effectively than mid-day firefighting."
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#short-haul-routes-amplify-the-ripple",
    "href": "stories/butterfly-morning-delays.html#short-haul-routes-amplify-the-ripple",
    "title": "Butterfly Morning Delays",
    "section": "âœˆï¸ Short-haul routes amplify the ripple",
    "text": "âœˆï¸ Short-haul routes amplify the ripple\nWhat we plot & why: We segment by distance band; short-haul flights cycle aircraft/crews more frequently, increasing propagation channels.\n\n\nCode\npm_by_band &lt;- fl |&gt;\n  mutate(am_wave = hour &gt;= 5 & hour &lt; 9,\n         pm_wave = hour &gt;= 12) |&gt;\n  group_by(origin, date, dist_band) |&gt;\n  summarise(\n    sigma_morning = sd(dep_delay[am_wave], na.rm = TRUE),\n    mean_arr_pm   = mean(arr_delay[pm_wave], na.rm = TRUE),\n    n_am = sum(am_wave, na.rm = TRUE),\n    n_pm = sum(pm_wave, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  filter(!is.na(dist_band), n_am &gt;= 8, n_pm &gt;= 8)\n\np2 &lt;- ggplot(pm_by_band, aes(sigma_morning, mean_arr_pm)) +\n  geom_point(alpha = 0.25) +\n  geom_smooth(se = FALSE) +\n  facet_wrap(~ dist_band) +\n  labs(\n    title = \"Propagation is Strongest on Short-Haul Days\",\n    subtitle = \"Per originÃ—date; first-wave SD vs. afternoon mean arrival delay.\",\n    x = \"First-wave SD (min)\",\n    y = \"Afternoon mean arrival delay (min)\"\n  ) +\n  quiet_theme()\np2\n\n\n\n\n\n\n\n\n\nImplication: Short routes act like â€œdelay multipliers.â€ Practical levers here include gate assignment discipline, turn-time buffers, and pushback sequencing in the first wave."
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#after-weather-controls-the-association-persists",
    "href": "stories/butterfly-morning-delays.html#after-weather-controls-the-association-persists",
    "title": "Butterfly Morning Delays",
    "section": "ğŸŒ¦ï¸ After weather controls, the association persists",
    "text": "ğŸŒ¦ï¸ After weather controls, the association persists\nWhat we fit & why: A linear model predicting afternoon mean arrival delay from first-wave SD, controlling for precipitation, wind, visibility, month and weekday (to partially absorb seasonality/peaks). This is associational, not causal.\n\n\nCode\nmod_data &lt;- daily |&gt;\n  filter(!is.na(mean_arr_pm), !is.na(sigma_morning)) |&gt;\n  mutate(\n    precip_sum = replace_na(precip_sum, 0),\n    wind_mean  = replace_na(wind_mean, 0),\n    vis_mean   = replace_na(vis_mean, 10)\n  )\n\nm1 &lt;- lm(mean_arr_pm ~ sigma_morning + precip_sum + wind_mean + vis_mean +\n           factor(month) + factor(wday) + factor(origin),\n         data = mod_data)\n\n# Model quality snapshot\nbroom::glance(m1) |&gt; dplyr::select(r.squared, adj.r.squared, sigma, nobs)\n\n\n\n  \n\n\n\nCode\n# Slope of interest\nbroom::tidy(m1) |&gt;\n  filter(term == \"sigma_morning\") |&gt;\n  mutate(interpret = paste0(\"~\", round(estimate, 2),\n                            \" min more afternoon delay per +1 min SD in first wave\"))\n\n\n\n  \n\n\n\nWhat it tells us: The coefficient on sigma_morning (typically positive) indicates that even after accounting for weather and calendar effects, noisier first waves align with worse afternoons.\n\n\n\n\n\n\nNote\n\n\n\nInterpretation: If the coefficient is, say, 0.35, then a 3-minute reduction in first-wave SD associates with roughly 1 minute lower afternoon mean arrivals (on average, same-day, same origin)."
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#a-tiny-what-if-simulation",
    "href": "stories/butterfly-morning-delays.html#a-tiny-what-if-simulation",
    "title": "Butterfly Morning Delays",
    "section": "ğŸ’­ A tiny â€œwhat-ifâ€ simulation",
    "text": "ğŸ’­ A tiny â€œwhat-ifâ€ simulation\nWhat we simulate & why: Suppose operations could shave 2 minutes of SD from the first wave (tightened sequencing, better gate prep). Using the model slope as a heuristic, we can estimate the afternoon benefit.\n\n\nCode\ncoef_sigma &lt;- broom::tidy(m1) |&gt;\n  filter(term == \"sigma_morning\") |&gt;\n  pull(estimate)\n\ndelta_sd &lt;- -2  # hypothetical tightening (minutes)\navg_effect &lt;- coef_sigma * delta_sd\n\ntibble(\n  hypothetical_sd_change_min = delta_sd,\n  predicted_change_pm_mean_min = avg_effect\n)\n\n\n\n  \n\n\n\nSo what: Even modest improvements early could meaningfully reduce afternoon averagesâ€”multiplied across hundreds of flights, that is real passenger time and crew/aircraft efficiency."
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#practical-applications",
    "href": "stories/butterfly-morning-delays.html#practical-applications",
    "title": "Butterfly Morning Delays",
    "section": "ğŸ§° Practical Applications",
    "text": "ğŸ§° Practical Applications\n\nCrew/Aircraft Rotations: Prioritize first-wave turn readiness (catering, fueling, pushback order) to minimize scatter.\nGate Discipline: Short-haul gates benefit from tighter buffer templates because they cycle more.\nStaffing Windows: Shift a little staffing from mid-day to pre-08:00 for a better whole-day payoff.\nPlaybooks for â€œNoisy Morningsâ€: When Ïƒâ‚ spikes, proactively re-sequence sensitive turns to break chains."
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#limitations-what-we-didnt-model",
    "href": "stories/butterfly-morning-delays.html#limitations-what-we-didnt-model",
    "title": "Butterfly Morning Delays",
    "section": "ğŸš§ Limitations (what we didnâ€™t model)",
    "text": "ğŸš§ Limitations (what we didnâ€™t model)\n\nThis is NYC-2013 only; airports differ in geometry/ATC regimes.\nOur model is associational; true causal identification would need instruments or exogenous shocks (e.g., runway closures).\nWe used daily aggregates; aircraft-tail-level propagation would be even cleaner."
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#summary-what-we-discovered",
    "href": "stories/butterfly-morning-delays.html#summary-what-we-discovered",
    "title": "Butterfly Morning Delays",
    "section": "ğŸ“‹ Summary (what we discovered)",
    "text": "ğŸ“‹ Summary (what we discovered)\n\nFirst-wave scatter (not just average delay) is a strong signal for afternoon performance.\nThe effect is strongest on short-haul daysâ€”where rotations are more frequent.\nControlling for weather/time effects, the relationship persists.\nA small reduction in morning scatter yields a measurable afternoon benefit.\n\n\n Dataset: nycflights13\n\n\n What do these icons mean?"
  },
  {
    "objectID": "stories/butterfly-morning-delays.html#references-credits",
    "href": "stories/butterfly-morning-delays.html#references-credits",
    "title": "Butterfly Morning Delays",
    "section": "ğŸ“š References & Credits",
    "text": "ğŸ“š References & Credits\n\nData: nycflights13 (flights, weather) â€” Wickham et al.Â (CRAN)\nPackages: tidyverse, lubridate, broom, ggplot2\nAuthoring: Quarto\nReading: Wickham, R for Data Science (data wrangling & viz patterns); Quarto documentation\nRepro style: quiet_theme() in this doc (CC-0)\nLinks:\n\nhttps://cran.r-project.org/package=nycflights13\nhttps://r4ds.hadley.nz/\nhttps://quarto.org/ ```"
  },
  {
    "objectID": "stories/index.html",
    "href": "stories/index.html",
    "title": "All Stories",
    "section": "",
    "text": "ğŸ¦‹ All Stories\nExplore every data narrative published on Insightful Tales â€”\nfrom flight delays to weather memory and everything between.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "stories/the-hype-curve-life-and-death-of-a-game.html",
    "href": "stories/the-hype-curve-life-and-death-of-a-game.html",
    "title": "ğŸ•¹ï¸ The Hype Curve â€” Life and Death of a Game",
    "section": "",
    "text": "Every game on Steam has a heartbeat. Some explode and vanish in weeks. Others smolder for years before catching fire. A few never die. In this story, we use Steam-style player-count data to reveal the secret shapes of digital life â€” hype spikes, slow burns, collapses, and miraculous revivals that behave more like ecosystems than products.\nThis story is built around a simple idea:\nWeâ€™ll use simulated data with a â€œSteam-likeâ€ structure to show how:\nLater, you can plug in your own Steam dataset and watch these shapes emerge from the real world."
  },
  {
    "objectID": "stories/the-hype-curve-life-and-death-of-a-game.html#the-pulse-of-a-digital-planet",
    "href": "stories/the-hype-curve-life-and-death-of-a-game.html#the-pulse-of-a-digital-planet",
    "title": "ğŸ•¹ï¸ The Hype Curve â€” Life and Death of a Game",
    "section": "ğŸŒ 1. The Pulse of a Digital Planet",
    "text": "ğŸŒ 1. The Pulse of a Digital Planet\nSteam is not just a store. It is a seismograph of global attention.\nEvery hour, thousands of tiny signals ripple through its servers:\n\nplayers logging in and out\n\npeaks during weekends and holidays\n\nreview bursts after major patches\n\nâ€œflash floodsâ€ of players from big streamers\n\nlong, slow declines as communities age\n\nAt the level of a single game, these look like noisy time series.\nAt the level of thousands of games, they look like a living ecosystem.\nBelow, weâ€™ll build a toy version of a Steam-like dataset to explore the basic shapes of game life.\n\n\nShow the code behind these visualizations\nlibrary(tidyverse)\n\nset.seed(42)\n\nn_points &lt;- 120  # \"days\" since launch for each synthetic curve\ntime &lt;- seq(0, 119)\n\n# helper functions to generate archetype shapes\nsupernova_curve &lt;- function(t) {\n  peak_day &lt;- 5\n  peak &lt;- dnorm(t, mean = peak_day, sd = 3)\n  tail &lt;- dnorm(t, mean = peak_day + 7, sd = 15)\n  (peak + 0.4 * tail) / max(peak + 0.4 * tail)\n}\n\nslowburn_curve &lt;- function(t) {\n  growth &lt;- plogis((t - 40) / 6)\n  decay &lt;- exp(-(t - 40) / 80)\n  curve &lt;- growth * decay\n  curve &lt;- ifelse(t &lt; 5, 0, curve)\n  curve / max(curve)\n}\n\nundying_curve &lt;- function(t) {\n  base &lt;- 0.5 + 0.1 * sin(2 * pi * t / 7) + 0.15 * sin(2 * pi * t / 30)\n  trend &lt;- 0.3 + 0.3 * exp(-(t - 30) / 300)\n  curve &lt;- pmax(base * trend, 0)\n  curve / max(curve)\n}\n\ndormant_revival_curve &lt;- function(t) {\n  base &lt;- rep(0.02, length(t))\n  revival &lt;- dnorm(t, mean = 80, sd = 5)\n  curve &lt;- base + 2 * revival\n  curve / max(curve)\n}\n\narchetypes_df &lt;-\n  tibble(\n    day = rep(time, times = 4),\n    archetype = factor(\n      rep(c(\"Supernova\", \"Slow Burn â†’ Wildfire\", \"Undying Live Service\", \"Dormant Revival\"),\n          each = length(time)),\n      levels = c(\"Supernova\", \"Slow Burn â†’ Wildfire\",\n                 \"Undying Live Service\", \"Dormant Revival\")\n    )\n  ) %&gt;%\n  mutate(\n    relative_players = case_when(\n      archetype == \"Supernova\" ~ supernova_curve(day),\n      archetype == \"Slow Burn â†’ Wildfire\" ~ slowburn_curve(day),\n      archetype == \"Undying Live Service\" ~ undying_curve(day),\n      archetype == \"Dormant Revival\" ~ dormant_revival_curve(day),\n      TRUE ~ NA_real_\n    ),\n    # scale to \"concurrent players\"\n    players = round(relative_players * case_when(\n      archetype == \"Supernova\" ~ 120000,\n      archetype == \"Slow Burn â†’ Wildfire\" ~ 40000,\n      archetype == \"Undying Live Service\" ~ 80000,\n      archetype == \"Dormant Revival\" ~ 20000\n    ))\n  )\n\narchetypes_df %&gt;% head()\n\n\n\n  \n\n\n\nThis isnâ€™t real Steam data â€” but it behaves like it:\n\none game explodes and crashes,\none quietly grows then erupts,\none oscillates indefinitely,\none lies â€œdeadâ€ until a surprise revival."
  },
  {
    "objectID": "stories/the-hype-curve-life-and-death-of-a-game.html#the-hype-curve-archetypes",
    "href": "stories/the-hype-curve-life-and-death-of-a-game.html#the-hype-curve-archetypes",
    "title": "ğŸ•¹ï¸ The Hype Curve â€” Life and Death of a Game",
    "section": "ğŸ§¬ 2. The Hype Curve Archetypes",
    "text": "ğŸ§¬ 2. The Hype Curve Archetypes\nLetâ€™s visualize these four archetypes side by side.\n\n\nShow the code behind these visualizations\narchetypes_df %&gt;%\nggplot(aes(x = day, y = players)) +\ngeom_line(linewidth = 1) +\nfacet_wrap(~ archetype, ncol = 2, scales = \"free_y\") +\nlabs(\ntitle = \"The Hype Curve Archetypes\",\nsubtitle = \"Synthetic Steam-style player counts over time for four types of game lifecycles\",\nx = \"Days since launch\",\ny = \"Concurrent players (simulated)\"\n) +\ntheme_minimal(base_size = 13) +\ntheme(\nstrip.text = element_text(face = \"bold\"),\nplot.title = element_text(face = \"bold\")\n)\n\n\n\n\n\n\n\n\n\n\nğŸ”­ How to read this chart\nEach panel is a story shape:\n\nSupernova\n\nHuge launch.\nMassive spike.\nCollapse almost as fast.\nThis is hype-driven, marketing-heavy, and often paired with rough launches.\n\nSlow Burn â†’ Wildfire\n\nModest launch.\nGradual word-of-mouth growth.\nA delayed explosion once communities, streamers, and friends kick in.\nThis is trust made visible.\n\nUndying Live Service\n\nNever truly dies.\nWeekly cycles (weekend spikes), monthly arcs, seasonal events.\nThe game behaves like a living ecosystem.\n\nDormant Revival\n\nFlatline for a long time.\nA sudden resurgence years later.\nOften triggered by a YouTube documentary, a big streamer, a meme, or an anniversary sale.\n\n\nThese shapes appear again and again in real Steam game data.\nOnce you see them, itâ€™s hard to unsee them."
  },
  {
    "objectID": "stories/the-hype-curve-life-and-death-of-a-game.html#slow-death-vs.-sudden-death",
    "href": "stories/the-hype-curve-life-and-death-of-a-game.html#slow-death-vs.-sudden-death",
    "title": "ğŸ•¹ï¸ The Hype Curve â€” Life and Death of a Game",
    "section": "âš°ï¸ 3. Slow Death vs.Â Sudden Death",
    "text": "âš°ï¸ 3. Slow Death vs.Â Sudden Death\nNot every game decays the same way.\nSome fall off a cliff. Others fade like a sunset.\nLetâ€™s zoom into the Supernova archetype and fit a simple â€œdecayâ€ model.\n\n\nShow the code behind these visualizations\nsupernova_df &lt;- archetypes_df %&gt;%\nfilter(archetype == \"Supernova\") %&gt;%\nfilter(day &gt;= 5) %&gt;%  # post-peak\nmutate(day_since_peak = day - min(day))\n\nsupernova_df %&gt;% head()\n\n\n\n  \n\n\n\nFor illustration, we can treat the decay as roughly exponential: \\[\n\\text{players}(t) \\approx N_0 e^{-\\lambda t}\n\\]\n\n\nShow the code behind these visualizations\ndecay_model &lt;- nls(\nplayers ~ N0 * exp(-lambda * day_since_peak),\ndata = supernova_df %&gt;% filter(players &gt; 0),\nstart = list(N0 = max(supernova_df$players), lambda = 0.05)\n)\n\ncoef(decay_model)\n\n\n          N0       lambda \n1.337889e+05 2.330013e-01 \n\n\nNow we overlay the fitted decay curve on the observed values.\n\n\nShow the code behind these visualizations\nsupernova_df_pred &lt;- supernova_df %&gt;%\nmutate(\nfitted = predict(decay_model, newdata = supernova_df)\n)\n\nsupernova_df_pred %&gt;%\nggplot(aes(x = day_since_peak)) +\ngeom_point(aes(y = players), alpha = 0.6) +\ngeom_line(aes(y = fitted), linewidth = 1) +\nlabs(\ntitle = \"Sudden Death: Exponential Decay After a Hype Supernova\",\nsubtitle = \"A cliff-shaped collapse after a massive launch peak\",\nx = \"Days since peak\",\ny = \"Concurrent players (simulated)\"\n) +\ntheme_minimal(base_size = 13) +\ntheme(plot.title = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\nWhat this tells us\n\nA tight fit suggests a predictable collapse.\nMarketing drove the spike; disappointment drove the fall.\nStudios can use this shape to ask:\n\nDid we earn this launch?\nDid players stick, or did we simply rent their attention for a weekend?\n\n\nIn contrast, a slow death game would show:\n\na smaller, gentler slope,\ndecay over many half-lives,\nperiodic bumps as updates land.\n\nYou donâ€™t just see revenue here.\nYou see trust, patience, and forgiveness â€” or the lack of them."
  },
  {
    "objectID": "stories/the-hype-curve-life-and-death-of-a-game.html#nostalgia-events-when-the-dead-rise",
    "href": "stories/the-hype-curve-life-and-death-of-a-game.html#nostalgia-events-when-the-dead-rise",
    "title": "ğŸ•¹ï¸ The Hype Curve â€” Life and Death of a Game",
    "section": "ğŸ”¥ 4. Nostalgia Events â€” When the Dead Rise",
    "text": "ğŸ”¥ 4. Nostalgia Events â€” When the Dead Rise\nNow for the most magical shape in Steam data: the resurrection.\nLetâ€™s isolate the Dormant Revival archetype and highlight the â€œeventâ€ that wakes it up.\n\n\nShow the code behind these visualizations\nrevival_df &lt;- archetypes_df %&gt;%\nfilter(archetype == \"Dormant Revival\") %&gt;%\nmutate(\nevent_flag = if_else(day &gt;= 75 & day &lt;= 85, \"Revival window\", \"Baseline\")\n)\n\nrevival_df %&gt;%\nggplot(aes(x = day, y = players)) +\ngeom_line(linewidth = 1) +\ngeom_rect(\ninherit.aes = FALSE,\nxmin = 75, xmax = 85,\nymin = -Inf, ymax = Inf,\nalpha = 0.1\n) +\nlabs(\ntitle = \"Nostalgia Event: A Dormant Game Suddenly Awakens\",\nsubtitle = \"Years of near-zero activity followed by a dramatic spike\",\nx = \"Days since launch (proxy for years)\",\ny = \"Concurrent players (simulated)\"\n) +\ntheme_minimal(base_size = 13) +\ntheme(plot.title = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\nIn real data, this rectangle might correspond to:\n\na major streamer discovering the game,\na fan-made documentary on YouTube,\na big sale or anniversary,\na TikTok meme that suddenly makes the game â€œfreshâ€ again.\n\n\nWhy this matters\n\nNostalgia spikes prove that â€œdeadâ€ games are often just sleeping.\n\nFor developers and publishers, this shape is a business opportunity:\n\nre-release or remaster timing,\ncross-promotions with new titles,\ncurated â€œback from the deadâ€ sales,\nre-engaging old communities with updates or events.\n\nFor cultural historians, itâ€™s a memory graph:\n\nhow long it takes for a childhood game to resurface,\nhow strong the emotional pull remains,\nwhich titles become comfort food for entire generations."
  },
  {
    "objectID": "stories/the-hype-curve-life-and-death-of-a-game.html#the-cultural-half-life-of-fun",
    "href": "stories/the-hype-curve-life-and-death-of-a-game.html#the-cultural-half-life-of-fun",
    "title": "ğŸ•¹ï¸ The Hype Curve â€” Life and Death of a Game",
    "section": "â³ 5. The Cultural Half-Life of Fun",
    "text": "â³ 5. The Cultural Half-Life of Fun\nOne of the most powerful ways to summarize a gameâ€™s lifespan is its half-life of attention:\n\nThe time it takes for concurrent players to fall to half of their peak.\n\nWeâ€™ll compute a rough half-life for each archetype.\n\n\nShow the code behind these visualizations\n# Compute peak info per archetype\n\nhalf_life_df &lt;- archetypes_df %&gt;%\ndplyr::group_by(archetype) %&gt;%\ndplyr::summarise(\npeak_players = max(players),\npeak_day = day[which.max(players)],\n.groups = \"drop\"\n) %&gt;%\ndplyr::mutate(\nhalf_threshold = peak_players / 2\n)\n\n# For each archetype, find the first day after the peak\n\n# where players fall to or below half of the peak\n\nhalf_life_df &lt;- half_life_df %&gt;%\ndplyr::mutate(\nhalf_life_day = purrr::pmap_dbl(\nlist(archetype, peak_day, half_threshold),\nfunction(arch, pday, hthresh) {\ndf &lt;- archetypes_df %&gt;%\ndplyr::filter(archetype == arch, day &gt;= pday) %&gt;%\ndplyr::arrange(day)\n\n    idx &lt;- which(df$players &lt;= hthresh)[1]\n\n    if (is.na(idx)) {\n      NA_real_\n    } else {\n      df$day[idx]\n    }\n  }\n),\nhalf_life_length = half_life_day - peak_day\n\n)\n\nhalf_life_df\n\n\n\n  \n\n\n\nLetâ€™s visualize these half-lives.\n\n\nShow the code behind these visualizations\nhalf_life_df %&gt;%\nggplot(aes(x = archetype, y = half_life_length)) +\ngeom_col() +\ncoord_flip() +\nlabs(\ntitle = \"The Cultural Half-Life of Fun (Simulated)\",\nsubtitle = \"How long each archetype takes to lose half of its peak concurrent players\",\nx = NULL,\ny = \"Days to fall to half of peak players\"\n) +\ntheme_minimal(base_size = 13) +\ntheme(plot.title = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\nHow to interpret this\n\nSupernova: very short half-life.\n\nAttention is explosive but fragile.\nPlayers leave once the hype fog clears.\n\nSlow Burn â†’ Wildfire: longer half-life.\n\nCommunities stick because the game earned its growth.\nWord-of-mouth builds resilience.\n\nUndying Live Service: half-life may be extremely long or not well-defined.\n\nThe game oscillates around a stable core community.\n\nDormant Revival: might show multiple half-lives:\n\none after the original launch,\nanother after the revival.\n\n\nThe beauty of this metric is that it compresses an entire lifecycle curve into a single, intuitive number â€” a kind of â€œdecay constantâ€ for cultural attention."
  },
  {
    "objectID": "stories/the-hype-curve-life-and-death-of-a-game.html#revelations-hidden-in-the-curves",
    "href": "stories/the-hype-curve-life-and-death-of-a-game.html#revelations-hidden-in-the-curves",
    "title": "ğŸ•¹ï¸ The Hype Curve â€” Life and Death of a Game",
    "section": "ğŸ’¡ 6. Revelations Hidden in the Curves",
    "text": "ğŸ’¡ 6. Revelations Hidden in the Curves\nWhen you stare at thousands of these curves (for real games), certain patterns begin to feel like laws of nature for digital worlds.\n\nğŸ§· Revelation 1 â€” Communities outlive content\nGames with:\n\nmod support,\ndedicated servers,\nstrong social structures (guilds, clans, roleplay communities),\n\noften keep pulsing long after official content stops.\nThe curve decouples from the studioâ€™s roadmap â€” and attaches itself to the players.\n\n\nğŸ“‰ Revelation 2 â€” Hype is predictable, longevity is not\nYou can buy a big spike with marketing.\nYou cannot buy a long, graceful tail.\nLongevity seems to depend on:\n\nmechanical depth,\nreplayability,\nsocial stickiness,\ncreator ecosystems (mods, maps, skins, custom modes).\n\n\n\nğŸ•’ Revelation 3 â€” The half-life of hype is shrinking\nAs the digital world becomes more crowded:\n\nspikes get sharper,\ncliffs get steeper,\npatience gets shorter.\n\nThe Supernova shape becomes more common.\nWe rush in faster â€” and leave earlier.\n\n\nğŸª¦ Revelation 4 â€” Nothing really dies\nBack-catalog games:\n\nreappear with movie tie-ins,\nresurface through influencer nostalgia,\nsuddenly spike when a new generation discovers them.\n\nSteamâ€™s long memory means the past is always available, just waiting for a spark."
  },
  {
    "objectID": "stories/the-hype-curve-life-and-death-of-a-game.html#beyond-games-a-template-for-modern-attention",
    "href": "stories/the-hype-curve-life-and-death-of-a-game.html#beyond-games-a-template-for-modern-attention",
    "title": "ğŸ•¹ï¸ The Hype Curve â€” Life and Death of a Game",
    "section": "ğŸŒ 7. Beyond Games â€” A Template for Modern Attention",
    "text": "ğŸŒ 7. Beyond Games â€” A Template for Modern Attention\nThe shapes weâ€™ve explored here donâ€™t just describe games.\nThey reappear across digital culture:\n\nMusic on Spotify: viral hits with short half-lives vs.Â catalog songs with long, flat tails.\nTV series on streaming platforms: intense binge peaks followed by cliffs or â€œcomfort-watchâ€ plateaus.\nMemes on social media: Supernovas by default.\nNews cycles: outrage spikes, rapid decay.\n\nWhat Steam gives us is a clean, high-resolution laboratory for studying attention:\n\nthe physics of hype,\nthe ecology of communities,\nthe thermodynamics of nostalgia.\n\nOnce you know how to read these curves, you can look at almost any digital system and ask:\n\nIs this a Supernova or a Slow Burn?\nHas it really died, or is it just dormant?\nWhat is the cultural half-life of this phenomenon?"
  },
  {
    "objectID": "stories/weather-memory.html",
    "href": "stories/weather-memory.html",
    "title": "Weather Memory",
    "section": "",
    "text": "Bad weather can disrupt schedules, but does the pain linger after skies clear? This short data story explores whether todayâ€™s flight delays still carry the fingerprint of yesterdayâ€™s weather â€” a simple form of system memory."
  },
  {
    "objectID": "stories/weather-memory.html#goal-approach",
    "href": "stories/weather-memory.html#goal-approach",
    "title": "Weather Memory",
    "section": "ğŸ¯ Goal & approach",
    "text": "ğŸ¯ Goal & approach\nGoal. Test whether yesterdayâ€™s weather helps predict todayâ€™s flight delays at NYC airports, beyond the effect of todayâ€™s weather.\nMethod (high level). 1) Join flights with hourly weather, aggregate to a daily panel by airport; 2) create lagged weather features (yesterdayâ€™s precip, wind, visibility, temperature); 3) model todayâ€™s mean departure delay using both today and yesterday weather plus calendar controls; 4) visualize the patterns and airport differences; 5) run a small what-if experiment.\n\nâš™ï¸ Setup\n\n\nCode\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(nycflights13)\nlibrary(broom)\nlibrary(gt)\nlibrary(scales)\nlibrary(modelr)\nlibrary(plotly)  # interactivity\n\n# Use site-wide theme if present; otherwise a quiet fallback\nquiet_theme &lt;- if (exists(\"quiet_theme\")) quiet_theme else function() {\n  theme_minimal(base_size = 12) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title.position = \"plot\",\n      plot.caption.position = \"plot\",\n      plot.title = element_text(face = \"bold\"),\n      legend.position = \"bottom\"\n    )\n}"
  },
  {
    "objectID": "stories/weather-memory.html#build-a-daily-panel-flights-weather",
    "href": "stories/weather-memory.html#build-a-daily-panel-flights-weather",
    "title": "Weather Memory",
    "section": "ğŸ§± Build a daily panel: flights Ã— weather",
    "text": "ğŸ§± Build a daily panel: flights Ã— weather\nGoal â†’ Create a tidy daily dataset by airport (JFK/LGA/EWR) with delay outcomes and weather inputs.\n\n\nCode\n# Flights (2013) â†’ daily outcomes by origin\nfl_daily &lt;- flights %&gt;%\n  mutate(\n    date = make_date(year, month, day),\n    cancelled = is.na(dep_time)\n  ) %&gt;%\n  group_by(origin, date) %&gt;%\n  summarise(\n    n_flights      = n(),\n    cancel_rate    = mean(cancelled),\n    mean_dep_delay = mean(dep_delay[!cancelled], na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n# Weather (hourly) â†’ daily inputs by origin\nwx_daily &lt;- weather %&gt;%\n  mutate(date = as_date(time_hour)) %&gt;%\n  group_by(origin, date) %&gt;%\n  summarise(\n    precip_in = sum(replace_na(precip, 0)),      # inches\n    wind_mph  = mean(wind_speed, na.rm = TRUE),  # mph\n    visib_mi  = mean(visib, na.rm = TRUE),       # miles\n    temp_F    = mean(temp, na.rm = TRUE),        # Â°F\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    precip_mm = precip_in * 25.4,                # convert to mm\n    rain_day  = as.integer(precip_in &gt; 0)\n  )\n\n# Join and create 1-day lags per-airport\npanel_daily &lt;- fl_daily %&gt;%\n  inner_join(wx_daily, by = c(\"origin\",\"date\")) %&gt;%\n  arrange(origin, date) %&gt;%\n  group_by(origin) %&gt;%\n  mutate(\n    precip_mm_lag1 = lag(precip_mm, 1),\n    wind_mph_lag1  = lag(wind_mph, 1),\n    visib_mi_lag1  = lag(visib_mi, 1),\n    temp_F_lag1    = lag(temp_F, 1),\n    rain_yday      = lag(rain_day, 1),\n    dow   = wday(date, label = TRUE, abbr = TRUE),\n    month = month(date, label = TRUE, abbr = TRUE)\n  ) %&gt;%\n  ungroup()\n\n# Keep rows with both today and lagged weather present\npanel_model &lt;- panel_daily %&gt;%\n  drop_na(\n    mean_dep_delay,\n    precip_mm, wind_mph, visib_mi, temp_F,\n    precip_mm_lag1, wind_mph_lag1, visib_mi_lag1, temp_F_lag1\n  )"
  },
  {
    "objectID": "stories/weather-memory.html#first-look-does-delay-rise-with-yesterdays-rain",
    "href": "stories/weather-memory.html#first-look-does-delay-rise-with-yesterdays-rain",
    "title": "Weather Memory",
    "section": "ğŸ”ï¸ First look: does delay rise with yesterdayâ€™s rain?",
    "text": "ğŸ”ï¸ First look: does delay rise with yesterdayâ€™s rain?\n\n\nCode\np_scatter &lt;- panel_model %&gt;%\n  ggplot(aes(x = precip_mm_lag1, y = mean_dep_delay)) +\n  geom_point(alpha = 0.4, size = 1) +\n  geom_smooth(method = \"loess\", se = TRUE) +\n  facet_wrap(~ origin, nrow = 1) +\n  labs(\n    title = \"Weather memory: yesterday's rain vs. today's mean departure delay\",\n    x = \"Yesterday's precipitation (mm)\",\n    y = \"Today's mean departure delay (minutes)\",\n    caption = \"Dots are daily airport means in 2013; line = loess trend.\"\n  ) +\n  quiet_theme()\n\np_scatter\n\n\n\n\n\n\n\n\n\n\n\nCode\nsubplot &lt;- ggplotly(p_scatter, tooltip = c(\"x\", \"y\")) %&gt;%\n  layout(title = list(text = \"Weather memory: Yesterday's rain vs. today's delays\"))\n\nsubplot"
  },
  {
    "objectID": "stories/weather-memory.html#today-vs.-yesterday-contrasting-weather-states",
    "href": "stories/weather-memory.html#today-vs.-yesterday-contrasting-weather-states",
    "title": "Weather Memory",
    "section": "ğŸŒ¤ï¸ï¸ Today vs.Â yesterday: contrasting weather states",
    "text": "ğŸŒ¤ï¸ï¸ Today vs.Â yesterday: contrasting weather states\n\n\nCode\np_contrast &lt;- ggplot(panel_model, aes(precip_mm_lag1, precip_mm)) +\n  stat_summary_2d(aes(z = mean_dep_delay), bins = 20) +\n  facet_wrap(~ origin, nrow = 1) +\n  labs(\n    title = \"Today vs yesterday precipitation (colored by mean delay)\",\n    x = \"Yesterday precip (mm)\", y = \"Today precip (mm)\", fill = \"Mean delay (min)\"\n  ) +\n  quiet_theme()\n\np_contrast"
  },
  {
    "objectID": "stories/weather-memory.html#modeling-does-yesterdays-weather-matter-after-controls",
    "href": "stories/weather-memory.html#modeling-does-yesterdays-weather-matter-after-controls",
    "title": "Weather Memory",
    "section": "âš™ï¸ Modeling: does yesterdayâ€™s weather matter after controls?",
    "text": "âš™ï¸ Modeling: does yesterdayâ€™s weather matter after controls?\n\n\nCode\nmodel_full &lt;- lm(\n  mean_dep_delay ~ precip_mm_lag1 + wind_mph_lag1 + visib_mi_lag1 + temp_F_lag1 +\n    precip_mm + wind_mph + visib_mi + temp_F + origin + dow + month,\n  data = panel_model\n)\n\ntidy_coefs &lt;- broom::tidy(model_full, conf.int = TRUE) %&gt;%\n  filter(term %in% c(\"precip_mm_lag1\",\"wind_mph_lag1\",\"visib_mi_lag1\",\"temp_F_lag1\")) %&gt;%\n  mutate(term = recode(term,\n    precip_mm_lag1 = \"Yesterday precip (mm)\",\n    wind_mph_lag1  = \"Yesterday wind (mph)\",\n    visib_mi_lag1  = \"Yesterday visibility (mi)\",\n    temp_F_lag1    = \"Yesterday temperature (Â°F)\"\n  ))\n\ngt_tbl &lt;- tidy_coefs %&gt;%\n  select(term, estimate, conf.low, conf.high, p.value) %&gt;%\n  mutate(across(c(estimate, conf.low, conf.high), ~round(., 3)),\n         p.value = format.pval(p.value, digits = 3, eps = 0.001)) %&gt;%\n  gt::gt() %&gt;%\n  gt::tab_header(title = \"Lagged weather coefficients (partial effects)\") %&gt;%\n  gt::fmt_markdown(columns = 1) %&gt;%\n  gt::cols_label(\n    term = \"Predictor\",\n    estimate = \"Estimate\",\n    conf.low = \"CI low\",\n    conf.high = \"CI high\",\n    p.value = \"p\"\n  )\n\ngt_tbl\n\n\n\n\n\n\n\n\nLagged weather coefficients (partial effects)\n\n\nPredictor\nEstimate\nCI low\nCI high\np\n\n\n\n\nYesterday precip (mm)\n-0.135\n-0.227\n-0.043\n0.00397\n\n\nYesterday wind (mph)\n0.320\n0.138\n0.503\n&lt; 0.001\n\n\nYesterday visibility (mi)\n0.170\n-0.378\n0.719\n0.54234\n\n\nYesterday temperature (Â°F)\n0.168\n0.040\n0.296\n0.01035"
  },
  {
    "objectID": "stories/weather-memory.html#airport-where-memory-is-strongest",
    "href": "stories/weather-memory.html#airport-where-memory-is-strongest",
    "title": "Weather Memory",
    "section": "âœˆï¸ Airport where memory is strongest",
    "text": "âœˆï¸ Airport where memory is strongest\n\n\nCode\nby_airport &lt;- panel_model %&gt;%\n  group_by(origin) %&gt;%\n  group_modify(~ broom::tidy(lm(\n      mean_dep_delay ~ precip_mm_lag1 + wind_mph_lag1 + visib_mi_lag1 + temp_F_lag1 +\n        precip_mm + wind_mph + visib_mi + temp_F + dow + month,\n      data = .x\n    ), conf.int = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  filter(term == \"precip_mm_lag1\")\n\np_airport &lt;- by_airport %&gt;%\n  ggplot(aes(x = reorder(origin, estimate), y = estimate)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.1) +\n  coord_flip() +\n  labs(title = \"Where is the weather-memory effect strongest?\",\n       subtitle = \"Coefficient on *yesterday precipitation* by airport (separate regressions)\",\n       x = \"Airport\", y = \"Lag precip coefficient (minutes per mm)\",\n       caption = \"Points = estimates; bars = 95% CI.\") +\n  quiet_theme()\n\np_airport"
  },
  {
    "objectID": "stories/weather-memory.html#cancellation-link-does-yesterdays-rain-raise-todays-cancels",
    "href": "stories/weather-memory.html#cancellation-link-does-yesterdays-rain-raise-todays-cancels",
    "title": "Weather Memory",
    "section": "ğŸ”—ï¸ Cancellation link: does yesterdayâ€™s rain raise todayâ€™s cancels?",
    "text": "ğŸ”—ï¸ Cancellation link: does yesterdayâ€™s rain raise todayâ€™s cancels?\n\n\nCode\ncor_overall &lt;- panel_model %&gt;%\n  summarise(cor = cor(precip_mm_lag1, cancel_rate, use = \"complete.obs\"))\n\ncor_by_airport &lt;- panel_model %&gt;%\n  group_by(origin) %&gt;%\n  summarise(cor = cor(precip_mm_lag1, cancel_rate, use = \"complete.obs\"))\n\ncor_overall; cor_by_airport"
  },
  {
    "objectID": "stories/weather-memory.html#what-if-set-yesterdays-rain-to-zero",
    "href": "stories/weather-memory.html#what-if-set-yesterdays-rain-to-zero",
    "title": "Weather Memory",
    "section": "ğŸ”® What-if: set yesterdayâ€™s rain to zero",
    "text": "ğŸ”® What-if: set yesterdayâ€™s rain to zero\n\n\nCode\npred_obs &lt;- augment(model_full, data = panel_model) %&gt;% select(origin, date, .fitted)\n\npred_counterf &lt;- panel_model %&gt;%\n  mutate(precip_mm_lag1 = 0) %&gt;%\n  add_predictions(model_full) %&gt;%\n  transmute(origin, date, fitted_zero_lag_rain = pred)\n\nwhat_if &lt;- pred_obs %&gt;%\n  left_join(pred_counterf, by = c(\"origin\",\"date\")) %&gt;%\n  left_join(panel_model %&gt;% select(origin, date, mean_dep_delay), by = c(\"origin\",\"date\")) %&gt;%\n  mutate(\n    delta_minutes = .fitted - fitted_zero_lag_rain,\n    ontime_rate_obs = pmax(0, 1 - mean_dep_delay/15),\n    ontime_rate_cf  = pmax(0, 1 - fitted_zero_lag_rain/15),\n    ontime_gain_pct = (ontime_rate_cf - ontime_rate_obs) * 100\n  )\n\nsummary(what_if$ontime_gain_pct)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-112.844  -34.609   -4.604  -13.325    0.000  105.299 \n\n\nCode\np_whatif &lt;- what_if %&gt;%\n  ggplot(aes(x = ontime_gain_pct)) +\n  geom_histogram(bins = 30, alpha = 0.7) +\n  facet_wrap(~ origin, nrow = 1) +\n  labs(title = \"What-if yesterday were dry: implied on-time rate improvement\",\n       x = \"Percentage points (counterfactual âˆ’ observed)\", y = \"Days\",\n       caption = \"Back-of-envelope: converts minutes to on-time share via a 15-minute threshold on the mean.\") +\n  quiet_theme()\n\np_whatif"
  },
  {
    "objectID": "stories/weather-memory.html#bonus-today-vs.-yesterday-on-a-joint-scatter-hoverable",
    "href": "stories/weather-memory.html#bonus-today-vs.-yesterday-on-a-joint-scatter-hoverable",
    "title": "Weather Memory",
    "section": "ğŸ”„Bonus: Today vs.Â Yesterday on a joint scatter (hoverable)",
    "text": "ğŸ”„Bonus: Today vs.Â Yesterday on a joint scatter (hoverable)\n\n\nCode\np_scatter2 &lt;- panel_model %&gt;%\n  ggplot(aes(x = precip_mm_lag1, y = precip_mm, size = mean_dep_delay, text = paste(date, origin))) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~ origin) +\n  labs(title = \"Today vs. yesterday precipitation (point size = today's mean delay)\",\n       x = \"Yesterday (mm)\", y = \"Today (mm)\") +\n  quiet_theme()\n\nggplotly(p_scatter2, tooltip = c(\"text\", \"x\", \"y\")) %&gt;%\n  layout(\n    margin = list(t = 100),  # more top room\n    title = list(\n      text = \"&lt;b&gt;Today vs. yesterday precipitation (point size = today's mean delay)&lt;/b&gt;\",\n      font = list(size = 12),\n      x = 0.5, xanchor = \"center\"\n    )\n  )"
  },
  {
    "objectID": "stories/weather-memory.html#story-beats",
    "href": "stories/weather-memory.html#story-beats",
    "title": "Weather Memory",
    "section": "ğŸµ Story beats",
    "text": "ğŸµ Story beats\nWe asked: Do storms leave a hangover in the schedule?\nWe did: Calculated daily delays by airport, then compared them to both todayâ€™s and yesterdayâ€™s precipitation, wind, visibility, and temperature.\nWe found: Yesterdayâ€™s weather often remains a statistically meaningful predictor of todayâ€™s delays, even when we control for todayâ€™s weather and calendar effects.\nWhy it matters: Recovery is a process. Aircraft and crews are networked; disruptions propagate and decay, sometimes over more than one day."
  },
  {
    "objectID": "stories/weather-memory.html#practical-implications",
    "href": "stories/weather-memory.html#practical-implications",
    "title": "Weather Memory",
    "section": "âš™ï¸ Practical implications",
    "text": "âš™ï¸ Practical implications\n\nScheduling & rotations: Build buffers the day after major weather events; rotate slack where the lag effect is strongest.\nMaintenance & de-icing ops: Staff for spillover demand on wetâ†’dry transitions.\nPassenger comms: Proactive alerts for potential residual delays the day after storms."
  },
  {
    "objectID": "stories/weather-memory.html#limitations",
    "href": "stories/weather-memory.html#limitations",
    "title": "Weather Memory",
    "section": "ğŸš§ Limitations",
    "text": "ğŸš§ Limitations\n\nWe use means of delays; distributional effects (e.g., big tails) are not fully captured.\nOnly 1-day memory is modeled; longer lags may matter.\nWeather aggregation is daily; within-day timing (e.g., late-night rain) could drive stronger effects.\nLinear models are a simplification; non-linearities and interactions (e.g., wind Ã— visibility) are likely.\n\n\n Dataset: nycflights13 \n\n\n What do these icons mean?"
  },
  {
    "objectID": "stories/weather-memory.html#references-credits",
    "href": "stories/weather-memory.html#references-credits",
    "title": "Weather Memory",
    "section": "ğŸ“š References & Credits",
    "text": "ğŸ“š References & Credits\n\nData: nycflights13 (Hadley Wickham et al.) â€” CRAN: https://CRAN.R-project.org/package=nycflights13\nPackages: tidyverse, lubridate, broom, gt, plotly, modelr.\nTheme: falls back to a quiet minimal style if your siteâ€™s quiet_theme() is not available."
  },
  {
    "objectID": "stories/weather-memory.html#gentle-call-to-curiosity",
    "href": "stories/weather-memory.html#gentle-call-to-curiosity",
    "title": "Weather Memory",
    "section": "ğŸ•Šï¸ Gentle call to curiosity",
    "text": "ğŸ•Šï¸ Gentle call to curiosity\nWhat other forms of memory might our systems hold?"
  }
]